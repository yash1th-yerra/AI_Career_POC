{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7058eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d5e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADVANCED JOB MATCHING SYSTEM WITH EMBEDDINGS\n",
    "# ============================================================================\n",
    "# This section contains the embedding-based job matching system\n",
    "# Move this to the bottom as requested\n",
    "\n",
    "# Additional imports for embedding system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env for API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c3295eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6196d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760040668.684293    5344 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash LLM with safe optimizations (maintains consistency)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,  # Restore original temperature for consistency\n",
    "    api_key=api_key,\n",
    "    # max_output_tokens=4096,  # Set explicit limit for faster response\n",
    "    max_retries=2,  # Keep reasonable retries for reliability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a109c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_file(file_path: str, file_type: str) -> str:\n",
    "    \"\"\"Extract text from PDF or DOCX resume.\"\"\"\n",
    "    if file_type.lower() == \"pdf\":\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            max_pages = min(2, len(pdf.pages))\n",
    "            for i in range(max_pages):\n",
    "                text_parts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(text_parts)\n",
    "    elif file_type.lower() == \"docx\":\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Optional cleaning\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9814675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Unified Resume Extraction with JSON Output (Fixed Parser Issue)\n",
    "import json\n",
    "\n",
    "# Create a LangChain prompt with direct JSON output\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert Resume Intelligence Agent that extracts structured data and evaluates resumes for ATS compatibility.\n",
    "\n",
    "Analyze the following resume text and return ONLY a valid JSON object with these exact keys:\n",
    "\n",
    "{{\n",
    "  \"name\": \"\",\n",
    "  \"location\": \"\",\n",
    "  \"summary\": \"\",\n",
    "  \"skills\": [],\n",
    "  \"extra_skills\": [],\n",
    "  \"work_experience\": [],\n",
    "  \"projects\": [],\n",
    "  \"certifications\": [],\n",
    "  \"education\": [],\n",
    "  \"experience_level\": \"\",\n",
    "  \"recommended_roles\": [],\n",
    "  \"ats_feedback\": {{\n",
    "    \"score\": 0,\n",
    "    \"summary\": \"\",\n",
    "    \"strengths\": [],\n",
    "    \"improvements\": []\n",
    "  }}\n",
    "}}\n",
    "\n",
    "CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "1. **NAME**: Extract the full name exactly as written, using the most prominent name (usually at the top).\n",
    "\n",
    "2. **LOCATION**: Extract specific city and country from contact info, address, or personal details. Format as \"City, Country\" (e.g., \"Chennai, India\", \"Bangalore, India\"). If no city is specified, use \" Country\".\n",
    "\n",
    "3. **SUMMARY**: Look for sections titled \"Summary\", \"Objective\", \"Profile\", \"About Me\", \"Career Summary\". Extract complete professional summary.\n",
    "\n",
    "4. **SKILLS**: Extract skills ONLY from dedicated \"Skills\", \"Technical Skills\", \"Core Skills\", \"Programming Languages\", or similar sections:\n",
    "   - ONLY include skills explicitly listed in a dedicated skills section\n",
    "   - Programming languages, frameworks, tools, technologies mentioned in skills section\n",
    "   - Return as array of individual skills from the skills section only\n",
    "\n",
    "5. **EXTRA_SKILLS**: Extract additional skills mentioned in other contexts:\n",
    "   - Skills mentioned in work experience descriptions\n",
    "   - Technologies used in projects\n",
    "   - Skills mentioned in certifications or education\n",
    "   - Any other skills not in the main skills section\n",
    "   - Return as array of individual skills from non-skills sections\n",
    "\n",
    "6. **WORK_EXPERIENCE**: Extract each position with:\n",
    "   - Job title, company, duration, location\n",
    "   - Key responsibilities and achievements\n",
    "   - Format as structured objects with consistent fields\n",
    "\n",
    "7. **PROJECTS**: Extract personal/academic projects with:\n",
    "   - Project name, duration, technologies used\n",
    "   - Brief description and key features\n",
    "   - Any notable achievements or results\n",
    "\n",
    "8. **CERTIFICATIONS**: Extract all certifications with:\n",
    "   - Certification name, issuing organization, year\n",
    "   - Include online courses, professional certifications\n",
    "\n",
    "9. **EDUCATION**: Extract educational background with:\n",
    "   - Degree, institution, graduation year\n",
    "   - Relevant coursework or achievements\n",
    "\n",
    "10. **EXPERIENCE_LEVEL**: Analyze the candidate's work experience and determine their experience level:\n",
    "    - \"Entry Level\" (0-1 years): Fresh graduates, internships, or minimal professional experience\n",
    "    - \"Junior\" (1-3 years): Some professional experience, early career roles\n",
    "    - \"Mid-Level\" (3-7 years): Solid professional experience, can work independently\n",
    "    - \"Senior\" (7-12 years): Advanced experience, can lead projects and mentor others\n",
    "    - \"Lead/Principal\" (12+ years): Expert level, can architect solutions and lead teams\n",
    "    - Consider total years of experience, complexity of roles, leadership responsibilities\n",
    "    - Return a single string value\n",
    "\n",
    "11. **RECOMMENDED_ROLES**: Based on the candidate's skills, experience, education, and projects, recommend 2-3 specific job roles they would be suitable for:\n",
    "    - Consider their technical skills, domain expertise, and career progression\n",
    "    - Include roles that match their current skill level and potential growth areas\n",
    "    - Format as array of role titles (e.g., [\"Software Engineer\", \"Data Analyst\", \"Frontend Developer\"])\n",
    "    - Be specific and industry-relevant\n",
    "\n",
    "12. **ATS_FEEDBACK**: Provide objective evaluation:\n",
    "    - score: 0-100 based on ATS compatibility\n",
    "    - summary: Brief assessment\n",
    "    - strengths: Positive aspects\n",
    "    - improvements: Areas for enhancement \n",
    "\n",
    "Guidelines:\n",
    "- Detect section names dynamically (e.g., \"Profile\", \"About Me\", \"Objective\" ‚Üí summary).\n",
    "- CRITICAL: Skills extraction must be source-aware:\n",
    "  * \"skills\" array: ONLY from dedicated skills sections (Skills, Technical Skills, Core Skills, Programming Languages, etc.)\n",
    "  * \"extra_skills\" array: Skills mentioned in work experience, projects, certifications, education, or other contexts\n",
    "- Extract job/project details separately.\n",
    "- For EXPERIENCE_LEVEL: Analyze total years of professional experience, role complexity, and leadership indicators\n",
    "- For RECOMMENDED_ROLES: Analyze the candidate's profile holistically and suggest roles that align with their skills and experience level\n",
    "- Be consistent and produce clean JSON only.\n",
    "- Prioritize accuracy over completeness.\n",
    "- IMPORTANT: Return ONLY the JSON object, no additional text or explanations.\n",
    "\n",
    "Resume Text:\n",
    "{resume_text}\n",
    "\"\"\")\n",
    "\n",
    "# Build the chain with StrOutputParser for better JSON handling\n",
    "resume_parser_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a559f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract text\n",
    "resume_path = \"app/resumes/Yeswanth_Yerra_CV.pdf\"\n",
    "text = extract_text_from_file(resume_path, \"pdf\")\n",
    "\n",
    "# Step 2: Parse with Gemini\n",
    "try:\n",
    "    raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "    # print(\"Raw LLM Output:\")\n",
    "    # print(raw_output)\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Parse JSON from the output\n",
    "    structured_output = json.loads(raw_output)\n",
    "    print(\"Parsed JSON Output:\")\n",
    "    print(json.dumps(structured_output, indent=2))\n",
    "    \n",
    "    # Display experience level prominently\n",
    "    # if \"experience_level\" in structured_output and structured_output[\"experience_level\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"üìä EXPERIENCE LEVEL:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     print(f\"Level: {structured_output['experience_level']}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "    # # Display recommended roles prominently\n",
    "    # if \"recommended_roles\" in structured_output and structured_output[\"recommended_roles\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"üéØ RECOMMENDED ROLES FOR THIS CANDIDATE:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     for i, role in enumerate(structured_output[\"recommended_roles\"], 1):\n",
    "    #         print(f\"{i}. {role}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "    print(\"Raw output that failed to parse:\")\n",
    "    print(raw_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Raw output:\")\n",
    "    print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c197ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn Job Scraper Integration\n",
    "# Import necessary packages for web scraping and logging\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "# Configure logging settings\n",
    "logging.basicConfig(filename=\"linkedin_scraping.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40393c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInJobScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"\n",
    "        Initialize the LinkedIn Job Scraper\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        headless : bool\n",
    "            Whether to run Chrome in headless mode (default: False)\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.setup_driver()\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with appropriate options\"\"\"\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            \n",
    "            # Basic options\n",
    "            options.add_argument(\"--start-maximized\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            \n",
    "            # User agent to avoid detection\n",
    "            options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument(\"--headless\")\n",
    "            \n",
    "            self.driver = webdriver.Chrome(options=options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logging.info(\"Chrome WebDriver initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def build_search_url(self, job_title: str, location: str = \"India\", experience_level: str = None, \n",
    "                        time_posted: str = None, remote: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Build LinkedIn job search URL with filters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        job_title : str\n",
    "            Job title to search for\n",
    "        location : str\n",
    "            Location to search in (default: \"India\")\n",
    "        experience_level : str\n",
    "            Experience level filter (Entry level, Associate, Mid-Senior level, Director, Executive)\n",
    "        time_posted : str\n",
    "            Time posted filter (r86400, r604800, r2592000, r31536000)\n",
    "        remote : bool\n",
    "            Whether to include remote jobs only\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete LinkedIn job search URL\n",
    "        \"\"\"\n",
    "        base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "        \n",
    "        # URL encode parameters\n",
    "        job_title_encoded = urllib.parse.quote(job_title)\n",
    "        location_encoded = urllib.parse.quote(location)\n",
    "        \n",
    "        # Build query parameters\n",
    "        params = {\n",
    "            \"keywords\": job_title_encoded,\n",
    "            \"location\": location_encoded,\n",
    "            \"f_TPR\": time_posted if time_posted else None,  # Time posted filter\n",
    "            \"f_E\": self._get_experience_filter(experience_level) if experience_level else None,  # Experience filter\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # Filter out None values and build query string\n",
    "        query_params = {k: v for k, v in params.items() if v is not None}\n",
    "        query_string = \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "        \n",
    "        full_url = f\"{base_url}?{query_string}\"\n",
    "        logging.info(f\"Built search URL: {full_url}\")\n",
    "        \n",
    "        return full_url\n",
    "    \n",
    "    def _get_experience_filter(self, experience_level: str) -> str:\n",
    "        \"\"\"\n",
    "        Map experience level to LinkedIn filter values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        experience_level : str\n",
    "            Experience level from resume analysis\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn experience filter value\n",
    "        \"\"\"\n",
    "        experience_mapping = {\n",
    "            \"Entry Level\": \"1\",           # Entry level\n",
    "            \"Junior\": \"2\",                # Associate\n",
    "            \"Mid-Level\": \"3\",             # Mid-Senior level\n",
    "            \"Senior\": \"4\",                # Director\n",
    "            \"Lead/Principal\": \"5\"         # Executive\n",
    "        }\n",
    "        \n",
    "        return experience_mapping.get(experience_level, \"1\")  # Default to Entry level\n",
    "    \n",
    "    def _get_time_filter(self, days: int) -> str:\n",
    "        \"\"\"\n",
    "        Get time posted filter based on days\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        days : int\n",
    "            Number of days to look back\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn time filter value\n",
    "        \"\"\"\n",
    "        time_mapping = {\n",
    "            1: \"r86400\",      # Past 24 hours\n",
    "            7: \"r604800\",     # Past week\n",
    "            30: \"r2592000\",   # Past month\n",
    "            365: \"r31536000\"  # Past year\n",
    "        }\n",
    "        \n",
    "        return time_mapping.get(days, \"r604800\")  # Default to past week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8927a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the remaining methods to LinkedInJobScraper class\n",
    "def add_methods_to_scraper():\n",
    "    \"\"\"Add the scrape_jobs and close methods to LinkedInJobScraper class\"\"\"\n",
    "    \n",
    "    def scrape_jobs(self, job_title: str, location: str = \"India\", pages: int = 1, \n",
    "                   experience_level: str = None, days_back: int = 7) -> list:\n",
    "        \"\"\"Scrape job listings from LinkedIn with filters\"\"\"\n",
    "        logging.info(f'Starting LinkedIn job scrape for \"{job_title}\" in \"{location}\"...')\n",
    "        \n",
    "        # Build search URL with filters\n",
    "        time_filter = self._get_time_filter(days_back)\n",
    "        search_url = self.build_search_url(\n",
    "            job_title=job_title,\n",
    "            location=location,\n",
    "            experience_level=experience_level,\n",
    "            time_posted=time_filter\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the LinkedIn job search page\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)  # Wait for page to load\n",
    "            \n",
    "            # Scroll through the specified number of pages\n",
    "            for i in range(pages):\n",
    "                logging.info(f\"Scrolling to bottom of page {i+1}...\")\n",
    "                \n",
    "                # Scroll to the bottom of the page using JavaScript\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                \n",
    "                try:\n",
    "                    # Wait for the \"Show more\" button to be present on the page\n",
    "                    element = WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.XPATH, \"/html/body/div[1]/div/main/section[2]/button\")\n",
    "                        )\n",
    "                    )\n",
    "                    # Click on the \"Show more\" button\n",
    "                    element.click()\n",
    "                    logging.info(\"Clicked 'Show more' button\")\n",
    "                    \n",
    "                except Exception:\n",
    "                    logging.info(\"Show more button not found or not clickable\")\n",
    "                \n",
    "                # Wait for a random amount of time before scrolling to the next page\n",
    "                time.sleep(random.choice(list(range(3, 7))))\n",
    "            \n",
    "            # Scrape the job postings\n",
    "            jobs = []\n",
    "            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Updated selectors for current LinkedIn structure\n",
    "            job_listings = soup.find_all(\n",
    "                \"div\",\n",
    "                class_=\"base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card\",\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Found {len(job_listings)} job listings to process\")\n",
    "            \n",
    "            for idx, job in enumerate(job_listings):\n",
    "                try:\n",
    "                    # Extract job details with error handling\n",
    "                    job_title_elem = job.find(\"h3\", class_=\"base-search-card__title\")\n",
    "                    job_title_text = job_title_elem.text.strip() if job_title_elem else \"N/A\"\n",
    "                    \n",
    "                    job_company_elem = job.find(\"h4\", class_=\"base-search-card__subtitle\")\n",
    "                    job_company_text = job_company_elem.text.strip() if job_company_elem else \"N/A\"\n",
    "                    \n",
    "                    job_location_elem = job.find(\"span\", class_=\"job-search-card__location\")\n",
    "                    job_location_text = job_location_elem.text.strip() if job_location_elem else \"N/A\"\n",
    "                    \n",
    "                    apply_link_elem = job.find(\"a\", class_=\"base-card__full-link\")\n",
    "                    apply_link = apply_link_elem[\"href\"] if apply_link_elem else \"N/A\"\n",
    "                    \n",
    "                    # Navigate to the job posting page and scrape the description\n",
    "                    if apply_link != \"N/A\":\n",
    "                        self.driver.get(apply_link)\n",
    "                        time.sleep(random.choice(list(range(5, 11))))\n",
    "                        \n",
    "                        try:\n",
    "                            description_soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "                            job_description_elem = description_soup.find(\n",
    "                                \"div\", class_=\"description__text description__text--rich\"\n",
    "                            )\n",
    "                            job_description = job_description_elem.text.strip() if job_description_elem else \"Description not available\"\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Could not retrieve job description: {str(e)}\")\n",
    "                            job_description = \"Description not available\"\n",
    "                    else:\n",
    "                        job_description = \"Description not available\"\n",
    "                    \n",
    "                    # Add job details to the jobs list\n",
    "                    job_data = {\n",
    "                        \"title\": job_title_text,\n",
    "                        \"company\": job_company_text,\n",
    "                        \"location\": job_location_text,\n",
    "                        \"link\": apply_link,\n",
    "                        \"description\": job_description,\n",
    "                        \"searched_for\": job_title,\n",
    "                        \"experience_level_filter\": experience_level,\n",
    "                        \"days_back\": days_back,\n",
    "                        \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    jobs.append(job_data)\n",
    "                    logging.info(f'Scraped \"{job_title_text}\" at {job_company_text} in {job_location_text}...')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing job listing {idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logging.info(f\"Successfully scraped {len(jobs)} jobs\")\n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while scraping jobs: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"WebDriver closed successfully\")\n",
    "    \n",
    "    # Add methods to the class\n",
    "    LinkedInJobScraper.scrape_jobs = scrape_jobs\n",
    "    LinkedInJobScraper.close = close\n",
    "\n",
    "# Execute the function to add methods to the class\n",
    "add_methods_to_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff3776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for job scraping and data management\n",
    "\n",
    "def save_jobs_to_csv(jobs_data: list, filename: str = \"linkedin_jobs.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Save job data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        jobs_data: A list of dictionaries containing job data\n",
    "        filename: Name of the CSV file to save to\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not jobs_data:\n",
    "        logging.warning(\"No job data to save\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create a pandas DataFrame from the job data\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file without including the index column\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Log a message indicating how many jobs were successfully scraped and saved\n",
    "        logging.info(f\"Successfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        print(f\"‚úÖ Saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving jobs to CSV: {str(e)}\")\n",
    "        print(f\"‚ùå Error saving jobs to CSV: {str(e)}\")\n",
    "\n",
    "def scrape_jobs_for_resume(resume_data: dict, pages_per_role: int = 1, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Scrape LinkedIn jobs based on resume analysis results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_data : dict\n",
    "        Resume analysis data containing recommended_roles and experience_level\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Combined list of all scraped jobs\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    # Extract recommended roles and experience level\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    experience_level = resume_data.get(\"experience_level\", \"\")\n",
    "    \n",
    "    if not recommended_roles:\n",
    "        logging.warning(\"No recommended roles found in resume data\")\n",
    "        print(\"‚ö†Ô∏è No recommended roles found in resume data\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üéØ Found {len(recommended_roles)} recommended roles: {recommended_roles}\")\n",
    "    print(f\"üìä Experience Level: {experience_level}\")\n",
    "    print(f\"‚è∞ Looking for jobs posted in the last {days_back} days\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nüîç Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nüéâ Total jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"‚ùå Error during job scraping: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def analyze_and_scrape_jobs(resume_path: str, file_type: str = \"pdf\", \n",
    "                          pages_per_role: int = 1, days_back: int = 7) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Analyze resume and scrape relevant jobs\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_path : str\n",
    "        Path to the resume file\n",
    "    file_type : str\n",
    "        Type of resume file (pdf or docx)\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Combined resume analysis and job scraping results\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Extract and analyze resume\n",
    "    print(\"üìÑ Step 1: Analyzing resume...\")\n",
    "    text = extract_text_from_file(resume_path, file_type)\n",
    "    \n",
    "    try:\n",
    "        raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "        resume_data = json.loads(raw_output)\n",
    "        \n",
    "        print(\"‚úÖ Resume analysis completed\")\n",
    "        print(f\"üë§ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "        print(f\"üìç Location: {resume_data.get('location', 'N/A')}\")\n",
    "        print(f\"üìä Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "        print(f\"üéØ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing resume: {str(e)}\")\n",
    "        return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    # Step 2: Scrape jobs based on analysis\n",
    "    print(f\"\\nüîç Step 2: Scraping jobs for recommended roles...\")\n",
    "    scraped_jobs = scrape_jobs_for_resume(resume_data, pages_per_role, days_back)\n",
    "    \n",
    "    # Step 3: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": scraped_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(scraped_jobs),\n",
    "            \"recommended_roles_searched\": resume_data.get(\"recommended_roles\", []),\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    if scraped_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"jobs_{resume_data.get('name', 'candidate').replace(' ', '_')}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(scraped_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f998073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Fixed Pipeline...\n",
      "üöÄ Starting Resume Analysis and Job Scraping Pipeline\n",
      "================================================================================\n",
      "üìÑ Step 1: Analyzing resume...\n",
      "‚úÖ Resume data ready\n",
      "üë§ Candidate: Yeswanth Yerra\n",
      "üìç Location: Chennai, India\n",
      "üìä Experience Level: Entry Level\n",
      "üéØ Recommended Roles: ['Junior Software Engineer', 'Junior Machine Learning Engineer', 'Cloud Engineer']\n",
      "\n",
      "üîç Step 2: Scraping real jobs for 3 roles...\n",
      "‚ö†Ô∏è This will open a browser window and scrape real LinkedIn jobs\n",
      "\n",
      "üîç Scraping jobs for role 1/3: 'Junior Software Engineer'\n",
      "‚úÖ Found 68 jobs for 'Junior Software Engineer'\n",
      "‚è≥ Waiting before next search...\n",
      "\n",
      "üîç Scraping jobs for role 2/3: 'Junior Machine Learning Engineer'\n",
      "‚ö†Ô∏è No jobs found for 'Junior Machine Learning Engineer'\n",
      "‚è≥ Waiting before next search...\n",
      "\n",
      "üîç Scraping jobs for role 3/3: 'Cloud Engineer'\n",
      "‚úÖ Found 21 jobs for 'Cloud Engineer'\n",
      "\n",
      "üéâ Total real jobs scraped: 89\n",
      "üîí Browser closed\n",
      "‚úÖ Saved 89 jobs to jobs_Yeswanth_Yerra_20251009_123332.csv\n",
      "üíæ Jobs saved to: jobs_Yeswanth_Yerra_20251009_123332.csv\n",
      "\n",
      "üéâ Pipeline completed successfully!\n",
      "================================================================================\n",
      "\n",
      "üìä PIPELINE SUMMARY:\n",
      "==================================================\n",
      "Total Jobs Found: 89\n",
      "Roles Searched: ['Junior Software Engineer', 'Junior Machine Learning Engineer', 'Cloud Engineer']\n",
      "Experience Level: Entry Level\n",
      "Status: Real LinkedIn jobs scraped\n",
      "CSV File: jobs_Yeswanth_Yerra_20251009_123332.csv\n"
     ]
    }
   ],
   "source": [
    "# Fixed version with better error handling\n",
    "def analyze_and_scrape_jobs_fixed(resume_path: str, file_type: str = \"pdf\", \n",
    "                                pages_per_role: int = 1, days_back: int = 7, resume_data: dict | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Reuse parsed resume if available; otherwise analyze, then scrape jobs.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Get resume data (prefer existing parsed output)\n",
    "    if resume_data is None:\n",
    "        try:\n",
    "            if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "                resume_data = structured_output\n",
    "                print(\"‚úÖ Using existing parsed resume from structured_output\")\n",
    "            elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "                resume_data = result['resume_analysis']\n",
    "                print(\"‚úÖ Using existing parsed resume from result['resume_analysis']\")\n",
    "        except Exception:\n",
    "            resume_data = None\n",
    "    \n",
    "    if resume_data is None:\n",
    "        print(\"üìÑ Step 1: Analyzing resume...\")\n",
    "        text = extract_text_from_file(resume_path, file_type)\n",
    "        try:\n",
    "            raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "            # Clean the output to handle markdown formatting\n",
    "            cleaned_output = raw_output.strip()\n",
    "            if cleaned_output.startswith(\"```json\"):\n",
    "                cleaned_output = cleaned_output[7:]\n",
    "            if cleaned_output.endswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[:-3]\n",
    "            cleaned_output = cleaned_output.strip()\n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                resume_data = json.loads(cleaned_output)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ùå JSON parsing failed: {e}\")\n",
    "                json_start = cleaned_output.find('{')\n",
    "                if json_start != -1:\n",
    "                    json_part = cleaned_output[json_start:]\n",
    "                    try:\n",
    "                        resume_data = json.loads(json_part)\n",
    "                        print(\"‚úÖ JSON parsing successful after extraction!\")\n",
    "                    except json.JSONDecodeError as e2:\n",
    "                        print(f\"‚ùå Still failed after extraction: {e2}\")\n",
    "                        return {\"error\": f\"JSON parsing failed: {e2}\"}\n",
    "                else:\n",
    "                    return {\"error\": f\"No JSON found in output: {cleaned_output[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing resume: {str(e)}\")\n",
    "            return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    print(\"‚úÖ Resume data ready\")\n",
    "    print(f\"üë§ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "    print(f\"üìç Location: {resume_data.get('location', 'N/A')}\")\n",
    "    print(f\"üìä Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "    print(f\"üéØ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "    \n",
    "    # Step 2: Check if we have recommended roles\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    if not recommended_roles:\n",
    "        print(\"‚ö†Ô∏è No recommended roles found. Cannot scrape jobs.\")\n",
    "        return {\n",
    "            \"resume_analysis\": resume_data,\n",
    "            \"scraped_jobs\": [],\n",
    "            \"summary\": {\n",
    "                \"total_jobs_found\": 0,\n",
    "                \"recommended_roles_searched\": [],\n",
    "                \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "                \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"status\": \"No recommended roles found\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Step 3: Scrape jobs based on analysis\n",
    "    print(f\"\\nüîç Step 2: Scraping real jobs for {len(recommended_roles)} roles...\")\n",
    "    print(\"‚ö†Ô∏è This will open a browser window and scrape real LinkedIn jobs\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nüîç Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=resume_data.get(\"experience_level\", \"\"),\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                print(f\"‚è≥ Waiting before next search...\")\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nüéâ Total real jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"‚ùå Error during job scraping: {str(e)}\")\n",
    "        all_jobs = []\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "        print(\"üîí Browser closed\")\n",
    "    \n",
    "    # Step 4: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": all_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(all_jobs),\n",
    "            \"recommended_roles_searched\": recommended_roles,\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"status\": \"Real LinkedIn jobs scraped\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    if all_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        candidate_name = resume_data.get('name', 'candidate').replace(' ', '_')\n",
    "        csv_filename = f\"jobs_{candidate_name}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(all_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "        print(f\"üíæ Jobs saved to: {csv_filename}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No jobs found to save\")\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"üß™ Testing Fixed Pipeline...\")\n",
    "result = analyze_and_scrape_jobs_fixed(\n",
    "    resume_path=\"app/resumes/Yeswanth_Yerra_CV.pdf\",\n",
    "    file_type=\"pdf\",\n",
    "    pages_per_role=1,\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Print summary safely\n",
    "print(\"\\nüìä PIPELINE SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "if \"summary\" in result:\n",
    "    print(f\"Total Jobs Found: {result['summary']['total_jobs_found']}\")\n",
    "    print(f\"Roles Searched: {result['summary']['recommended_roles_searched']}\")\n",
    "    print(f\"Experience Level: {result['summary']['experience_level']}\")\n",
    "    print(f\"Status: {result['summary']['status']}\")\n",
    "    if 'csv_file' in result:\n",
    "        print(f\"CSV File: {result['csv_file']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a1a7d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test with specific roles\\ntest_roles = [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\\njobs = scrape_jobs_directly(\\n    job_roles=test_roles,\\n    location=\"India\", \\n    pages=1,\\n    experience_level=\"Junior\",\\n    days_back=7\\n)\\n\\n# Save results\\nif jobs:\\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\\n    filename = f\"direct_jobs_{timestamp}.csv\"\\n    save_jobs_to_csv(jobs, filename)\\n    print(f\"üíæ Saved {len(jobs)} jobs to {filename}\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple function to scrape jobs for specific roles\n",
    "def scrape_jobs_directly(job_roles: list, location: str = \"India\", pages: int = 1, \n",
    "                        experience_level: str = None, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Directly scrape jobs for given roles without resume analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    job_roles : list\n",
    "        List of job titles to search for\n",
    "    location : str\n",
    "        Location to search in (default: \"India\")\n",
    "    pages : int\n",
    "        Number of pages to scrape per role\n",
    "    experience_level : str\n",
    "        Experience level filter\n",
    "    days_back : int\n",
    "        Days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of scraped job dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"üîç Direct Job Scraping for {len(job_roles)} roles\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìç Location: {location}\")\n",
    "    print(f\"üìä Experience Level: {experience_level or 'Any'}\")\n",
    "    print(f\"‚è∞ Days Back: {days_back}\")\n",
    "    print(f\"üìÑ Pages per role: {pages}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)\n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(job_roles, 1):\n",
    "            print(f\"\\nüîç [{i}/{len(job_roles)}] Scraping: '{role}'\")\n",
    "            \n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=location,\n",
    "                pages=pages,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "                \n",
    "                # Show first few job titles as preview\n",
    "                for j, job in enumerate(jobs[:3], 1):\n",
    "                    print(f\"   {j}. {job['title']} at {job['company']}\")\n",
    "                if len(jobs) > 3:\n",
    "                    print(f\"   ... and {len(jobs) - 3} more\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between searches\n",
    "            if i < len(job_roles):\n",
    "                delay = random.choice(list(range(5, 10)))\n",
    "                print(f\"‚è≥ Waiting {delay} seconds before next search...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        print(f\"\\nüéâ Scraping completed!\")\n",
    "        print(f\"üìä Total jobs found: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "        logging.error(f\"Direct scraping error: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        scraper.close()\n",
    "        print(\"üîí Browser closed\")\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "\"\"\"\n",
    "# Test with specific roles\n",
    "test_roles = [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\n",
    "jobs = scrape_jobs_directly(\n",
    "    job_roles=test_roles,\n",
    "    location=\"India\", \n",
    "    pages=1,\n",
    "    experience_level=\"Junior\",\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Save results\n",
    "if jobs:\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"direct_jobs_{timestamp}.csv\"\n",
    "    save_jobs_to_csv(jobs, filename)\n",
    "    print(f\"üíæ Saved {len(jobs)} jobs to {filename}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "860d0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Advanced Embedding Service for Job Matching\n",
    "class EmbeddingService:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding service with a sentence-transformer model\"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    \n",
    "    def create_resume_embeddings(self, resume_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create both global and section-based embeddings for resume (vectors + token sets)\n",
    "        \"\"\"\n",
    "        # Build texts\n",
    "        global_text = self._build_global_resume_text(resume_data)\n",
    "        skills_list = (resume_data.get('skills') or []) + (resume_data.get('extra_skills') or [])\n",
    "        experience_text = self._build_experience_text(resume_data)\n",
    "        certifications_list = resume_data.get('certifications') or []\n",
    "        \n",
    "        # Encode to vectors\n",
    "        vectors = {\n",
    "            'global': self._encode_text(global_text),\n",
    "            'skills': self._encode_text(' '.join(skills_list)),\n",
    "            'experience': self._encode_text(experience_text),\n",
    "            'certifications': self._encode_text(' '.join([c if isinstance(c, str) else c.get('certification', '') for c in certifications_list]))\n",
    "        }\n",
    "        \n",
    "        # Token sets for overlap\n",
    "        tokens = lambda s: {t.lower() for t in (s or '').split() if t}\n",
    "        overlap = {\n",
    "            'skills_set': {s.lower() for s in skills_list if isinstance(s, str)},\n",
    "            'experience_set': tokens(experience_text),\n",
    "            'certifications_set': { (c if isinstance(c, str) else c.get('certification', '')).lower() for c in certifications_list }\n",
    "        }\n",
    "        return { 'vectors': vectors, 'overlap': overlap }\n",
    "    \n",
    "    def create_job_embeddings(self, job_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create embeddings for job posting (vectors + token sets)\n",
    "        \"\"\"\n",
    "        title = job_data.get('title', '')\n",
    "        description = job_data.get('description', '')\n",
    "        company = job_data.get('company', '')\n",
    "        global_job_text = f\"{title} {description} {company}\".strip()\n",
    "        requirements_text = self._extract_requirements(description).strip()\n",
    "        if not requirements_text:\n",
    "            requirements_text = description.strip() or global_job_text\n",
    "        combined_job = f\"{title} {requirements_text}\".strip()\n",
    "        \n",
    "        vectors = {\n",
    "            'global': self._encode_text(global_job_text),\n",
    "            'requirements': self._encode_text(requirements_text),\n",
    "            'context': self._encode_text(combined_job)\n",
    "        }\n",
    "        \n",
    "        # naive skill tokens from title+description\n",
    "        tokens = lambda s: {t.lower() for t in (s or '').split() if t}\n",
    "        overlap = {\n",
    "            'skills_set': tokens(title + ' ' + requirements_text),\n",
    "            'experience_set': tokens(description)\n",
    "        }\n",
    "        return { 'vectors': vectors, 'overlap': overlap }\n",
    "    \n",
    "    def _zero_vec(self):\n",
    "        try:\n",
    "            probe = self.embeddings.embed_query(\" \")\n",
    "            return np.zeros(len(probe), dtype=float)\n",
    "        except Exception:\n",
    "            return np.zeros(384, dtype=float)  # common default for MiniLM\n",
    "    \n",
    "    def _encode_text(self, text: str):\n",
    "        text_clean = (text or \"\").strip()\n",
    "        if not text_clean or text_clean.lower() in {\"description not available\", \"n/a\"}:\n",
    "            return self._zero_vec()\n",
    "        vec = self.embeddings.embed_query(text_clean)\n",
    "        return np.array(vec, dtype=float)\n",
    "    \n",
    "    def _build_global_resume_text(self, resume_data: dict) -> str:\n",
    "        parts = []\n",
    "        if resume_data.get('summary'):\n",
    "            parts.append(resume_data['summary'])\n",
    "        if resume_data.get('skills'):\n",
    "            parts.extend(resume_data['skills'])\n",
    "        if resume_data.get('extra_skills'):\n",
    "            parts.extend(resume_data['extra_skills'])\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                parts.append(f\"{exp.get('job_title', '')} at {exp.get('company', '')}\")\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    parts.append(exp['description'])\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                parts.append(f\"Project: {project.get('project_name', '')}\")\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    parts.extend(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    parts.append(project['description'])\n",
    "        if resume_data.get('education'):\n",
    "            for edu in resume_data['education']:\n",
    "                parts.append(f\"{edu.get('degree', '')} from {edu.get('institution', '')}\")\n",
    "        return ' '.join(parts)\n",
    "    \n",
    "    def _build_skills_text(self, resume_data: dict) -> str:\n",
    "        skills_parts = []\n",
    "        if resume_data.get('skills'):\n",
    "            skills_parts.extend(resume_data['skills'])\n",
    "        if resume_data.get('extra_skills'):\n",
    "            skills_parts.extend(resume_data['extra_skills'])\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    skills_parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    skills_parts.append(exp['description'])\n",
    "        return ' '.join(skills_parts)\n",
    "    \n",
    "    def _build_experience_text(self, resume_data: dict) -> str:\n",
    "        experience_parts = []\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                exp_text = f\"{exp.get('job_title', '')} {exp.get('company', '')}\"\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    exp_text += ' ' + ' '.join(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    exp_text += ' ' + exp['description']\n",
    "                experience_parts.append(exp_text)\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                project_text = f\"Project: {project.get('project_name', '')}\"\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    project_text += ' ' + ' '.join(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    project_text += ' ' + project['description']\n",
    "                experience_parts.append(project_text)\n",
    "        return ' '.join(experience_parts)\n",
    "    \n",
    "    def _extract_requirements(self, job_description: str) -> str:\n",
    "        if not job_description:\n",
    "            return \"\"\n",
    "        requirements_keywords = [\n",
    "            'requirements', 'qualifications', 'skills', 'experience',\n",
    "            'must have', 'should have', 'preferred', 'bachelor', 'master',\n",
    "            'years of experience', 'proficient', 'knowledge of'\n",
    "        ]\n",
    "        sentences = job_description.split('.')\n",
    "        requirement_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            if any(keyword in sentence_lower for keyword in requirements_keywords):\n",
    "                requirement_sentences.append(sentence.strip())\n",
    "        # Fallback: first few sentences if none\n",
    "        if not requirement_sentences:\n",
    "            requirement_sentences = sentences[:3]\n",
    "        return ' '.join([s.strip() for s in requirement_sentences if s.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63cd7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Job Matching System with Weighted Similarity\n",
    "class JobMatchingService:\n",
    "    def __init__(self, embedding_service: EmbeddingService):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.weights = {\n",
    "            'global': 0.7,\n",
    "            'skills': 0.2,\n",
    "            'experience': 0.1\n",
    "        }\n",
    "    \n",
    "    def _jaccard(self, a_set: set, b_set: set) -> float:\n",
    "        if not a_set or not b_set:\n",
    "            return 0.0\n",
    "        inter = len(a_set & b_set)\n",
    "        union = len(a_set | b_set)\n",
    "        return inter / union if union else 0.0\n",
    "    \n",
    "    def calculate_similarity(self, resume_embeddings: dict, job_embeddings: dict) -> dict:\n",
    "        similarities = {}\n",
    "        try:\n",
    "            # Unpack vectors and overlap sets\n",
    "            r_vecs = resume_embeddings['vectors']\n",
    "            r_ov = resume_embeddings['overlap']\n",
    "            j_vecs = job_embeddings['vectors']\n",
    "            j_ov = job_embeddings['overlap']\n",
    "            \n",
    "            # Semantic similarities\n",
    "            global_sem = self._cosine(r_vecs['global'], j_vecs['global'])\n",
    "            skills_sem = self._cosine(r_vecs['skills'], j_vecs.get('requirements', j_vecs['global']))\n",
    "            exp_sem = self._cosine(r_vecs['experience'], j_vecs.get('context', j_vecs['global']))\n",
    "            cert_sem = self._cosine(r_vecs.get('certifications'), j_vecs.get('requirements'))\n",
    "            \n",
    "            # Overlap (lexical) similarities\n",
    "            skills_overlap = self._jaccard(r_ov['skills_set'], j_ov['skills_set'])\n",
    "            exp_overlap = self._jaccard(r_ov['experience_set'], j_ov['experience_set'])\n",
    "            cert_overlap = self._jaccard(r_ov['certifications_set'], j_ov.get('certifications_set', set()))\n",
    "            \n",
    "            # Blend semantics and overlap per channel\n",
    "            skills_score = 0.7 * skills_sem + 0.3 * skills_overlap\n",
    "            exp_score = 0.6 * exp_sem + 0.4 * exp_overlap\n",
    "            cert_score = 0.6 * cert_sem + 0.4 * cert_overlap\n",
    "            \n",
    "            similarities['global'] = global_sem\n",
    "            similarities['skills'] = skills_score\n",
    "            similarities['experience'] = exp_score\n",
    "            similarities['certifications'] = cert_score\n",
    "            \n",
    "            final_score = (\n",
    "                self.weights['global'] * similarities['global'] +\n",
    "                self.weights['skills'] * similarities['skills'] +\n",
    "                self.weights['experience'] * similarities['experience']\n",
    "            )\n",
    "            # small bonus for certifications\n",
    "            final_score = 0.95 * final_score + 0.05 * similarities['certifications']\n",
    "            \n",
    "            similarities['final_score'] = float(final_score)\n",
    "            return similarities\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {e}\")\n",
    "            return {'global': 0.0, 'skills': 0.0, 'experience': 0.0, 'final_score': 0.0, 'error': str(e)}\n",
    "    \n",
    "    def _ensure_vec(self, x) -> np.ndarray:\n",
    "        # Accept either vector or raw text; convert text to embedding\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return np.array(x, dtype=float)\n",
    "        if isinstance(x, str):\n",
    "            return self.embedding_service._encode_text(x)\n",
    "        return self.embedding_service._zero_vec()\n",
    "    \n",
    "    def _cosine(self, a, b) -> float:\n",
    "        a_vec = self._ensure_vec(a)\n",
    "        b_vec = self._ensure_vec(b)\n",
    "        a_norm = np.linalg.norm(a_vec)\n",
    "        b_norm = np.linalg.norm(b_vec)\n",
    "        if a_norm == 0.0 or b_norm == 0.0:\n",
    "            return 0.0\n",
    "        return float(np.dot(a_vec, b_vec) / (a_norm * b_norm))\n",
    "    \n",
    "    def match_jobs(self, resume_data: dict, jobs_df: pd.DataFrame, threshold: float = 0.2, top_n: int = 0) -> pd.DataFrame:\n",
    "        print(f\"üéØ Starting job matching for {len(jobs_df)} jobs...\")\n",
    "        print(f\"üìä Using threshold: {threshold}\")\n",
    "        \n",
    "        resume_embeddings = self.embedding_service.create_resume_embeddings(resume_data)\n",
    "        job_scores = []\n",
    "        for idx, job_row in jobs_df.iterrows():\n",
    "            job_data = {\n",
    "                'title': job_row.get('title', ''),\n",
    "                'description': job_row.get('description', ''),\n",
    "                'company': job_row.get('company', ''),\n",
    "                'location': job_row.get('location', ''),\n",
    "                'link': job_row.get('link', '')\n",
    "            }\n",
    "            job_embeddings = self.embedding_service.create_job_embeddings(job_data)\n",
    "            similarities = self.calculate_similarity(resume_embeddings, job_embeddings)\n",
    "            job_scores.append({\n",
    "                'job_index': idx,\n",
    "                'title': job_data['title'],\n",
    "                'company': job_data['company'],\n",
    "                'location': job_data['location'],\n",
    "                'link': job_data['link'],\n",
    "                'global_similarity': similarities['global'],\n",
    "                'skills_similarity': similarities['skills'],\n",
    "                'experience_similarity': similarities['experience'],\n",
    "                'final_score': similarities['final_score']\n",
    "            })\n",
    "        results_df = pd.DataFrame(job_scores)\n",
    "        filtered_df = results_df[results_df['final_score'] >= threshold].copy()\n",
    "        filtered_df = filtered_df.sort_values('final_score', ascending=False)\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} jobs above threshold {threshold}\")\n",
    "        return filtered_df\n",
    "    \n",
    "    def update_weights(self, global_weight: float = None, skills_weight: float = None, experience_weight: float = None):\n",
    "        if global_weight is not None:\n",
    "            self.weights['global'] = global_weight\n",
    "        if skills_weight is not None:\n",
    "            self.weights['skills'] = skills_weight\n",
    "        if experience_weight is not None:\n",
    "            self.weights['experience'] = experience_weight\n",
    "        total_weight = sum(self.weights.values())\n",
    "        if total_weight > 0:\n",
    "            for key in self.weights:\n",
    "                self.weights[key] = self.weights[key] / total_weight\n",
    "        print(f\"üîÑ Updated weights: {self.weights}\")\n",
    "\n",
    "# Initialize services\n",
    "embedding_service = EmbeddingService()\n",
    "job_matching_service = JobMatchingService(embedding_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a3755d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION-EMBEDDING-BASED GRANULAR JOB MATCHING (No synonym matching)\n",
    "# ============================================================================\n",
    "# Matches resume to jobs by embedding semantically-related sections from both sides\n",
    "# and combining section-wise cosine similarities. No hardcoded synonym lists.\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "\n",
    "class SectionEmbeddingMatcher:\n",
    "    def __init__(self, embedding_service: EmbeddingService):\n",
    "        self.embedding_service = embedding_service\n",
    "        # Final score is a weighted sum of section-wise similarities\n",
    "        self.section_weights = {\n",
    "            'skills': 0.35,\n",
    "            'experience': 0.25,\n",
    "            'role_alignment': 0.20,\n",
    "            'education': 0.10,\n",
    "            'location': 0.05,\n",
    "            'company_culture': 0.05,\n",
    "        }\n",
    "\n",
    "    # ------------ Section extraction (resume) ------------\n",
    "    def _resume_sections(self, resume: dict) -> Dict[str, str]:\n",
    "        # skills\n",
    "        skills = []\n",
    "        if resume.get('skills'): skills.extend(resume['skills'])\n",
    "        if resume.get('extra_skills'): skills.extend(resume['extra_skills'])\n",
    "        skills_text = ' '.join([s for s in skills if isinstance(s, str)])\n",
    "\n",
    "        # experience (work + projects)\n",
    "        exp_parts = []\n",
    "        for exp in (resume.get('work_experience') or []):\n",
    "            title = exp.get('job_title', '')\n",
    "            comp = exp.get('company', '')\n",
    "            resp = exp.get('responsibilities') or exp.get('description')\n",
    "            if isinstance(resp, list): resp = ' '.join(resp)\n",
    "            exp_parts.append(f\"{title} at {comp} {resp or ''}\")\n",
    "        for prj in (resume.get('projects') or []):\n",
    "            name = prj.get('project_name', '')\n",
    "            desc = prj.get('description')\n",
    "            if isinstance(desc, list): desc = ' '.join(desc)\n",
    "            exp_parts.append(f\"Project {name} {desc or ''}\")\n",
    "        experience_text = ' '.join(exp_parts)\n",
    "\n",
    "        # role alignment (summary + recommended roles)\n",
    "        role_parts = []\n",
    "        if resume.get('summary'): role_parts.append(resume['summary'])\n",
    "        if resume.get('recommended_roles'): role_parts.extend(resume['recommended_roles'])\n",
    "        role_text = ' '.join(role_parts)\n",
    "\n",
    "        # education\n",
    "        edu_parts = []\n",
    "        for edu in (resume.get('education') or []):\n",
    "            deg = edu.get('degree', '')\n",
    "            inst = edu.get('institution', '')\n",
    "            fos = edu.get('field_of_study') or ''\n",
    "            edu_parts.append(f\"{deg} {fos} {inst}\")\n",
    "        education_text = ' '.join(edu_parts)\n",
    "\n",
    "        # certifications\n",
    "        cert_parts = []\n",
    "        for cert in (resume.get('certifications') or []):\n",
    "            if isinstance(cert, dict):\n",
    "                cert_parts.append(f\"{cert.get('certification','')} {cert.get('issuing_organization','')}\")\n",
    "            else:\n",
    "                cert_parts.append(str(cert))\n",
    "        certifications_text = ' '.join(cert_parts)\n",
    "\n",
    "        return {\n",
    "            'skills': skills_text.strip(),\n",
    "            'experience': experience_text.strip(),\n",
    "            'role_alignment': role_text.strip(),\n",
    "            'education': education_text.strip(),\n",
    "            'certifications': certifications_text.strip(),\n",
    "            'location_value': (resume.get('location') or '').strip(),\n",
    "        }\n",
    "\n",
    "    # ------------ Section extraction (job) ------------\n",
    "    def _extract_sentences_by_keywords(self, text: str, keywords: list, fallback_count: int = 3) -> str:\n",
    "        if not text: return ''\n",
    "        sentences = [s.strip() for s in text.split('.')]\n",
    "        picked = [s for s in sentences if any(k in s.lower() for k in keywords)]\n",
    "        if not picked:\n",
    "            picked = sentences[:fallback_count]\n",
    "        return ' '.join([s for s in picked if s])\n",
    "\n",
    "    def _job_sections(self, job: dict) -> Dict[str, str]:\n",
    "        title = job.get('title', '')\n",
    "        desc = job.get('description', '')\n",
    "        company = job.get('company', '')\n",
    "\n",
    "        # requirements/skills\n",
    "        req_text = self._extract_sentences_by_keywords(\n",
    "            desc,\n",
    "            keywords=['requirements','qualifications','skills','must have','should have','experience with','knowledge of','proficient','preferred']\n",
    "        )\n",
    "\n",
    "        # experience expectations\n",
    "        exp_text = self._extract_sentences_by_keywords(\n",
    "            desc,\n",
    "            keywords=['years of experience','experience in','minimum','at least','entry level','junior','senior','lead','principal','intern']\n",
    "        )\n",
    "\n",
    "        # responsibilities + title for role alignment\n",
    "        resp_text = self._extract_sentences_by_keywords(\n",
    "            desc,\n",
    "            keywords=['responsibilities','duties','role','will be responsible','key responsibilities','primary role']\n",
    "        )\n",
    "        role_text = f\"{title} {resp_text}\".strip()\n",
    "\n",
    "        # education requirements\n",
    "        edu_text = self._extract_sentences_by_keywords(\n",
    "            desc,\n",
    "            keywords=['bachelor','master','phd','degree','diploma','certification','btech','mtech','be','me','bs','ms','mba','education']\n",
    "        )\n",
    "\n",
    "        # culture/environment\n",
    "        culture_text = self._extract_sentences_by_keywords(\n",
    "            desc,\n",
    "            keywords=['culture','environment','team','collaborative','innovative','startup','fast-paced','remote','flexible','work-life balance','growth','learning','mentorship']\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'skills': req_text,\n",
    "            'experience': exp_text,\n",
    "            'role_alignment': role_text,\n",
    "            'education': edu_text,\n",
    "            'company_culture': f\"{company} {culture_text}\".strip(),\n",
    "            'location_value': (job.get('location') or '').strip(),\n",
    "        }\n",
    "\n",
    "    # ------------ Embeddings and similarity ------------\n",
    "    def _embed_sections(self, sections: Dict[str, str]) -> Dict[str, np.ndarray]:\n",
    "        out = {}\n",
    "        for name, text in sections.items():\n",
    "            if name.endswith('_value'):\n",
    "                continue\n",
    "            out[name] = self.embedding_service._encode_text(text)\n",
    "        return out\n",
    "\n",
    "    def _cosine(self, a: np.ndarray, b: np.ndarray) -> float:\n",
    "        a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "        na = np.linalg.norm(a); nb = np.linalg.norm(b)\n",
    "        if na == 0.0 or nb == 0.0: return 0.0\n",
    "        return float(np.dot(a, b) / (na * nb))\n",
    "\n",
    "    def _location_score(self, resume_loc: str, job_loc: str) -> float:\n",
    "        if not resume_loc or not job_loc: return 0.5\n",
    "        rl = re.sub(r'\\s+', ' ', resume_loc.strip().lower())\n",
    "        jl = re.sub(r'\\s+', ' ', job_loc.strip().lower())\n",
    "        if rl == jl: return 1.0\n",
    "        rc = rl.split(',')[0].strip(); jc = jl.split(',')[0].strip()\n",
    "        if rc and jc and rc == jc: return 0.8\n",
    "        if any(x in jl for x in ['remote','wfh','work from home','hybrid']): return 0.7\n",
    "        rcn = rl.split(',')[-1].strip(); jcn = jl.split(',')[-1].strip()\n",
    "        if rcn and jcn and rcn == jcn: return 0.6\n",
    "        return 0.3\n",
    "\n",
    "    # ------------ Public API ------------\n",
    "    def score_job(self, resume: dict, job: dict) -> Dict[str, Any]:\n",
    "        r_secs = self._resume_sections(resume)\n",
    "        j_secs = self._job_sections(job)\n",
    "        r_emb = self._embed_sections(r_secs)\n",
    "        j_emb = self._embed_sections(j_secs)\n",
    "\n",
    "        section_scores = {\n",
    "            'skills': self._cosine(r_emb.get('skills', np.zeros(1)), j_emb.get('skills', np.zeros(1))),\n",
    "            'experience': self._cosine(r_emb.get('experience', np.zeros(1)), j_emb.get('experience', np.zeros(1))),\n",
    "            'role_alignment': self._cosine(r_emb.get('role_alignment', np.zeros(1)), j_emb.get('role_alignment', np.zeros(1))),\n",
    "            'education': self._cosine(r_emb.get('education', np.zeros(1)), j_emb.get('education', np.zeros(1))),\n",
    "            'company_culture': self._cosine(r_emb.get('experience', np.zeros(1)), j_emb.get('company_culture', np.zeros(1))),\n",
    "            'location': self._location_score(r_secs.get('location_value',''), j_secs.get('location_value','')),\n",
    "        }\n",
    "\n",
    "        final_score = sum(self.section_weights[k] * section_scores.get(k, 0.0) for k in self.section_weights)\n",
    "        return {\n",
    "            'final_score': float(min(final_score, 1.0)),\n",
    "            'sections': section_scores,\n",
    "        }\n",
    "\n",
    "    def match_jobs(self, resume: dict, jobs_df: pd.DataFrame, threshold: float = 0.3, top_n: int = 10) -> pd.DataFrame:\n",
    "        rows = []\n",
    "        for idx, row in jobs_df.iterrows():\n",
    "            job = {\n",
    "                'title': row.get('title',''),\n",
    "                'description': row.get('description',''),\n",
    "                'company': row.get('company',''),\n",
    "                'location': row.get('location',''),\n",
    "                'link': row.get('link',''),\n",
    "            }\n",
    "            scored = self.score_job(resume, job)\n",
    "            rows.append({\n",
    "                'job_index': idx,\n",
    "                'title': job['title'],\n",
    "                'company': job['company'],\n",
    "                'location': job['location'],\n",
    "                'link': job['link'],\n",
    "                'final_score': scored['final_score'],\n",
    "                'skills_score': scored['sections']['skills'],\n",
    "                'experience_score': scored['sections']['experience'],\n",
    "                'role_alignment_score': scored['sections']['role_alignment'],\n",
    "                'education_score': scored['sections']['education'],\n",
    "                'company_culture_score': scored['sections']['company_culture'],\n",
    "                'location_score': scored['sections']['location'],\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        df = df[df['final_score'] >= threshold].sort_values('final_score', ascending=False)\n",
    "        if top_n > 0: df = df.head(top_n)\n",
    "        return df\n",
    "\n",
    "# Initialize section-embedding matcher\n",
    "section_embedding_matcher = SectionEmbeddingMatcher(embedding_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f182f5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Scoring + Persist Demo (no apply)\n",
      "üíæ Scored CSV: jobs_Yeswanth_Yerra_20251009_123332_scored.csv (rows: 89)\n",
      "üíæ Shortlist CSV (>= 0.4): jobs_Yeswanth_Yerra_20251009_123332_shortlist_04.csv (rows: 5)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCORING: Compute and persist section-embedding scores into jobs CSV\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Audit logger for apply flow\n",
    "logging.basicConfig(\n",
    "    filename='apply_audit.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(message)s'\n",
    ")\n",
    "\n",
    "\n",
    "def compute_and_persist_scores(\n",
    "    input_csv_path: str,\n",
    "    resume_data: dict,\n",
    "    output_csv_path: str | None = None,\n",
    "    threshold: float = 0.4,\n",
    "    matcher: SectionEmbeddingMatcher | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Read jobs CSV, compute section-embedding scores, persist into a new CSV with\n",
    "    appended score columns, and also write a shortlist CSV of jobs >= threshold.\n",
    "    Returns paths to written files and counts.\n",
    "    \"\"\"\n",
    "    if matcher is None:\n",
    "        matcher = section_embedding_matcher\n",
    "\n",
    "    if not os.path.exists(input_csv_path):\n",
    "        raise FileNotFoundError(f\"Jobs CSV not found: {input_csv_path}\")\n",
    "\n",
    "    jobs_df = pd.read_csv(input_csv_path)\n",
    "\n",
    "    # Compute scores\n",
    "    scored_df = matcher.match_jobs(\n",
    "        resume=resume_data,\n",
    "        jobs_df=jobs_df,\n",
    "        threshold=0.0,   # compute for all, we will filter later\n",
    "        top_n=0\n",
    "    )\n",
    "\n",
    "    # Merge back scores with original rows using job_index\n",
    "    merged = jobs_df.copy()\n",
    "    merged = merged.reset_index().rename(columns={'index': 'orig_index'})\n",
    "    scored_df = scored_df.rename(columns={'job_index': 'orig_index'})\n",
    "    merged = merged.merge(\n",
    "        scored_df[\n",
    "            [\n",
    "                'orig_index','final_score','skills_score','experience_score',\n",
    "                'role_alignment_score','education_score','company_culture_score','location_score'\n",
    "            ]\n",
    "        ],\n",
    "        on='orig_index', how='left'\n",
    "    )\n",
    "\n",
    "    # Output file paths\n",
    "    if output_csv_path is None:\n",
    "        base, ext = os.path.splitext(input_csv_path)\n",
    "        output_csv_path = f\"{base}_scored{ext}\"\n",
    "    shortlist_csv_path = output_csv_path.replace('_scored', f'_shortlist_{str(threshold).replace(\".\", \"\")}')\n",
    "\n",
    "    # Persist scored file\n",
    "    merged.to_csv(output_csv_path, index=False)\n",
    "\n",
    "    # Create and persist shortlist\n",
    "    shortlist_df = merged[(merged['final_score'].fillna(0.0) >= threshold)].copy()\n",
    "    shortlist_df.to_csv(shortlist_csv_path, index=False)\n",
    "\n",
    "    print(f\"üíæ Scored CSV: {output_csv_path} (rows: {len(merged)})\")\n",
    "    print(f\"üíæ Shortlist CSV (>= {threshold}): {shortlist_csv_path} (rows: {len(shortlist_df)})\")\n",
    "\n",
    "    return {\n",
    "        'scored_csv': output_csv_path,\n",
    "        'shortlist_csv': shortlist_csv_path,\n",
    "        'total_rows': len(merged),\n",
    "        'shortlist_count': len(shortlist_df)\n",
    "    }\n",
    "\n",
    "# Demo: compute and persist using latest CSV if resume is available\n",
    "print(\"üß™ Scoring + Persist Demo (no apply)\")\n",
    "try:\n",
    "    current_resume = None\n",
    "    if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "        current_resume = structured_output\n",
    "    elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "        current_resume = result['resume_analysis']\n",
    "\n",
    "    if current_resume:\n",
    "        latest_jobs_csv = 'jobs_Yeswanth_Yerra_20251009_123332.csv'\n",
    "        if os.path.exists(latest_jobs_csv):\n",
    "            _persist_info = compute_and_persist_scores(\n",
    "                input_csv_path=latest_jobs_csv,\n",
    "                resume_data=current_resume,\n",
    "                threshold=0.4\n",
    "            )\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Jobs CSV not found: {latest_jobs_csv}. Skip demo run.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No resume data (structured_output/result). Run parsing first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in scoring demo: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6463d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PLAYWRIGHT UTILITIES: Save LinkedIn session and gated apply flow\n",
    "# ============================================================================\n",
    "# Secondary path only; requires explicit user consent and secure storage_state.\n",
    "\n",
    "from playwright.sync_api import sync_playwright\n",
    "\n",
    "\n",
    "def login_and_save_linkedin_session(username: str, password: str, storage_path: str = \"linkedin_storage.json\") -> str:\n",
    "    \"\"\"\n",
    "    Login to LinkedIn with credentials, handle 2FA/CAPTCHA if needed, then persist storage_state.\n",
    "    Returns storage_path on success, raises exception on failure.\n",
    "    \"\"\"\n",
    "    print(f\"üîê Logging into LinkedIn with username: {username}\")\n",
    "    \n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=False)\n",
    "        context = browser.new_context()\n",
    "        page = context.new_page()\n",
    "        \n",
    "        try:\n",
    "            # Navigate to LinkedIn login\n",
    "            page.goto(\"https://www.linkedin.com/login\", wait_until=\"networkidle\")\n",
    "            \n",
    "            # Fill login form\n",
    "            page.fill('input[name=\"session_key\"]', username)\n",
    "            page.fill('input[name=\"session_password\"]', password)\n",
    "            page.click('button[type=\"submit\"]')\n",
    "            \n",
    "            # Wait for login to process\n",
    "            page.wait_for_load_state(\"networkidle\", timeout=10000)\n",
    "            \n",
    "            # Check if login was successful by looking for common post-login elements\n",
    "            login_success = False\n",
    "            \n",
    "            # Check for various success indicators\n",
    "            success_indicators = [\n",
    "                \"button[data-control-name='nav.settings_and_privacy']\",  # Settings button\n",
    "                \"input[placeholder*='Search']\",  # Search box\n",
    "                \"button[data-control-name='nav.skynet_header_profile']\",  # Profile button\n",
    "                \".global-nav__me\"  # User menu\n",
    "            ]\n",
    "            \n",
    "            for indicator in success_indicators:\n",
    "                try:\n",
    "                    page.wait_for_selector(indicator, timeout=5000)\n",
    "                    login_success = True\n",
    "                    print(\"‚úÖ Login successful - found success indicator\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Check for 2FA or verification challenges\n",
    "            if not login_success:\n",
    "                # Look for 2FA challenge\n",
    "                try:\n",
    "                    page.wait_for_selector('input[name=\"pin\"]', timeout=3000)\n",
    "                    print(\"üîê 2FA detected - please complete verification manually\")\n",
    "                    print(\"‚è≥ Waiting for you to complete 2FA... (30 seconds)\")\n",
    "                    page.wait_for_selector(success_indicators[0], timeout=30000)\n",
    "                    login_success = True\n",
    "                    print(\"‚úÖ 2FA completed successfully\")\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Check for CAPTCHA\n",
    "            if not login_success:\n",
    "                captcha_selectors = [\n",
    "                    \"iframe[src*='captcha']\",\n",
    "                    \".captcha\",\n",
    "                    \"[data-test-id='captcha']\"\n",
    "                ]\n",
    "                for selector in captcha_selectors:\n",
    "                    if page.locator(selector).count() > 0:\n",
    "                        print(\"ü§ñ CAPTCHA detected - please solve manually\")\n",
    "                        print(\"‚è≥ Waiting for you to solve CAPTCHA... (60 seconds)\")\n",
    "                        try:\n",
    "                            page.wait_for_selector(success_indicators[0], timeout=60000)\n",
    "                            login_success = True\n",
    "                            print(\"‚úÖ CAPTCHA solved successfully\")\n",
    "                        except:\n",
    "                            pass\n",
    "                        break\n",
    "            \n",
    "            if not login_success:\n",
    "                # Check current URL to see if we're redirected to feed\n",
    "                current_url = page.url\n",
    "                if \"feed\" in current_url or \"linkedin.com/in/\" in current_url:\n",
    "                    login_success = True\n",
    "                    print(\"‚úÖ Login successful - redirected to feed/profile\")\n",
    "                else:\n",
    "                    raise Exception(\"Login failed - could not verify successful login\")\n",
    "            \n",
    "            # Save session state\n",
    "            context.storage_state(path=storage_path)\n",
    "            print(f\"‚úÖ LinkedIn session saved to {storage_path}\")\n",
    "            \n",
    "            # Test the saved session by trying to access a protected page\n",
    "            print(\"üß™ Testing saved session...\")\n",
    "            test_page = context.new_page()\n",
    "            test_page.goto(\"https://www.linkedin.com/feed/\")\n",
    "            test_page.wait_for_load_state(\"networkidle\", timeout=5000)\n",
    "            test_page.close()\n",
    "            print(\"‚úÖ Session test successful\")\n",
    "            \n",
    "            browser.close()\n",
    "            return storage_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Login failed: {str(e)}\")\n",
    "            browser.close()\n",
    "            raise Exception(f\"LinkedIn login failed: {str(e)}\")\n",
    "\n",
    "\n",
    "def check_session_validity(storage_path: str = \"linkedin_storage.json\") -> bool:\n",
    "    \"\"\"\n",
    "    Check if the saved session is still valid by trying to access a protected page.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(storage_path):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        with sync_playwright() as p:\n",
    "            browser = p.chromium.launch(headless=True)\n",
    "            context = browser.new_context(storage_state=storage_path)\n",
    "            page = context.new_page()\n",
    "            page.goto(\"https://www.linkedin.com/feed/\", timeout=10000)\n",
    "            page.wait_for_load_state(\"networkidle\", timeout=5000)\n",
    "            \n",
    "            # Check if we're actually logged in by looking for login indicators\n",
    "            login_indicators = [\n",
    "                \"button[data-control-name='nav.settings_and_privacy']\",\n",
    "                \"input[placeholder*='Search']\",\n",
    "                \".global-nav__me\"\n",
    "            ]\n",
    "            \n",
    "            for indicator in login_indicators:\n",
    "                if page.locator(indicator).count() > 0:\n",
    "                    browser.close()\n",
    "                    return True\n",
    "            \n",
    "            browser.close()\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Session validation failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def apply_with_saved_session(job_url: str, storage_path: str = \"linkedin_storage.json\", dry_run: bool = True, max_steps: int = 50) -> dict:\n",
    "    \"\"\"\n",
    "    Use saved LinkedIn storage_state to open a job link and initiate application.\n",
    "    - dry_run=True: navigate and detect buttons, but do not submit.\n",
    "    - Human-in-loop checkpoints for long/ambiguous forms.\n",
    "    - Returns an audit dict; also logs to apply_audit.log\n",
    "    \"\"\"\n",
    "    audit = {\"job_url\": job_url, \"storage_path\": storage_path, \"dry_run\": dry_run, \"status\": \"started\"}\n",
    "    logging.info(f\"APPLY_START url={job_url} dry_run={dry_run}\")\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        context = browser.new_context(storage_state=storage_path)\n",
    "        page = context.new_page()\n",
    "        page.goto(job_url, wait_until=\"networkidle\")\n",
    "\n",
    "        # Try common apply buttons; selectors vary by page\n",
    "        selectors = [\n",
    "            \"button[aria-label*='Easy Apply']\",\n",
    "            \"button:has-text('Easy Apply')\",\n",
    "            \"button[data-control-name='apply']\",\n",
    "            \"a[data-control-name='jobdetails_topcard_inapply']\",\n",
    "        ]\n",
    "        clicked = False\n",
    "        for sel in selectors:\n",
    "            try:\n",
    "                page.wait_for_selector(sel, timeout=3000)\n",
    "                page.click(sel)\n",
    "                clicked = True\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if not clicked:\n",
    "            audit[\"status\"] = \"no_apply_button\"\n",
    "            logging.info(f\"APPLY_NO_BUTTON url={job_url}\")\n",
    "            browser.close()\n",
    "            return audit\n",
    "\n",
    "        # If not dry run, proceed cautiously with limited steps\n",
    "        if not dry_run:\n",
    "            step = 0\n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                try:\n",
    "                    # Try to find Next/Submit\n",
    "                    if page.locator(\"button:has-text('Submit')\").count() > 0:\n",
    "                        page.click(\"button:has-text('Submit')\")\n",
    "                        audit[\"status\"] = \"submitted\"\n",
    "                        logging.info(f\"APPLY_SUBMITTED url={job_url}\")\n",
    "                        break\n",
    "                    elif page.locator(\"button:has-text('Next')\").count() > 0:\n",
    "                        page.click(\"button:has-text('Next')\")\n",
    "                    else:\n",
    "                        # Human-in-loop recommended here for complex forms\n",
    "                        audit[\"status\"] = \"needs_human_review\"\n",
    "                        logging.info(f\"APPLY_HUMAN_REVIEW url={job_url}\")\n",
    "                        break\n",
    "                except Exception:\n",
    "                    audit[\"status\"] = \"interaction_error\"\n",
    "                    logging.exception(f\"APPLY_INTERACTION_ERROR url={job_url}\")\n",
    "                    break\n",
    "        else:\n",
    "            audit[\"status\"] = \"dry_run_clicked\"\n",
    "            logging.info(f\"APPLY_DRY_RUN_CLICKED url={job_url}\")\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "    return audit\n",
    "\n",
    "\n",
    "def apply_to_shortlisted(shortlist_csv_path: str, storage_path: str, limit: int = 5, dry_run: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Iterate over shortlisted jobs and attempt apply flow with rate limits and logging.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(shortlist_csv_path):\n",
    "        raise FileNotFoundError(f\"Shortlist CSV not found: {shortlist_csv_path}\")\n",
    "    df = pd.read_csv(shortlist_csv_path)\n",
    "    audits = []\n",
    "    count = 0\n",
    "    for _, row in df.iterrows():\n",
    "        if count >= limit: break\n",
    "        link = str(row.get('link') or '').strip()\n",
    "        if not link or link == 'N/A':\n",
    "            continue\n",
    "        audit = apply_with_saved_session(job_url=link, storage_path=storage_path, dry_run=dry_run)\n",
    "        audits.append(audit)\n",
    "        count += 1\n",
    "    print(f\"üìù Applied to {len(audits)} jobs (dry_run={dry_run}). See apply_audit.log for details.\")\n",
    "    return audits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56940f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Complete Job Application Flow Demo\n",
      "============================================================\n",
      "‚ùå No resume data found. Run resume parsing first.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DEMO: End-to-end shortlist + login + apply flow\n",
    "# ============================================================================\n",
    "\n",
    "LATEST_JOBS_CSV = 'jobs_Yeswanth_Yerra_20251009_123332.csv'\n",
    "THRESHOLD = 0.4\n",
    "STORAGE_STATE_PATH = 'linkedin_storage.json'\n",
    "\n",
    "print('üöÄ Complete Job Application Flow Demo')\n",
    "print('='*60)\n",
    "\n",
    "try:\n",
    "    # Step 1: Prepare resume data and compute scores\n",
    "    current_resume = None\n",
    "    if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "        current_resume = structured_output\n",
    "    elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "        current_resume = result['resume_analysis']\n",
    "\n",
    "    if not current_resume:\n",
    "        print('‚ùå No resume data found. Run resume parsing first.')\n",
    "    elif not os.path.exists(LATEST_JOBS_CSV):\n",
    "        print(f'‚ùå Jobs CSV not found: {LATEST_JOBS_CSV}')\n",
    "    else:\n",
    "        print('üìä Step 1: Computing job matching scores...')\n",
    "        info = compute_and_persist_scores(\n",
    "            input_csv_path=LATEST_JOBS_CSV,\n",
    "            resume_data=current_resume,\n",
    "            threshold=THRESHOLD\n",
    "        )\n",
    "        shortlist_csv = info['shortlist_csv']\n",
    "        print(f\"‚úÖ Shortlist ready: {shortlist_csv} ({info['shortlist_count']} jobs)\")\n",
    "        \n",
    "        # Step 2: Check session or login\n",
    "        print('\\nüîê Step 2: LinkedIn session management...')\n",
    "        \n",
    "        if os.path.exists(STORAGE_STATE_PATH):\n",
    "            print(f\"üìÅ Found existing session: {STORAGE_STATE_PATH}\")\n",
    "            if check_session_validity(STORAGE_STATE_PATH):\n",
    "                print(\"‚úÖ Existing session is valid\")\n",
    "                session_ready = True\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è Existing session is invalid or expired\")\n",
    "                session_ready = False\n",
    "        else:\n",
    "            print(\"üìÅ No existing session found\")\n",
    "            session_ready = False\n",
    "        \n",
    "        if not session_ready:\n",
    "            print(\"\\nüîë To login and save session, run:\")\n",
    "            if check_linkedin_credentials():\n",
    "                print(\"   quick_login_and_apply_from_env(apply_limit=3, dry_run=True)\")\n",
    "                print(\"   # This will use credentials from .env and handle 2FA/CAPTCHA if needed\")\n",
    "            else:\n",
    "                print(\"   login_and_save_linkedin_session('your_email@example.com', 'your_password')\")\n",
    "                print(\"   # This will handle 2FA/CAPTCHA if needed\")\n",
    "        \n",
    "        # Step 3: Apply to jobs (dry run by default)\n",
    "        print(f'\\nüìù Step 3: Job application (dry run)...')\n",
    "        \n",
    "        if session_ready:\n",
    "            audits = apply_to_shortlisted(\n",
    "                shortlist_csv_path=shortlist_csv, \n",
    "                storage_path=STORAGE_STATE_PATH, \n",
    "                limit=3, \n",
    "                dry_run=True\n",
    "            )\n",
    "            print(f\"‚úÖ Completed dry-run on {len(audits)} jobs\")\n",
    "            \n",
    "            # Show summary\n",
    "            status_counts = {}\n",
    "            for audit in audits:\n",
    "                status = audit.get('status', 'unknown')\n",
    "                status_counts[status] = status_counts.get(status, 0) + 1\n",
    "            \n",
    "            print(\"\\nüìä Application Summary:\")\n",
    "            for status, count in status_counts.items():\n",
    "                print(f\"   {status}: {count} jobs\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Cannot apply without valid session. Please login first.\")\n",
    "            \n",
    "        print(f\"\\nüéØ Next Steps:\")\n",
    "        if check_linkedin_credentials():\n",
    "            print(f\"   1. Login & Apply: quick_login_and_apply_from_env(apply_limit=5, dry_run=False)\")\n",
    "        else:\n",
    "            print(f\"   1. Login: login_and_save_linkedin_session('email', 'password')\")\n",
    "            print(f\"   2. Apply: apply_to_shortlisted('{shortlist_csv}', '{STORAGE_STATE_PATH}', limit=5, dry_run=False)\")\n",
    "        print(f\"   3. Check logs: apply_audit.log\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Demo error: {e}')\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf3eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Check if credentials are available:\\ncheck_linkedin_credentials()\\n\\n# For dry run using .env credentials (recommended):\\nquick_login_and_apply_from_env(apply_limit=2, dry_run=True)\\n\\n# For real applications using .env credentials:\\nquick_login_and_apply_from_env(apply_limit=5, dry_run=False)\\n\\n# Or use the original function (will fallback to .env if no credentials provided):\\nquick_login_and_apply(apply_limit=3, dry_run=True)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# HELPER: Easy login and apply workflow (using .env credentials)\n",
    "# ============================================================================\n",
    "\n",
    "def quick_login_and_apply_from_env(apply_limit: int = 3, dry_run: bool = True):\n",
    "    \"\"\"\n",
    "    One-step function to login using .env credentials and apply to shortlisted jobs.\n",
    "    \n",
    "    Args:\n",
    "        apply_limit: Number of jobs to apply to\n",
    "        dry_run: If True, only simulate applications\n",
    "    \"\"\"\n",
    "    # Read credentials from .env\n",
    "    linkedin_email = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "    linkedin_password = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "    \n",
    "    if not linkedin_email or not linkedin_password:\n",
    "        print(\"‚ùå LinkedIn credentials not found in .env file\")\n",
    "        print(\"Please add LINKEDIN_EMAIL and LINKEDIN_PASSWORD to your .env file\")\n",
    "        return []\n",
    "    \n",
    "    return quick_login_and_apply(linkedin_email, linkedin_password, apply_limit, dry_run)\n",
    "\n",
    "\n",
    "def quick_login_and_apply(email: str = None, password: str = None, apply_limit: int = 3, dry_run: bool = True):\n",
    "    \"\"\"\n",
    "    One-step function to login, validate session, and apply to shortlisted jobs.\n",
    "    \n",
    "    Args:\n",
    "        email: LinkedIn email (if None, will try to read from .env)\n",
    "        password: LinkedIn password (if None, will try to read from .env)\n",
    "        apply_limit: Number of jobs to apply to\n",
    "        dry_run: If True, only simulate applications\n",
    "    \"\"\"\n",
    "    # If credentials not provided, try to read from .env\n",
    "    if not email or not password:\n",
    "        linkedin_email = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "        linkedin_password = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "        \n",
    "        if not linkedin_email or not linkedin_password:\n",
    "            print(\"‚ùå LinkedIn credentials not provided and not found in .env file\")\n",
    "            print(\"Please provide credentials or add LINKEDIN_EMAIL and LINKEDIN_PASSWORD to your .env file\")\n",
    "            return []\n",
    "        \n",
    "        email = linkedin_email\n",
    "        password = linkedin_password\n",
    "        print(\"üìß Using LinkedIn credentials from .env file\")\n",
    "    \n",
    "    storage_path = \"linkedin_storage.json\"\n",
    "    \n",
    "    try:\n",
    "        print(\"üîê Logging into LinkedIn...\")\n",
    "        login_and_save_linkedin_session(email, password, storage_path)\n",
    "        \n",
    "        print(\"üß™ Validating session...\")\n",
    "        if not check_session_validity(storage_path):\n",
    "            raise Exception(\"Session validation failed after login\")\n",
    "        \n",
    "        print(\"üìù Finding shortlist...\")\n",
    "        shortlist_files = [f for f in os.listdir('.') if f.endswith('_shortlist_04.csv')]\n",
    "        if not shortlist_files:\n",
    "            raise Exception(\"No shortlist found. Run the scoring demo first.\")\n",
    "        \n",
    "        shortlist_csv = shortlist_files[0]  # Use the first shortlist found\n",
    "        print(f\"üìÑ Using shortlist: {shortlist_csv}\")\n",
    "        \n",
    "        print(f\"üöÄ Applying to {apply_limit} jobs (dry_run={dry_run})...\")\n",
    "        audits = apply_to_shortlisted(\n",
    "            shortlist_csv_path=shortlist_csv,\n",
    "            storage_path=storage_path,\n",
    "            limit=apply_limit,\n",
    "            dry_run=dry_run\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Completed applications to {len(audits)} jobs\")\n",
    "        \n",
    "        # Summary\n",
    "        status_counts = {}\n",
    "        for audit in audits:\n",
    "            status = audit.get('status', 'unknown')\n",
    "            status_counts[status] = status_counts.get(status, 0) + 1\n",
    "        \n",
    "        print(\"\\nüìä Results:\")\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"   {status}: {count} jobs\")\n",
    "            \n",
    "        return audits\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def check_linkedin_credentials():\n",
    "    \"\"\"Check if LinkedIn credentials are available in .env\"\"\"\n",
    "    email = os.getenv(\"LINKEDIN_EMAIL\")\n",
    "    password = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "    \n",
    "    if email and password:\n",
    "        print(\"‚úÖ LinkedIn credentials found in .env file\")\n",
    "        print(f\"üìß Email: {email}\")\n",
    "        print(f\"üîë Password: {'*' * len(password)}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå LinkedIn credentials not found in .env file\")\n",
    "        print(\"Please add the following to your .env file:\")\n",
    "        print(\"LINKEDIN_EMAIL=your_email@example.com\")\n",
    "        print(\"LINKEDIN_PASSWORD=your_password\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Check if credentials are available:\n",
    "check_linkedin_credentials()\n",
    "\n",
    "# For dry run using .env credentials (recommended):\n",
    "quick_login_and_apply_from_env(apply_limit=2, dry_run=True)\n",
    "\n",
    "# For real applications using .env credentials:\n",
    "quick_login_and_apply_from_env(apply_limit=5, dry_run=False)\n",
    "\n",
    "# Or use the original function (will fallback to .env if no credentials provided):\n",
    "quick_login_and_apply(apply_limit=3, dry_run=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23772f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Job Matching Demo...\n",
      "üöÄ AI Career System - Advanced Job Matching Demo\n",
      "======================================================================\n",
      "‚úÖ Loaded 89 jobs from jobs_Yeswanth_Yerra_20251009_123332.csv\n",
      "\n",
      "üìä Sample Job Data:\n",
      "Columns: ['title', 'company', 'location', 'link', 'description', 'searched_for', 'experience_level_filter', 'days_back', 'scraped_at']\n",
      "First job title: Tester Fresher\n",
      "First company: Crisil\n",
      "‚úÖ Using resume data from result['resume_analysis']\n",
      "\n",
      "üë§ Resume Analysis:\n",
      "Name: Yeswanth Yerra\n",
      "Experience Level: Entry Level\n",
      "Key Skills: ['Java', 'JavaScript', 'Python', 'Data Structures & Algorithms', 'Computer Networks']\n",
      "\n",
      "üéØ Running Job Matching...\n",
      "\n",
      "üìä Testing with threshold: 0.2\n",
      "üéØ Starting job matching for 89 jobs...\n",
      "üìä Using threshold: 0.2\n",
      "‚úÖ Found 84 jobs above threshold 0.2\n",
      "‚úÖ Found 84 matching jobs:\n",
      "  36. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.458)\n",
      "  39. MERN stack Developer (Internship) at Kartavya Technology (Score: 0.421)\n",
      "  16. Software Development Intern at Talentorix (Score: 0.417)\n",
      "  74. DevOps Engineer Intern at FinacPlus (Score: 0.401)\n",
      "  21. Web Development Internship in Delhi at Traincape Technology (Score: 0.400)\n",
      "  60. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.393)\n",
      "  26. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.393)\n",
      "  28. Web Development Internship in Jaipur at Trumpet Media (Score: 0.391)\n",
      "  62. Web Development Internship in Jaipur at Trumpet Media (Score: 0.391)\n",
      "  80. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.387)\n",
      "  81. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.387)\n",
      "  35. Full Stack Developer Intern at OneDot Communications (Score: 0.382)\n",
      "  52. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.373)\n",
      "  18. Software Engineer Intern at Readyly (Score: 0.371)\n",
      "  79. Software Engineer Intern at Readyly (Score: 0.371)\n",
      "  65. Front End Development Intern at Essor Talents Consultancy (Score: 0.371)\n",
      "  31. Front End Development Intern at Essor Talents Consultancy (Score: 0.371)\n",
      "  22. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.367)\n",
      "  1. Tester Fresher at Crisil (Score: 0.364)\n",
      "  69. Interesting Job Opportunity: Cloud Support Engineer - SysOps at Comprinno (Score: 0.361)\n",
      "  63. front-end developer intern at Geniuses Factory (Score: 0.360)\n",
      "  29. front-end developer intern at Geniuses Factory (Score: 0.360)\n",
      "  45. Full Stack Development Internship in Noida (Hybrid) at Namekart (Score: 0.358)\n",
      "  54. Django + AI Developer Intern (0-1 year of experience) at Codevantage Inc (Score: 0.357)\n",
      "  14. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.354)\n",
      "  50. Frontend Developer at Zetheta Algorithms Private Limited (Score: 0.354)\n",
      "  12. INTERN - Full Stack Python(Angular) at Kots (Score: 0.352)\n",
      "  78. Software Engineer Intern (6 months) at Moloco (Score: 0.350)\n",
      "  5. Software Engineer Intern (6 months) at Moloco (Score: 0.350)\n",
      "  75. Cloud Engineer Intern at NanoByte IT Services (Score: 0.343)\n",
      "  72. DEVOPS INTERN at Elevate Labs (Score: 0.340)\n",
      "  30. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  64. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  86. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  25. AI Engineer, Intern at Newfold Digital (Score: 0.331)\n",
      "  59. AI Engineer, Intern at Newfold Digital (Score: 0.331)\n",
      "  76. DevOps Intern at NanoByte IT Services (Score: 0.327)\n",
      "  42. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.316)\n",
      "  47. Full Stack Development Internship in Bangalore at Think41 (Score: 0.313)\n",
      "  57. Junior Java Developer at Design on a Dime (Score: 0.312)\n",
      "  9. Frontend Intern at GreedyGame (Score: 0.312)\n",
      "  56. Python Intern at Zeno Talent (Score: 0.312)\n",
      "  73. AWS DevOps Engineer Internship in Noida at URE Legal Advocates (Score: 0.311)\n",
      "  44. Software Engineer Intern at RceeNetworks (Score: 0.307)\n",
      "  82. Software Engineer Intern at RceeNetworks (Score: 0.307)\n",
      "  17. Full Stack Developer Intern at PinSec.Ai (Score: 0.302)\n",
      "  11. Web Developer Intern at EazyByts.com (Score: 0.298)\n",
      "  15. Frontend Developer Intern at CODEXINTERN (Score: 0.296)\n",
      "  10. Frontend Developer Intern at CODEXINTERN (Score: 0.296)\n",
      "  68. WEB DEVELOPMENT INTERN at Elevate Labs (Score: 0.295)\n",
      "  34. WEB DEVELOPMENT INTERN at Elevate Labs (Score: 0.295)\n",
      "  71. DevOps Engineer Intern - Indore/Hyderabad - Job ID-109579 at Techdome (Score: 0.294)\n",
      "  58. DevOps Engineer Intern - Indore/Hyderabad - Job ID-109579 at Techdome (Score: 0.294)\n",
      "  55. Web Developer Intern at EB Softco (Score: 0.294)\n",
      "  8. SQL DEVELOPER INTERN at Elevate Labs (Score: 0.294)\n",
      "  46. Python Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.293)\n",
      "  37. Front End Web Development Intern at Calanjiyam Consultancies and Technologies (Score: 0.292)\n",
      "  23. Associate Developer Intern at Jivox (Score: 0.290)\n",
      "  7. PYTHON DEVELOPER INTERN at Elevate Labs (Score: 0.286)\n",
      "  48. Web Developer Intern at UM IT PRIVATE LIMITED (Score: 0.285)\n",
      "  6. Full Stack Developer (Internship) at CloudRedux (Score: 0.282)\n",
      "  13. Intern - Software Engineering (.Net) at Icertis (Score: 0.276)\n",
      "  40. Android Engineering Intern at 10x (Score: 0.276)\n",
      "  38. Full Stack Developer Intern at Calanjiyam Consultancies and Technologies (Score: 0.274)\n",
      "  4. Software Development Engineer Intern at HackerRank (Score: 0.270)\n",
      "  77. Software Development Engineer Intern at HackerRank (Score: 0.270)\n",
      "  53. Curriculum Developer Intern (AI & Coding) at Codingal (Score: 0.263)\n",
      "  24. Python Developer Trainee Internship in Gurgaon at Monkhub Innovations (Score: 0.261)\n",
      "  20. SDE Intern (Frontend) at UniCult (Score: 0.258)\n",
      "  49. Software Testing Internship in Chandigarh, Mohali, Chandigarh at Relinns Technologies (Score: 0.258)\n",
      "  33. Python Developer Intern at CODEXINTERN (Score: 0.258)\n",
      "  67. Python Developer Intern at CODEXINTERN (Score: 0.258)\n",
      "  66. Python Developer Intern at CODEXINTERN (Score: 0.258)\n",
      "  32. Python Developer Intern at CODEXINTERN (Score: 0.258)\n",
      "  3. Software Engineer Intern (Frontend) at PharmEasy (Score: 0.256)\n",
      "  70. Software Engineer Intern (Frontend) at PharmEasy (Score: 0.256)\n",
      "  83. Software Trainee Engineer at Essor Talents Consultancy (Score: 0.255)\n",
      "  19. Software Trainee Engineer at Essor Talents Consultancy (Score: 0.255)\n",
      "  61. MTS 2 (C++) at Adobe (Score: 0.241)\n",
      "  27. MTS 2 (C++) at Adobe (Score: 0.241)\n",
      "  2. Software Development Engineering (Web) Internship in Pune at brownbox consulting (Score: 0.234)\n",
      "  43. Full Stack Development Internship in Mumbai (Hybrid) at Walrus (Score: 0.227)\n",
      "  89. Intern Sales Engineer (MBA Freshers) at techvantage.ai (Score: 0.207)\n",
      "  85. Software Engineer at Indigiworld.in (Score: 0.204)\n",
      "\n",
      "üìä Testing with threshold: 0.3\n",
      "üéØ Starting job matching for 89 jobs...\n",
      "üìä Using threshold: 0.3\n",
      "‚úÖ Found 46 jobs above threshold 0.3\n",
      "‚úÖ Found 46 matching jobs:\n",
      "  36. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.458)\n",
      "  39. MERN stack Developer (Internship) at Kartavya Technology (Score: 0.421)\n",
      "  16. Software Development Intern at Talentorix (Score: 0.417)\n",
      "  74. DevOps Engineer Intern at FinacPlus (Score: 0.401)\n",
      "  21. Web Development Internship in Delhi at Traincape Technology (Score: 0.400)\n",
      "  26. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.393)\n",
      "  60. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.393)\n",
      "  28. Web Development Internship in Jaipur at Trumpet Media (Score: 0.391)\n",
      "  62. Web Development Internship in Jaipur at Trumpet Media (Score: 0.391)\n",
      "  80. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.387)\n",
      "  81. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.387)\n",
      "  35. Full Stack Developer Intern at OneDot Communications (Score: 0.382)\n",
      "  52. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.373)\n",
      "  18. Software Engineer Intern at Readyly (Score: 0.371)\n",
      "  79. Software Engineer Intern at Readyly (Score: 0.371)\n",
      "  65. Front End Development Intern at Essor Talents Consultancy (Score: 0.371)\n",
      "  31. Front End Development Intern at Essor Talents Consultancy (Score: 0.371)\n",
      "  22. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.367)\n",
      "  1. Tester Fresher at Crisil (Score: 0.364)\n",
      "  69. Interesting Job Opportunity: Cloud Support Engineer - SysOps at Comprinno (Score: 0.361)\n",
      "  29. front-end developer intern at Geniuses Factory (Score: 0.360)\n",
      "  63. front-end developer intern at Geniuses Factory (Score: 0.360)\n",
      "  45. Full Stack Development Internship in Noida (Hybrid) at Namekart (Score: 0.358)\n",
      "  54. Django + AI Developer Intern (0-1 year of experience) at Codevantage Inc (Score: 0.357)\n",
      "  14. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.354)\n",
      "  50. Frontend Developer at Zetheta Algorithms Private Limited (Score: 0.354)\n",
      "  12. INTERN - Full Stack Python(Angular) at Kots (Score: 0.352)\n",
      "  78. Software Engineer Intern (6 months) at Moloco (Score: 0.350)\n",
      "  5. Software Engineer Intern (6 months) at Moloco (Score: 0.350)\n",
      "  75. Cloud Engineer Intern at NanoByte IT Services (Score: 0.343)\n",
      "  72. DEVOPS INTERN at Elevate Labs (Score: 0.340)\n",
      "  86. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  30. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  64. Software Engineer Intern at Kran Consulting Pvt. Ltd (Score: 0.334)\n",
      "  25. AI Engineer, Intern at Newfold Digital (Score: 0.331)\n",
      "  59. AI Engineer, Intern at Newfold Digital (Score: 0.331)\n",
      "  76. DevOps Intern at NanoByte IT Services (Score: 0.327)\n",
      "  42. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.316)\n",
      "  47. Full Stack Development Internship in Bangalore at Think41 (Score: 0.313)\n",
      "  57. Junior Java Developer at Design on a Dime (Score: 0.312)\n",
      "  9. Frontend Intern at GreedyGame (Score: 0.312)\n",
      "  56. Python Intern at Zeno Talent (Score: 0.312)\n",
      "  73. AWS DevOps Engineer Internship in Noida at URE Legal Advocates (Score: 0.311)\n",
      "  44. Software Engineer Intern at RceeNetworks (Score: 0.307)\n",
      "  82. Software Engineer Intern at RceeNetworks (Score: 0.307)\n",
      "  17. Full Stack Developer Intern at PinSec.Ai (Score: 0.302)\n",
      "\n",
      "üìä Testing with threshold: 0.4\n",
      "üéØ Starting job matching for 89 jobs...\n",
      "üìä Using threshold: 0.4\n",
      "‚úÖ Found 5 jobs above threshold 0.4\n",
      "‚úÖ Found 5 matching jobs:\n",
      "  36. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.458)\n",
      "  39. MERN stack Developer (Internship) at Kartavya Technology (Score: 0.421)\n",
      "  16. Software Development Intern at Talentorix (Score: 0.417)\n",
      "  74. DevOps Engineer Intern at FinacPlus (Score: 0.401)\n",
      "  21. Web Development Internship in Delhi at Traincape Technology (Score: 0.400)\n",
      "\n",
      "üìä Testing with threshold: 0.5\n",
      "üéØ Starting job matching for 89 jobs...\n",
      "üìä Using threshold: 0.5\n",
      "‚úÖ Found 0 jobs above threshold 0.5\n",
      "‚ö†Ô∏è No jobs found above threshold 0.5\n"
     ]
    }
   ],
   "source": [
    "# Demo: Advanced Job Matching with Real Data\n",
    "def demo_job_matching():\n",
    "    \"\"\"\n",
    "    Demo function to test the advanced job matching system\n",
    "    \"\"\"\n",
    "    print(\"üöÄ AI Career System - Advanced Job Matching Demo\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the scraped jobs data\n",
    "    jobs_file = \"jobs_Yeswanth_Yerra_20251009_123332.csv\"\n",
    "    \n",
    "    try:\n",
    "        jobs_df = pd.read_csv(jobs_file)\n",
    "        print(f\"‚úÖ Loaded {len(jobs_df)} jobs from {jobs_file}\")\n",
    "        \n",
    "        # Display sample job data\n",
    "        print(f\"\\nüìä Sample Job Data:\")\n",
    "        print(f\"Columns: {list(jobs_df.columns)}\")\n",
    "        print(f\"First job title: {jobs_df.iloc[0]['title']}\")\n",
    "        print(f\"First company: {jobs_df.iloc[0]['company']}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Jobs file not found: {jobs_file}\")\n",
    "        print(\"Please run the job scraping first or provide a valid CSV file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading jobs: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Use real parsed resume data instead of a hardcoded sample\n",
    "    resume_data = None\n",
    "    try:\n",
    "        if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "            resume_data = structured_output\n",
    "            print(\"‚úÖ Using resume data from structured_output\")\n",
    "        elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "            resume_data = result['resume_analysis']\n",
    "            print(\"‚úÖ Using resume data from result['resume_analysis']\")\n",
    "    except Exception:\n",
    "        resume_data = None\n",
    "    \n",
    "    if not resume_data:\n",
    "        print(\"‚ö†Ô∏è No structured resume data available (structured_output or result['resume_analysis']).\")\n",
    "        print(\"   Please run the resume parsing cell first, then re-run this demo.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüë§ Resume Analysis:\")\n",
    "    print(f\"Name: {resume_data.get('name', 'N/A')}\")\n",
    "    print(f\"Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "    skills_list = resume_data.get('skills', []) or []\n",
    "    print(f\"Key Skills: {skills_list[:5]}\")\n",
    "    \n",
    "    # Test the matching system\n",
    "    print(f\"\\nüéØ Running Job Matching...\")\n",
    "    \n",
    "    # Test with different thresholds\n",
    "    thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nüìä Testing with threshold: {threshold}\")\n",
    "        matched_jobs = job_matching_service.match_jobs(\n",
    "            resume_data=resume_data,\n",
    "            jobs_df=jobs_df,\n",
    "            threshold=threshold,\n",
    "            top_n=5\n",
    "        )\n",
    "        \n",
    "        if len(matched_jobs) > 0:\n",
    "            print(f\"‚úÖ Found {len(matched_jobs)} matching jobs:\")\n",
    "            for idx, job in matched_jobs.iterrows():\n",
    "                print(f\"  {idx+1}. {job['title']} at {job['company']} (Score: {job['final_score']:.3f})\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No jobs found above threshold {threshold}\")\n",
    "    \n",
    "    return matched_jobs if 'matched_jobs' in locals() else None\n",
    "\n",
    "# Run the demo\n",
    "print(\"üß™ Running Job Matching Demo...\")\n",
    "demo_results = demo_job_matching()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
