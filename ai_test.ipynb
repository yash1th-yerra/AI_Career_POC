{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7058eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0975ba70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env for API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be431fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    raise ValueError(\"âŒ GOOGLE_API_KEY not found in .env file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff7201d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759938676.582822   13497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash LLM with safe optimizations (maintains consistency)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,  # Restore original temperature for consistency\n",
    "    api_key=api_key,\n",
    "    max_output_tokens=4096,  # Set explicit limit for faster response\n",
    "    max_retries=2,  # Keep reasonable retries for reliability\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "680ae862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_file(file_path: str, file_type: str) -> str:\n",
    "    \"\"\"Extract text from PDF or DOCX resume.\"\"\"\n",
    "    if file_type.lower() == \"pdf\":\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            max_pages = min(2, len(pdf.pages))\n",
    "            for i in range(max_pages):\n",
    "                text_parts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(text_parts)\n",
    "    elif file_type.lower() == \"docx\":\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Optional cleaning\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41d5eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Unified Resume Extraction with JSON Output (Fixed Parser Issue)\n",
    "import json\n",
    "\n",
    "# Create a LangChain prompt with direct JSON output\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert Resume Intelligence Agent that extracts structured data and evaluates resumes for ATS compatibility.\n",
    "\n",
    "Analyze the following resume text and return ONLY a valid JSON object with these exact keys:\n",
    "\n",
    "{{\n",
    "  \"name\": \"\",\n",
    "  \"location\": \"\",\n",
    "  \"summary\": \"\",\n",
    "  \"skills\": [],\n",
    "  \"extra_skills\": [],\n",
    "  \"work_experience\": [],\n",
    "  \"projects\": [],\n",
    "  \"certifications\": [],\n",
    "  \"education\": [],\n",
    "  \"experience_level\": \"\",\n",
    "  \"recommended_roles\": [],\n",
    "  \"ats_feedback\": {{\n",
    "    \"score\": 0,\n",
    "    \"summary\": \"\",\n",
    "    \"strengths\": [],\n",
    "    \"improvements\": []\n",
    "  }}\n",
    "}}\n",
    "\n",
    "CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "1. **NAME**: Extract the full name exactly as written, using the most prominent name (usually at the top).\n",
    "\n",
    "2. **LOCATION**: Extract specific city and country from contact info, address, or personal details. Format as \"City, Country\" (e.g., \"Chennai, India\", \"Bangalore, India\"). If no city is specified, use \" Country\".\n",
    "\n",
    "3. **SUMMARY**: Look for sections titled \"Summary\", \"Objective\", \"Profile\", \"About Me\", \"Career Summary\". Extract complete professional summary.\n",
    "\n",
    "4. **SKILLS**: Extract skills ONLY from dedicated \"Skills\", \"Technical Skills\", \"Core Skills\", \"Programming Languages\", or similar sections:\n",
    "   - ONLY include skills explicitly listed in a dedicated skills section\n",
    "   - Programming languages, frameworks, tools, technologies mentioned in skills section\n",
    "   - Return as array of individual skills from the skills section only\n",
    "\n",
    "5. **EXTRA_SKILLS**: Extract additional skills mentioned in other contexts:\n",
    "   - Skills mentioned in work experience descriptions\n",
    "   - Technologies used in projects\n",
    "   - Skills mentioned in certifications or education\n",
    "   - Any other skills not in the main skills section\n",
    "   - Return as array of individual skills from non-skills sections\n",
    "\n",
    "6. **WORK_EXPERIENCE**: Extract each position with:\n",
    "   - Job title, company, duration, location\n",
    "   - Key responsibilities and achievements\n",
    "   - Format as structured objects with consistent fields\n",
    "\n",
    "7. **PROJECTS**: Extract personal/academic projects with:\n",
    "   - Project name, duration, technologies used\n",
    "   - Brief description and key features\n",
    "   - Any notable achievements or results\n",
    "\n",
    "8. **CERTIFICATIONS**: Extract all certifications with:\n",
    "   - Certification name, issuing organization, year\n",
    "   - Include online courses, professional certifications\n",
    "\n",
    "9. **EDUCATION**: Extract educational background with:\n",
    "   - Degree, institution, graduation year\n",
    "   - Relevant coursework or achievements\n",
    "\n",
    "10. **EXPERIENCE_LEVEL**: Analyze the candidate's work experience and determine their experience level:\n",
    "    - \"Entry Level\" (0-1 years): Fresh graduates, internships, or minimal professional experience\n",
    "    - \"Junior\" (1-3 years): Some professional experience, early career roles\n",
    "    - \"Mid-Level\" (3-7 years): Solid professional experience, can work independently\n",
    "    - \"Senior\" (7-12 years): Advanced experience, can lead projects and mentor others\n",
    "    - \"Lead/Principal\" (12+ years): Expert level, can architect solutions and lead teams\n",
    "    - Consider total years of experience, complexity of roles, leadership responsibilities\n",
    "    - Return a single string value\n",
    "\n",
    "11. **RECOMMENDED_ROLES**: Based on the candidate's skills, experience, education, and projects, recommend 2-3 specific job roles they would be suitable for:\n",
    "    - Consider their technical skills, domain expertise, and career progression\n",
    "    - Include roles that match their current skill level and potential growth areas\n",
    "    - Format as array of role titles (e.g., [\"Software Engineer\", \"Data Analyst\", \"Frontend Developer\"])\n",
    "    - Be specific and industry-relevant\n",
    "\n",
    "12. **ATS_FEEDBACK**: Provide objective evaluation:\n",
    "    - score: 0-100 based on ATS compatibility\n",
    "    - summary: Brief assessment\n",
    "    - strengths: Positive aspects\n",
    "    - improvements: Areas for enhancement \n",
    "\n",
    "Guidelines:\n",
    "- Detect section names dynamically (e.g., \"Profile\", \"About Me\", \"Objective\" â†’ summary).\n",
    "- CRITICAL: Skills extraction must be source-aware:\n",
    "  * \"skills\" array: ONLY from dedicated skills sections (Skills, Technical Skills, Core Skills, Programming Languages, etc.)\n",
    "  * \"extra_skills\" array: Skills mentioned in work experience, projects, certifications, education, or other contexts\n",
    "- Extract job/project details separately.\n",
    "- For EXPERIENCE_LEVEL: Analyze total years of professional experience, role complexity, and leadership indicators\n",
    "- For RECOMMENDED_ROLES: Analyze the candidate's profile holistically and suggest roles that align with their skills and experience level\n",
    "- Be consistent and produce clean JSON only.\n",
    "- Prioritize accuracy over completeness.\n",
    "- IMPORTANT: Return ONLY the JSON object, no additional text or explanations.\n",
    "\n",
    "Resume Text:\n",
    "{resume_text}\n",
    "\"\"\")\n",
    "\n",
    "# Build the chain with StrOutputParser for better JSON handling\n",
    "resume_parser_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67bf4517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parsing Error: Expecting value: line 1 column 1 (char 0)\n",
      "Raw output that failed to parse:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"KODIPAKA SAIDIVYA\",\n",
      "  \"location\": \"India\",\n",
      "  \"summary\": \"I am a dedicated student and avid continuous learner with a strong proficiency in electrical and electronics engineering. I have gained valuable hands-on experience during a 6-month internship at a TS Transco substation. Currently, I am expanding my expertise in data science, SQL, and Python to keep pace with emerging technological advancements.\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"C\",\n",
      "    \"SQL\",\n",
      "    \"Data Structures\",\n",
      "    \"Data Science\",\n",
      "    \"MS Excel\",\n",
      "    \"AutoCAD\",\n",
      "    \"CMOS VLSI Design\",\n",
      "    \"Circuit Designing\",\n",
      "    \"Electronics Enthusiast\",\n",
      "    \"Electrical Proficiency\",\n",
      "    \"Quick Learning\",\n",
      "    \"problem solving\",\n",
      "    \"Adaptability\",\n",
      "    \"Leadership qualities\"\n",
      "  ],\n",
      "  \"extra_skills\": [\n",
      "    \"user interaction\",\n",
      "    \"automation\",\n",
      "    \"advanced grid technologies\",\n",
      "    \"smart grid systems\",\n",
      "    \"renewable energy integration\",\n",
      "    \"power distribution networks\",\n",
      "    \"MATLAB\",\n",
      "    \"Simulink\",\n",
      "    \"EV development\",\n",
      "    \"lithium-ion battery applications\",\n",
      "    \"EV efficiency\",\n",
      "    \"substation operations\",\n",
      "    \"maintenance\",\n",
      "    \"high-voltage equipment\",\n",
      "    \"system reliability\",\n",
      "    \"RL algorithms\",\n",
      "    \"Q-learning\",\n",
      "    \"DQN\",\n",
      "    \"UAV autonomous navigation\",\n",
      "    \"IC 741\",\n",
      "    \"audio quality\",\n",
      "    \"signal amplification\",\n",
      "    \"home air cooler\",\n",
      "    \"energy efficiency\",\n",
      "    \"controller board\",\n",
      "    \"Reinforcement Learning\",\n",
      "    \"AI-ML\",\n",
      "    \"Electrical and Electronics Engineering\"\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Developer (Intern)\",\n",
      "      \"company\": \"CodeAlpha India\",\n",
      "      \"duration\": \"01/06/24 â€“ 30/06/24\",\n",
      "      \"location\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Developed a Hangman game to enhance programming skills and problem-solving abilities.\",\n",
      "        \"Created a small chatbot to improve user interaction and automation.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Trainee (Modern Power Systems)\",\n",
      "      \"company\": \"Zafnish Power\",\n",
      "      \"duration\": \"05/05/24 â€“ 04/06/24\",\n",
      "      \"location\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Gained practical experience with advanced grid technologies and smart grid systems.\",\n",
      "        \"Worked on integrating renewable energy sources with modern power distribution networks.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Trainee (EV Design & Analysis)\",\n",
      "      \"company\": \"National Small Scale Industries NSIC\",\n",
      "      \"duration\": \"09/05/23 - 31/05/23\",\n",
      "      \"location\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Applied MATLAB and Simulink for EV development, gaining hands-on experience.\",\n",
      "        \"Explored lithium-ion battery applications, enhancing EV efficiency.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Industrial Trainee\",\n",
      "      \"company\": \"TS TRANSCO (220/112)KV\",\n",
      "      \"duration\": \"01/01/22 - 31/06/22\",\n",
      "      \"location\": null,\n",
      "      \"responsibilities\": [\n",
      "        \"Acquired extensive knowledge of substation operations and maintenance.\",\n",
      "        \"Developed practical skills in handling high-voltage equipment and ensuring system reliability.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_name\": \"Autonomous Unmanned Aerial Vehicle Navigation Using ML\",\n",
      "      \"duration\": \"Feb 2024 â€“ Present\",\n",
      "      \"technologies_used\": [\n",
      "        \"ML\",\n",
      "        \"RL algorithms\",\n",
      "        \"Q-learning\",\n",
      "        \"DQN\"\n",
      "      ],\n",
      "      \"description\": \"Implemented RL algorithms (Q-learning, DQN\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract text\n",
    "resume_path = \"app/resumes/SaidivyaResume.pdf\"\n",
    "text = extract_text_from_file(resume_path, \"pdf\")\n",
    "\n",
    "# Step 2: Parse with Gemini\n",
    "try:\n",
    "    raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "    # print(\"Raw LLM Output:\")\n",
    "    # print(raw_output)\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Parse JSON from the output\n",
    "    structured_output = json.loads(raw_output)\n",
    "    print(\"Parsed JSON Output:\")\n",
    "    print(json.dumps(structured_output, indent=2))\n",
    "    \n",
    "    # Display experience level prominently\n",
    "    if \"experience_level\" in structured_output and structured_output[\"experience_level\"]:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸ“Š EXPERIENCE LEVEL:\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Level: {structured_output['experience_level']}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    # Display recommended roles prominently\n",
    "    if \"recommended_roles\" in structured_output and structured_output[\"recommended_roles\"]:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ðŸŽ¯ RECOMMENDED ROLES FOR THIS CANDIDATE:\")\n",
    "        print(\"=\"*60)\n",
    "        for i, role in enumerate(structured_output[\"recommended_roles\"], 1):\n",
    "            print(f\"{i}. {role}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "    print(\"Raw output that failed to parse:\")\n",
    "    print(raw_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Raw output:\")\n",
    "    print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43d5e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADVANCED JOB MATCHING SYSTEM WITH EMBEDDINGS\n",
    "# ============================================================================\n",
    "# This section contains the embedding-based job matching system\n",
    "# Move this to the bottom as requested\n",
    "\n",
    "# Additional imports for embedding system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3316380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Embedding Service for Job Matching\n",
    "class EmbeddingService:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the embedding service with TF-IDF vectorizer\"\"\"\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2,\n",
    "            max_df=0.8\n",
    "        )\n",
    "        \n",
    "    def create_resume_embeddings(self, resume_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create both global and section-based embeddings for resume\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        resume_data : dict\n",
    "            Parsed resume data with all sections\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing different types of embeddings\n",
    "        \"\"\"\n",
    "        embeddings = {}\n",
    "        \n",
    "        # 1. Global Resume Embedding (entire resume context)\n",
    "        global_text = self._build_global_resume_text(resume_data)\n",
    "        embeddings['global'] = global_text\n",
    "        \n",
    "        # 2. Skills Embedding\n",
    "        skills_text = self._build_skills_text(resume_data)\n",
    "        embeddings['skills'] = skills_text\n",
    "        \n",
    "        # 3. Experience Embedding\n",
    "        experience_text = self._build_experience_text(resume_data)\n",
    "        embeddings['experience'] = experience_text\n",
    "        \n",
    "        # 4. Combined Section Embedding (for weighted similarity)\n",
    "        combined_sections = f\"{skills_text} {experience_text}\"\n",
    "        embeddings['sections'] = combined_sections\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def create_job_embeddings(self, job_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create embeddings for job posting\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        job_data : dict\n",
    "            Job data containing title, description, etc.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Dictionary containing different types of job embeddings\n",
    "        \"\"\"\n",
    "        embeddings = {}\n",
    "        \n",
    "        # 1. Global Job Embedding (title + description + company)\n",
    "        global_job_text = f\"{job_data.get('title', '')} {job_data.get('description', '')} {job_data.get('company', '')}\"\n",
    "        embeddings['global'] = global_job_text\n",
    "        \n",
    "        # 2. Requirements Embedding (extracted from description)\n",
    "        requirements_text = self._extract_requirements(job_data.get('description', ''))\n",
    "        embeddings['requirements'] = requirements_text\n",
    "        \n",
    "        # 3. Combined Job Context\n",
    "        combined_job = f\"{job_data.get('title', '')} {requirements_text}\"\n",
    "        embeddings['context'] = combined_job\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def _build_global_resume_text(self, resume_data: dict) -> str:\n",
    "        \"\"\"Build comprehensive resume text for global embedding\"\"\"\n",
    "        parts = []\n",
    "        \n",
    "        # Add summary\n",
    "        if resume_data.get('summary'):\n",
    "            parts.append(resume_data['summary'])\n",
    "        \n",
    "        # Add skills\n",
    "        if resume_data.get('skills'):\n",
    "            parts.extend(resume_data['skills'])\n",
    "        \n",
    "        # Add extra skills\n",
    "        if resume_data.get('extra_skills'):\n",
    "            parts.extend(resume_data['extra_skills'])\n",
    "        \n",
    "        # Add work experience\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                parts.append(f\"{exp.get('job_title', '')} at {exp.get('company', '')}\")\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    parts.append(exp['description'])\n",
    "        \n",
    "        # Add projects\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                parts.append(f\"Project: {project.get('project_name', '')}\")\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    parts.extend(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    parts.append(project['description'])\n",
    "        \n",
    "        # Add education\n",
    "        if resume_data.get('education'):\n",
    "            for edu in resume_data['education']:\n",
    "                parts.append(f\"{edu.get('degree', '')} from {edu.get('institution', '')}\")\n",
    "        \n",
    "        return ' '.join(parts)\n",
    "    \n",
    "    def _build_skills_text(self, resume_data: dict) -> str:\n",
    "        \"\"\"Build skills-focused text\"\"\"\n",
    "        skills_parts = []\n",
    "        \n",
    "        if resume_data.get('skills'):\n",
    "            skills_parts.extend(resume_data['skills'])\n",
    "        \n",
    "        if resume_data.get('extra_skills'):\n",
    "            skills_parts.extend(resume_data['extra_skills'])\n",
    "        \n",
    "        # Add technical skills from experience\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    skills_parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    skills_parts.append(exp['description'])\n",
    "        \n",
    "        return ' '.join(skills_parts)\n",
    "    \n",
    "    def _build_experience_text(self, resume_data: dict) -> str:\n",
    "        \"\"\"Build experience-focused text\"\"\"\n",
    "        experience_parts = []\n",
    "        \n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                exp_text = f\"{exp.get('job_title', '')} {exp.get('company', '')}\"\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    exp_text += ' ' + ' '.join(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    exp_text += ' ' + exp['description']\n",
    "                experience_parts.append(exp_text)\n",
    "        \n",
    "        # Add projects as experience\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                project_text = f\"Project: {project.get('project_name', '')}\"\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    project_text += ' ' + ' '.join(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    project_text += ' ' + project['description']\n",
    "                experience_parts.append(project_text)\n",
    "        \n",
    "        return ' '.join(experience_parts)\n",
    "    \n",
    "    def _extract_requirements(self, job_description: str) -> str:\n",
    "        \"\"\"Extract key requirements and qualifications from job description\"\"\"\n",
    "        if not job_description:\n",
    "            return \"\"\n",
    "        \n",
    "        # Simple keyword extraction for requirements\n",
    "        # Look for common requirement patterns\n",
    "        requirements_keywords = [\n",
    "            'requirements', 'qualifications', 'skills', 'experience',\n",
    "            'must have', 'should have', 'preferred', 'bachelor', 'master',\n",
    "            'years of experience', 'proficient', 'knowledge of'\n",
    "        ]\n",
    "        \n",
    "        sentences = job_description.split('.')\n",
    "        requirement_sentences = []\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            if any(keyword in sentence_lower for keyword in requirements_keywords):\n",
    "                requirement_sentences.append(sentence.strip())\n",
    "        \n",
    "        return ' '.join(requirement_sentences[:5])  # Take first 5 relevant sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91cfeabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Job Matching System with Weighted Similarity\n",
    "class JobMatchingService:\n",
    "    def __init__(self, embedding_service: EmbeddingService):\n",
    "        \"\"\"\n",
    "        Initialize the job matching service\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        embedding_service : EmbeddingService\n",
    "            Service for creating embeddings\n",
    "        \"\"\"\n",
    "        self.embedding_service = embedding_service\n",
    "        self.tfidf_vectorizer = embedding_service.tfidf_vectorizer\n",
    "        \n",
    "        # Default weights for similarity calculation\n",
    "        self.weights = {\n",
    "            'global': 0.5,      # Global resume vs global job\n",
    "            'skills': 0.3,      # Skills vs requirements\n",
    "            'experience': 0.2   # Experience vs job description\n",
    "        }\n",
    "    \n",
    "    def calculate_similarity(self, resume_embeddings: dict, job_embeddings: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Calculate weighted similarity between resume and job\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        resume_embeddings : dict\n",
    "            Resume embeddings (global, skills, experience, sections)\n",
    "        job_embeddings : dict\n",
    "            Job embeddings (global, requirements, context)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Similarity scores and final weighted score\n",
    "        \"\"\"\n",
    "        similarities = {}\n",
    "        \n",
    "        try:\n",
    "            # 1. Global Similarity (resume global vs job global)\n",
    "            global_sim = self._calculate_text_similarity(\n",
    "                resume_embeddings['global'], \n",
    "                job_embeddings['global']\n",
    "            )\n",
    "            similarities['global'] = global_sim\n",
    "            \n",
    "            # 2. Skills Similarity (resume skills vs job requirements)\n",
    "            skills_sim = self._calculate_text_similarity(\n",
    "                resume_embeddings['skills'], \n",
    "                job_embeddings['requirements']\n",
    "            )\n",
    "            similarities['skills'] = skills_sim\n",
    "            \n",
    "            # 3. Experience Similarity (resume experience vs job context)\n",
    "            experience_sim = self._calculate_text_similarity(\n",
    "                resume_embeddings['experience'], \n",
    "                job_embeddings['context']\n",
    "            )\n",
    "            similarities['experience'] = experience_sim\n",
    "            \n",
    "            # 4. Calculate weighted final score\n",
    "            final_score = (\n",
    "                self.weights['global'] * similarities['global'] +\n",
    "                self.weights['skills'] * similarities['skills'] +\n",
    "                self.weights['experience'] * similarities['experience']\n",
    "            )\n",
    "            \n",
    "            similarities['final_score'] = final_score\n",
    "            \n",
    "            return similarities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {e}\")\n",
    "            return {\n",
    "                'global': 0.0,\n",
    "                'skills': 0.0,\n",
    "                'experience': 0.0,\n",
    "                'final_score': 0.0,\n",
    "                'error': str(e)\n",
    "            }\n",
    "    \n",
    "    def _calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate cosine similarity between two text strings using TF-IDF\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text1 : str\n",
    "            First text string\n",
    "        text2 : str\n",
    "            Second text string\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Cosine similarity score (0-1)\n",
    "        \"\"\"\n",
    "        if not text1.strip() or not text2.strip():\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Vectorize the texts\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform([text1, text2])\n",
    "            \n",
    "            # Calculate cosine similarity\n",
    "            similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "            \n",
    "            # Return similarity between the two texts\n",
    "            return similarity_matrix[0][1]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in text similarity calculation: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def match_jobs(self, resume_data: dict, jobs_df: pd.DataFrame, \n",
    "                   threshold: float = 0.3, top_n: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Match jobs with resume and return filtered results\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        resume_data : dict\n",
    "            Parsed resume data\n",
    "        jobs_df : pd.DataFrame\n",
    "            DataFrame containing job postings\n",
    "        threshold : float\n",
    "            Minimum similarity threshold (0-1)\n",
    "        top_n : int\n",
    "            Maximum number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame\n",
    "            Filtered and ranked job matches\n",
    "        \"\"\"\n",
    "        print(f\"ðŸŽ¯ Starting job matching for {len(jobs_df)} jobs...\")\n",
    "        print(f\"ðŸ“Š Using threshold: {threshold}, Top N: {top_n}\")\n",
    "        \n",
    "        # Create resume embeddings\n",
    "        resume_embeddings = self.embedding_service.create_resume_embeddings(resume_data)\n",
    "        \n",
    "        # Calculate similarities for each job\n",
    "        job_scores = []\n",
    "        \n",
    "        for idx, job_row in jobs_df.iterrows():\n",
    "            job_data = {\n",
    "                'title': job_row.get('title', ''),\n",
    "                'description': job_row.get('description', ''),\n",
    "                'company': job_row.get('company', ''),\n",
    "                'location': job_row.get('location', ''),\n",
    "                'link': job_row.get('link', '')\n",
    "            }\n",
    "            \n",
    "            # Create job embeddings\n",
    "            job_embeddings = self.embedding_service.create_job_embeddings(job_data)\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarities = self.calculate_similarity(resume_embeddings, job_embeddings)\n",
    "            \n",
    "            # Add job info and scores\n",
    "            job_info = {\n",
    "                'job_index': idx,\n",
    "                'title': job_data['title'],\n",
    "                'company': job_data['company'],\n",
    "                'location': job_data['location'],\n",
    "                'link': job_data['link'],\n",
    "                'global_similarity': similarities['global'],\n",
    "                'skills_similarity': similarities['skills'],\n",
    "                'experience_similarity': similarities['experience'],\n",
    "                'final_score': similarities['final_score']\n",
    "            }\n",
    "            \n",
    "            job_scores.append(job_info)\n",
    "        \n",
    "        # Convert to DataFrame and filter\n",
    "        results_df = pd.DataFrame(job_scores)\n",
    "        \n",
    "        # Filter by threshold\n",
    "        filtered_df = results_df[results_df['final_score'] >= threshold].copy()\n",
    "        \n",
    "        # Sort by final score (descending)\n",
    "        filtered_df = filtered_df.sort_values('final_score', ascending=False)\n",
    "        \n",
    "        # Take top N results\n",
    "        if top_n > 0:\n",
    "            filtered_df = filtered_df.head(top_n)\n",
    "        \n",
    "        print(f\"âœ… Found {len(filtered_df)} jobs above threshold {threshold}\")\n",
    "        \n",
    "        return filtered_df\n",
    "    \n",
    "    def update_weights(self, global_weight: float = None, skills_weight: float = None, \n",
    "                      experience_weight: float = None):\n",
    "        \"\"\"\n",
    "        Update similarity calculation weights\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        global_weight : float\n",
    "            Weight for global similarity\n",
    "        skills_weight : float\n",
    "            Weight for skills similarity\n",
    "        experience_weight : float\n",
    "            Weight for experience similarity\n",
    "        \"\"\"\n",
    "        if global_weight is not None:\n",
    "            self.weights['global'] = global_weight\n",
    "        if skills_weight is not None:\n",
    "            self.weights['skills'] = skills_weight\n",
    "        if experience_weight is not None:\n",
    "            self.weights['experience'] = experience_weight\n",
    "        \n",
    "        # Normalize weights to sum to 1\n",
    "        total_weight = sum(self.weights.values())\n",
    "        if total_weight > 0:\n",
    "            for key in self.weights:\n",
    "                self.weights[key] = self.weights[key] / total_weight\n",
    "        \n",
    "        print(f\"ðŸ”„ Updated weights: {self.weights}\")\n",
    "\n",
    "# Initialize services\n",
    "embedding_service = EmbeddingService()\n",
    "job_matching_service = JobMatchingService(embedding_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63af7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Running Job Matching Demo...\n",
      "ðŸš€ AI Career System - Advanced Job Matching Demo\n",
      "======================================================================\n",
      "âœ… Loaded 69 jobs from jobs_KODIPAKA_SAIDIVYA_20251008_151622.csv\n",
      "\n",
      "ðŸ“Š Sample Job Data:\n",
      "Columns: ['title', 'company', 'location', 'link', 'description', 'searched_for', 'experience_level_filter', 'days_back', 'scraped_at']\n",
      "First job title: Graduate Engineering Trainee\n",
      "First company: Adani Electricity\n",
      "\n",
      "ðŸ‘¤ Resume Analysis:\n",
      "Name: KODIPAKA SAIDIVYA\n",
      "Experience Level: Entry Level\n",
      "Key Skills: ['Python', 'C', 'SQL', 'Data Structures', 'Data Science']\n",
      "\n",
      "ðŸŽ¯ Running Job Matching...\n",
      "\n",
      "ðŸ“Š Testing with threshold: 0.2\n",
      "ðŸŽ¯ Starting job matching for 69 jobs...\n",
      "ðŸ“Š Using threshold: 0.2, Top N: 5\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "âœ… Found 0 jobs above threshold 0.2\n",
      "âš ï¸ No jobs found above threshold 0.2\n",
      "\n",
      "ðŸ“Š Testing with threshold: 0.3\n",
      "ðŸŽ¯ Starting job matching for 69 jobs...\n",
      "ðŸ“Š Using threshold: 0.3, Top N: 5\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "âœ… Found 0 jobs above threshold 0.3\n",
      "âš ï¸ No jobs found above threshold 0.3\n",
      "\n",
      "ðŸ“Š Testing with threshold: 0.4\n",
      "ðŸŽ¯ Starting job matching for 69 jobs...\n",
      "ðŸ“Š Using threshold: 0.4, Top N: 5\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "âœ… Found 0 jobs above threshold 0.4\n",
      "âš ï¸ No jobs found above threshold 0.4\n",
      "\n",
      "ðŸ“Š Testing with threshold: 0.5\n",
      "ðŸŽ¯ Starting job matching for 69 jobs...\n",
      "ðŸ“Š Using threshold: 0.5, Top N: 5\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "Error in text similarity calculation: max_df corresponds to < documents than min_df\n",
      "âœ… Found 0 jobs above threshold 0.5\n",
      "âš ï¸ No jobs found above threshold 0.5\n"
     ]
    }
   ],
   "source": [
    "# Demo: Advanced Job Matching with Real Data\n",
    "def demo_job_matching():\n",
    "    \"\"\"\n",
    "    Demo function to test the advanced job matching system\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ AI Career System - Advanced Job Matching Demo\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the scraped jobs data\n",
    "    jobs_file = \"jobs_KODIPAKA_SAIDIVYA_20251008_151622.csv\"\n",
    "    \n",
    "    try:\n",
    "        jobs_df = pd.read_csv(jobs_file)\n",
    "        print(f\"âœ… Loaded {len(jobs_df)} jobs from {jobs_file}\")\n",
    "        \n",
    "        # Display sample job data\n",
    "        print(f\"\\nðŸ“Š Sample Job Data:\")\n",
    "        print(f\"Columns: {list(jobs_df.columns)}\")\n",
    "        print(f\"First job title: {jobs_df.iloc[0]['title']}\")\n",
    "        print(f\"First company: {jobs_df.iloc[0]['company']}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ Jobs file not found: {jobs_file}\")\n",
    "        print(\"Please run the job scraping first or provide a valid CSV file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading jobs: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Sample resume data (you can replace this with actual parsed resume data)\n",
    "    sample_resume = {\n",
    "        \"name\": \"KODIPAKA SAIDIVYA\",\n",
    "        \"location\": \"India\",\n",
    "        \"summary\": \"Dedicated student with strong proficiency in electrical and electronics engineering. Experience in data science, SQL, and Python. 6-month internship at TS Transco substation.\",\n",
    "        \"skills\": [\"Python\", \"C\", \"SQL\", \"Data Structures\", \"Data Science\", \"MS Excel\", \"AutoCAD\", \"CMOS VLSI Design\", \"Circuit Designing\"],\n",
    "        \"extra_skills\": [\"MATLAB\", \"ML\", \"Simulink\", \"Reinforcement Learning\", \"AI-ML\", \"Electrical and Electronics Engineering\"],\n",
    "        \"work_experience\": [\n",
    "            {\n",
    "                \"job_title\": \"Python Developer (Intern)\",\n",
    "                \"company\": \"CodeAlpha India\",\n",
    "                \"duration\": \"01/06/24 â€“ 30/06/24\",\n",
    "                \"description\": [\"Developed a Hangman game\", \"Created a small chatbot\"]\n",
    "            }\n",
    "        ],\n",
    "        \"projects\": [\n",
    "            {\n",
    "                \"project_name\": \"UAV Autonomous Navigation\",\n",
    "                \"description\": [\"Used RL algorithms (Q-learning, DQN)\", \"Implemented autonomous navigation system\"]\n",
    "            }\n",
    "        ],\n",
    "        \"education\": [\n",
    "            {\n",
    "                \"degree\": \"Bachelor of Technology in Electrical and Electronics Engineering\",\n",
    "                \"institution\": \"University Name\"\n",
    "            }\n",
    "        ],\n",
    "        \"experience_level\": \"Entry Level\",\n",
    "        \"recommended_roles\": [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ‘¤ Resume Analysis:\")\n",
    "    print(f\"Name: {sample_resume['name']}\")\n",
    "    print(f\"Experience Level: {sample_resume['experience_level']}\")\n",
    "    print(f\"Key Skills: {sample_resume['skills'][:5]}\")\n",
    "    \n",
    "    # Test the matching system\n",
    "    print(f\"\\nðŸŽ¯ Running Job Matching...\")\n",
    "    \n",
    "    # Test with different thresholds\n",
    "    thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nðŸ“Š Testing with threshold: {threshold}\")\n",
    "        matched_jobs = job_matching_service.match_jobs(\n",
    "            resume_data=sample_resume,\n",
    "            jobs_df=jobs_df,\n",
    "            threshold=threshold,\n",
    "            top_n=5\n",
    "        )\n",
    "        \n",
    "        if len(matched_jobs) > 0:\n",
    "            print(f\"âœ… Found {len(matched_jobs)} matching jobs:\")\n",
    "            for idx, job in matched_jobs.iterrows():\n",
    "                print(f\"  {idx+1}. {job['title']} at {job['company']} (Score: {job['final_score']:.3f})\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ No jobs found above threshold {threshold}\")\n",
    "    \n",
    "    return matched_jobs if 'matched_jobs' in locals() else None\n",
    "\n",
    "# Run the demo\n",
    "print(\"ðŸ§ª Running Job Matching Demo...\")\n",
    "demo_results = demo_job_matching()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c17cb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "139e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env for API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6c3295eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    raise ValueError(\"âŒ GOOGLE_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6196d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1759938783.987993   13497 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash LLM with safe optimizations (maintains consistency)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,  # Restore original temperature for consistency\n",
    "    api_key=api_key,\n",
    "    # max_output_tokens=4096,  # Set explicit limit for faster response\n",
    "    max_retries=2,  # Keep reasonable retries for reliability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a109c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_file(file_path: str, file_type: str) -> str:\n",
    "    \"\"\"Extract text from PDF or DOCX resume.\"\"\"\n",
    "    if file_type.lower() == \"pdf\":\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            max_pages = min(2, len(pdf.pages))\n",
    "            for i in range(max_pages):\n",
    "                text_parts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(text_parts)\n",
    "    elif file_type.lower() == \"docx\":\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Optional cleaning\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9814675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Unified Resume Extraction with JSON Output (Fixed Parser Issue)\n",
    "import json\n",
    "\n",
    "# Create a LangChain prompt with direct JSON output\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert Resume Intelligence Agent that extracts structured data and evaluates resumes for ATS compatibility.\n",
    "\n",
    "Analyze the following resume text and return ONLY a valid JSON object with these exact keys:\n",
    "\n",
    "{{\n",
    "  \"name\": \"\",\n",
    "  \"location\": \"\",\n",
    "  \"summary\": \"\",\n",
    "  \"skills\": [],\n",
    "  \"extra_skills\": [],\n",
    "  \"work_experience\": [],\n",
    "  \"projects\": [],\n",
    "  \"certifications\": [],\n",
    "  \"education\": [],\n",
    "  \"experience_level\": \"\",\n",
    "  \"recommended_roles\": [],\n",
    "  \"ats_feedback\": {{\n",
    "    \"score\": 0,\n",
    "    \"summary\": \"\",\n",
    "    \"strengths\": [],\n",
    "    \"improvements\": []\n",
    "  }}\n",
    "}}\n",
    "\n",
    "CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "1. **NAME**: Extract the full name exactly as written, using the most prominent name (usually at the top).\n",
    "\n",
    "2. **LOCATION**: Extract specific city and country from contact info, address, or personal details. Format as \"City, Country\" (e.g., \"Chennai, India\", \"Bangalore, India\"). If no city is specified, use \" Country\".\n",
    "\n",
    "3. **SUMMARY**: Look for sections titled \"Summary\", \"Objective\", \"Profile\", \"About Me\", \"Career Summary\". Extract complete professional summary.\n",
    "\n",
    "4. **SKILLS**: Extract skills ONLY from dedicated \"Skills\", \"Technical Skills\", \"Core Skills\", \"Programming Languages\", or similar sections:\n",
    "   - ONLY include skills explicitly listed in a dedicated skills section\n",
    "   - Programming languages, frameworks, tools, technologies mentioned in skills section\n",
    "   - Return as array of individual skills from the skills section only\n",
    "\n",
    "5. **EXTRA_SKILLS**: Extract additional skills mentioned in other contexts:\n",
    "   - Skills mentioned in work experience descriptions\n",
    "   - Technologies used in projects\n",
    "   - Skills mentioned in certifications or education\n",
    "   - Any other skills not in the main skills section\n",
    "   - Return as array of individual skills from non-skills sections\n",
    "\n",
    "6. **WORK_EXPERIENCE**: Extract each position with:\n",
    "   - Job title, company, duration, location\n",
    "   - Key responsibilities and achievements\n",
    "   - Format as structured objects with consistent fields\n",
    "\n",
    "7. **PROJECTS**: Extract personal/academic projects with:\n",
    "   - Project name, duration, technologies used\n",
    "   - Brief description and key features\n",
    "   - Any notable achievements or results\n",
    "\n",
    "8. **CERTIFICATIONS**: Extract all certifications with:\n",
    "   - Certification name, issuing organization, year\n",
    "   - Include online courses, professional certifications\n",
    "\n",
    "9. **EDUCATION**: Extract educational background with:\n",
    "   - Degree, institution, graduation year\n",
    "   - Relevant coursework or achievements\n",
    "\n",
    "10. **EXPERIENCE_LEVEL**: Analyze the candidate's work experience and determine their experience level:\n",
    "    - \"Entry Level\" (0-1 years): Fresh graduates, internships, or minimal professional experience\n",
    "    - \"Junior\" (1-3 years): Some professional experience, early career roles\n",
    "    - \"Mid-Level\" (3-7 years): Solid professional experience, can work independently\n",
    "    - \"Senior\" (7-12 years): Advanced experience, can lead projects and mentor others\n",
    "    - \"Lead/Principal\" (12+ years): Expert level, can architect solutions and lead teams\n",
    "    - Consider total years of experience, complexity of roles, leadership responsibilities\n",
    "    - Return a single string value\n",
    "\n",
    "11. **RECOMMENDED_ROLES**: Based on the candidate's skills, experience, education, and projects, recommend 2-3 specific job roles they would be suitable for:\n",
    "    - Consider their technical skills, domain expertise, and career progression\n",
    "    - Include roles that match their current skill level and potential growth areas\n",
    "    - Format as array of role titles (e.g., [\"Software Engineer\", \"Data Analyst\", \"Frontend Developer\"])\n",
    "    - Be specific and industry-relevant\n",
    "\n",
    "12. **ATS_FEEDBACK**: Provide objective evaluation:\n",
    "    - score: 0-100 based on ATS compatibility\n",
    "    - summary: Brief assessment\n",
    "    - strengths: Positive aspects\n",
    "    - improvements: Areas for enhancement \n",
    "\n",
    "Guidelines:\n",
    "- Detect section names dynamically (e.g., \"Profile\", \"About Me\", \"Objective\" â†’ summary).\n",
    "- CRITICAL: Skills extraction must be source-aware:\n",
    "  * \"skills\" array: ONLY from dedicated skills sections (Skills, Technical Skills, Core Skills, Programming Languages, etc.)\n",
    "  * \"extra_skills\" array: Skills mentioned in work experience, projects, certifications, education, or other contexts\n",
    "- Extract job/project details separately.\n",
    "- For EXPERIENCE_LEVEL: Analyze total years of professional experience, role complexity, and leadership indicators\n",
    "- For RECOMMENDED_ROLES: Analyze the candidate's profile holistically and suggest roles that align with their skills and experience level\n",
    "- Be consistent and produce clean JSON only.\n",
    "- Prioritize accuracy over completeness.\n",
    "- IMPORTANT: Return ONLY the JSON object, no additional text or explanations.\n",
    "\n",
    "Resume Text:\n",
    "{resume_text}\n",
    "\"\"\")\n",
    "\n",
    "# Build the chain with StrOutputParser for better JSON handling\n",
    "resume_parser_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57a559f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parsing Error: Expecting value: line 1 column 1 (char 0)\n",
      "Raw output that failed to parse:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"KODIPAKA SAIDIVYA\",\n",
      "  \"location\": \"India\",\n",
      "  \"summary\": \"I am a dedicated student and avid continuous learner with a strong proficiency in electrical and electronics engineering. I have gained valuable hands-on experience during a 6-month internship at a TS Transco substation. Currently, I am expanding my expertise in data science, SQL, and Python to keep pace with emerging technological advancements.\",\n",
      "  \"skills\": [\n",
      "    \"Python\",\n",
      "    \"C\",\n",
      "    \"SQL\",\n",
      "    \"Data Structures\",\n",
      "    \"Data Science\",\n",
      "    \"MS Excel\",\n",
      "    \"AutoCAD\",\n",
      "    \"CMOS VLSI Design\",\n",
      "    \"Circuit Designing\",\n",
      "    \"Electronics Enthusiast\",\n",
      "    \"Electrical Proficiency\",\n",
      "    \"Quick Learning\",\n",
      "    \"Problem Solving\",\n",
      "    \"Adaptability\",\n",
      "    \"Leadership Qualities\"\n",
      "  ],\n",
      "  \"extra_skills\": [\n",
      "    \"Hangman Game Development\",\n",
      "    \"Chatbot Development\",\n",
      "    \"Advanced Grid Technologies\",\n",
      "    \"Smart Grid Systems\",\n",
      "    \"Renewable Energy Integration\",\n",
      "    \"Power Distribution Networks\",\n",
      "    \"MATLAB\",\n",
      "    \"Simulink\",\n",
      "    \"EV Development\",\n",
      "    \"Lithium-ion Battery Applications\",\n",
      "    \"Substation Operations\",\n",
      "    \"Substation Maintenance\",\n",
      "    \"High-Voltage Equipment\",\n",
      "    \"System Reliability\",\n",
      "    \"Reinforcement Learning (RL)\",\n",
      "    \"Q-learning\",\n",
      "    \"DQN\",\n",
      "    \"UAV Autonomous Navigation\",\n",
      "    \"IC 741\",\n",
      "    \"Audio Amplifier Design\",\n",
      "    \"Signal Amplification\",\n",
      "    \"Controller Board Design\",\n",
      "    \"Energy Efficiency\",\n",
      "    \"Automation\",\n",
      "    \"AI-ML\",\n",
      "    \"Electrical and Electronics Engineering\"\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Python Developer Intern\",\n",
      "      \"company\": \"CodeAlpha India\",\n",
      "      \"duration\": \"01/06/24 â€“ 30/06/24\",\n",
      "      \"location\": \"India\",\n",
      "      \"responsibilities\": [\n",
      "        \"Developed a Hangman game to enhance programming skills and problem-solving abilities.\",\n",
      "        \"Created a small chatbot to improve user interaction and automation.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Power Systems Intern\",\n",
      "      \"company\": \"Zafnish Power\",\n",
      "      \"duration\": \"05/05/24 â€“ 04/06/24\",\n",
      "      \"location\": \"India\",\n",
      "      \"responsibilities\": [\n",
      "        \"Gained practical experience with advanced grid technologies and smart grid systems.\",\n",
      "        \"Worked on integrating renewable energy sources with modern power distribution networks.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"EV Design & Analysis Intern\",\n",
      "      \"company\": \"National Small Scale Industries (NSIC)\",\n",
      "      \"duration\": \"09/05/23 - 31/05/23\",\n",
      "      \"location\": \"India\",\n",
      "      \"responsibilities\": [\n",
      "        \"Applied MATLAB and Simulink for EV development, gaining hands-on experience.\",\n",
      "        \"Explored lithium-ion battery applications, enhancing EV efficiency.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"job_title\": \"Industrial Trainee\",\n",
      "      \"company\": \"TS TRANSCO (220/112)KV\",\n",
      "      \"duration\": \"01/01/22 - 31/06/22\",\n",
      "      \"location\": \"India\",\n",
      "      \"responsibilities\": [\n",
      "        \"Acquired extensive knowledge of substation operations and maintenance.\",\n",
      "        \"Developed practical skills in handling high-voltage equipment and ensuring system reliability.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_name\": \"Autonomous Unmanned Aerial Vehicle Navigation Using ML\",\n",
      "      \"duration\": \"Feb 2024 â€“ Present\",\n",
      "      \"technologies_used\": [\n",
      "        \"ML\",\n",
      "        \"RL algorithms\",\n",
      "        \"Q-learning\",\n",
      "        \"DQN\"\n",
      "      ],\n",
      "      \"description\": \"Implemented RL algorithms (Q-learning, DQN) for UAV autonomous navigation.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Audio Amplifier using IC 741\",\n",
      "      \"duration\": \"Sep 2023 - Dec 2023\",\n",
      "      \"technologies_used\": [\n",
      "        \"IC 741\"\n",
      "      ],\n",
      "      \"description\": \"Designed and evaluated a headphone amplifier circuit with IC 741 to enhance audio quality and signal amplification.\"\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Remote Control Air Cooler\",\n",
      "      \"duration\": \"Jul 2021 - Dec 2021\",\n",
      "      \"technologies_used\": [\n",
      "        \"Controller Board\"\n",
      "      ],\n",
      "      \"description\": \"Implemented Automated a home air cooler. Developed a controller board to enhance energy efficiency and automation.\"\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"Certified SQL & Python\",\n",
      "      \"issuing_organization\": \"HackerRank\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Electrical AutoCAD Certified Professional\",\n",
      "      \"issuing_organization\": \"LinkedIn Learning\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Certified in Reinforcement Learning\",\n",
      "      \"issuing_organization\": \"Infosys Springboard\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"SQL & Python Certified Professional\",\n",
      "      \"issuing_organization\": \"LinkedIn Learning\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Elite Certificate in CMOS Digital VLSI Design\",\n",
      "      \"issuing_organization\": \"NPTEL\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"AI-ML Virtual Internship Completion Certificate\",\n",
      "      \"issuing_organization\": \"AICTE, EduSkills Portal\",\n",
      "      \"year\": null\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"B.TECH in Electrical and Electronics Engineering\",\n",
      "      \"institution\": \"Sreenidhi Institute of Science and Technology\",\n",
      "      \"graduation_year\": \"Present\",\n",
      "      \"details\": \"CGPA : 8.5\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"DIPLOMA in Electrical and Electronics Engineering\",\n",
      "      \"institution\": \"SRRS Government Polytechnic\",\n",
      "      \"graduation_year\": \"2022\",\n",
      "      \"details\": \"CGPA : 9.5\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"SSC (Class X)\",\n",
      "      \"institution\": \"Sri Saraswathi Shishu Mandir\",\n",
      "      \"graduation_year\": \"2019\",\n",
      "      \"details\": \"CGPA: 9.2\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience_level\": \"Entry Level\",\n",
      "  \"recommended_roles\": [\n",
      "    \"Junior Electrical Engineer\",\n",
      "    \"Junior Power Systems Engineer\",\n",
      "    \"Junior Data Analyst\"\n",
      "  ],\n",
      "  \"ats_feedback\": {\n",
      "    \"score\": 75,\n",
      "    \"summary\": \"This resume presents a well-structured profile of a dedicated Electrical and Electronics Engineering student with a strong foundation in core engineering principles and a proactive approach to learning new technologies like Data Science, SQL, and Python. The candidate demonstrates practical application through various internships and projects.\",\n",
      "    \"strengths\": [\n",
      "      \"Clear and organized sectioning, enhancing readability for ATS and recruiters.\",\n",
      "      \"Strong academic background with excellent CGPA in both Diploma and B.Tech.\",\n",
      "      \"Proactive learning and acquisition of skills in emerging technologies (Data Science, Python, SQL, ML).\",\n",
      "      \"Hands-on experience demonstrated through multiple internships and practical projects.\",\n",
      "      \"Categorized skills section, making technical competencies easily identifiable.\",\n",
      "      \"Relevant certifications validate acquired technical skills.\",\n",
      "      \"Notable achievements including scholarships and competition wins.\"\n",
      "    ],\n",
      "    \"improvements\": [\n",
      "      \"Quantify achievements and responsibilities in work experience and projects with metrics (e.g., 'improved X by Y%', 'reduced Z by W%') to demonstrate impact.\",\n",
      "      \"Expand on project descriptions to highlight specific technical challenges overcome and the candidate's contribution.\",\n",
      "      \"Specify locations for all work experiences/internships if possible, even if inferred to be 'India'.\",\n",
      "      \"Consider adding a brief 'Objective' or 'Career Goal' statement to the summary if targeting specific roles, to align with future aspirations.\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract text\n",
    "resume_path = \"app/resumes/SaidivyaResume.pdf\"\n",
    "text = extract_text_from_file(resume_path, \"pdf\")\n",
    "\n",
    "# Step 2: Parse with Gemini\n",
    "try:\n",
    "    raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "    # print(\"Raw LLM Output:\")\n",
    "    # print(raw_output)\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Parse JSON from the output\n",
    "    structured_output = json.loads(raw_output)\n",
    "    print(\"Parsed JSON Output:\")\n",
    "    print(json.dumps(structured_output, indent=2))\n",
    "    \n",
    "    # Display experience level prominently\n",
    "    # if \"experience_level\" in structured_output and structured_output[\"experience_level\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"ðŸ“Š EXPERIENCE LEVEL:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     print(f\"Level: {structured_output['experience_level']}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "    # # Display recommended roles prominently\n",
    "    # if \"recommended_roles\" in structured_output and structured_output[\"recommended_roles\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"ðŸŽ¯ RECOMMENDED ROLES FOR THIS CANDIDATE:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     for i, role in enumerate(structured_output[\"recommended_roles\"], 1):\n",
    "    #         print(f\"{i}. {role}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "    print(\"Raw output that failed to parse:\")\n",
    "    print(raw_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Raw output:\")\n",
    "    print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c197ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn Job Scraper Integration\n",
    "# Import necessary packages for web scraping and logging\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "# Configure logging settings\n",
    "logging.basicConfig(filename=\"linkedin_scraping.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40393c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInJobScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"\n",
    "        Initialize the LinkedIn Job Scraper\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        headless : bool\n",
    "            Whether to run Chrome in headless mode (default: False)\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.setup_driver()\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with appropriate options\"\"\"\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            \n",
    "            # Basic options\n",
    "            options.add_argument(\"--start-maximized\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            \n",
    "            # User agent to avoid detection\n",
    "            options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument(\"--headless\")\n",
    "            \n",
    "            self.driver = webdriver.Chrome(options=options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logging.info(\"Chrome WebDriver initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def build_search_url(self, job_title: str, location: str = \"India\", experience_level: str = None, \n",
    "                        time_posted: str = None, remote: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Build LinkedIn job search URL with filters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        job_title : str\n",
    "            Job title to search for\n",
    "        location : str\n",
    "            Location to search in (default: \"India\")\n",
    "        experience_level : str\n",
    "            Experience level filter (Entry level, Associate, Mid-Senior level, Director, Executive)\n",
    "        time_posted : str\n",
    "            Time posted filter (r86400, r604800, r2592000, r31536000)\n",
    "        remote : bool\n",
    "            Whether to include remote jobs only\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete LinkedIn job search URL\n",
    "        \"\"\"\n",
    "        base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "        \n",
    "        # URL encode parameters\n",
    "        job_title_encoded = urllib.parse.quote(job_title)\n",
    "        location_encoded = urllib.parse.quote(location)\n",
    "        \n",
    "        # Build query parameters\n",
    "        params = {\n",
    "            \"keywords\": job_title_encoded,\n",
    "            \"location\": location_encoded,\n",
    "            \"f_TPR\": time_posted if time_posted else None,  # Time posted filter\n",
    "            \"f_E\": self._get_experience_filter(experience_level) if experience_level else None,  # Experience filter\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # Filter out None values and build query string\n",
    "        query_params = {k: v for k, v in params.items() if v is not None}\n",
    "        query_string = \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "        \n",
    "        full_url = f\"{base_url}?{query_string}\"\n",
    "        logging.info(f\"Built search URL: {full_url}\")\n",
    "        \n",
    "        return full_url\n",
    "    \n",
    "    def _get_experience_filter(self, experience_level: str) -> str:\n",
    "        \"\"\"\n",
    "        Map experience level to LinkedIn filter values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        experience_level : str\n",
    "            Experience level from resume analysis\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn experience filter value\n",
    "        \"\"\"\n",
    "        experience_mapping = {\n",
    "            \"Entry Level\": \"1\",           # Entry level\n",
    "            \"Junior\": \"2\",                # Associate\n",
    "            \"Mid-Level\": \"3\",             # Mid-Senior level\n",
    "            \"Senior\": \"4\",                # Director\n",
    "            \"Lead/Principal\": \"5\"         # Executive\n",
    "        }\n",
    "        \n",
    "        return experience_mapping.get(experience_level, \"1\")  # Default to Entry level\n",
    "    \n",
    "    def _get_time_filter(self, days: int) -> str:\n",
    "        \"\"\"\n",
    "        Get time posted filter based on days\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        days : int\n",
    "            Number of days to look back\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn time filter value\n",
    "        \"\"\"\n",
    "        time_mapping = {\n",
    "            1: \"r86400\",      # Past 24 hours\n",
    "            7: \"r604800\",     # Past week\n",
    "            30: \"r2592000\",   # Past month\n",
    "            365: \"r31536000\"  # Past year\n",
    "        }\n",
    "        \n",
    "        return time_mapping.get(days, \"r604800\")  # Default to past week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8927a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the remaining methods to LinkedInJobScraper class\n",
    "def add_methods_to_scraper():\n",
    "    \"\"\"Add the scrape_jobs and close methods to LinkedInJobScraper class\"\"\"\n",
    "    \n",
    "    def scrape_jobs(self, job_title: str, location: str = \"India\", pages: int = 1, \n",
    "                   experience_level: str = None, days_back: int = 7) -> list:\n",
    "        \"\"\"Scrape job listings from LinkedIn with filters\"\"\"\n",
    "        logging.info(f'Starting LinkedIn job scrape for \"{job_title}\" in \"{location}\"...')\n",
    "        \n",
    "        # Build search URL with filters\n",
    "        time_filter = self._get_time_filter(days_back)\n",
    "        search_url = self.build_search_url(\n",
    "            job_title=job_title,\n",
    "            location=location,\n",
    "            experience_level=experience_level,\n",
    "            time_posted=time_filter\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the LinkedIn job search page\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)  # Wait for page to load\n",
    "            \n",
    "            # Scroll through the specified number of pages\n",
    "            for i in range(pages):\n",
    "                logging.info(f\"Scrolling to bottom of page {i+1}...\")\n",
    "                \n",
    "                # Scroll to the bottom of the page using JavaScript\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                \n",
    "                try:\n",
    "                    # Wait for the \"Show more\" button to be present on the page\n",
    "                    element = WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.XPATH, \"/html/body/div[1]/div/main/section[2]/button\")\n",
    "                        )\n",
    "                    )\n",
    "                    # Click on the \"Show more\" button\n",
    "                    element.click()\n",
    "                    logging.info(\"Clicked 'Show more' button\")\n",
    "                    \n",
    "                except Exception:\n",
    "                    logging.info(\"Show more button not found or not clickable\")\n",
    "                \n",
    "                # Wait for a random amount of time before scrolling to the next page\n",
    "                time.sleep(random.choice(list(range(3, 7))))\n",
    "            \n",
    "            # Scrape the job postings\n",
    "            jobs = []\n",
    "            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Updated selectors for current LinkedIn structure\n",
    "            job_listings = soup.find_all(\n",
    "                \"div\",\n",
    "                class_=\"base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card\",\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Found {len(job_listings)} job listings to process\")\n",
    "            \n",
    "            for idx, job in enumerate(job_listings):\n",
    "                try:\n",
    "                    # Extract job details with error handling\n",
    "                    job_title_elem = job.find(\"h3\", class_=\"base-search-card__title\")\n",
    "                    job_title_text = job_title_elem.text.strip() if job_title_elem else \"N/A\"\n",
    "                    \n",
    "                    job_company_elem = job.find(\"h4\", class_=\"base-search-card__subtitle\")\n",
    "                    job_company_text = job_company_elem.text.strip() if job_company_elem else \"N/A\"\n",
    "                    \n",
    "                    job_location_elem = job.find(\"span\", class_=\"job-search-card__location\")\n",
    "                    job_location_text = job_location_elem.text.strip() if job_location_elem else \"N/A\"\n",
    "                    \n",
    "                    apply_link_elem = job.find(\"a\", class_=\"base-card__full-link\")\n",
    "                    apply_link = apply_link_elem[\"href\"] if apply_link_elem else \"N/A\"\n",
    "                    \n",
    "                    # Navigate to the job posting page and scrape the description\n",
    "                    if apply_link != \"N/A\":\n",
    "                        self.driver.get(apply_link)\n",
    "                        time.sleep(random.choice(list(range(5, 11))))\n",
    "                        \n",
    "                        try:\n",
    "                            description_soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "                            job_description_elem = description_soup.find(\n",
    "                                \"div\", class_=\"description__text description__text--rich\"\n",
    "                            )\n",
    "                            job_description = job_description_elem.text.strip() if job_description_elem else \"Description not available\"\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Could not retrieve job description: {str(e)}\")\n",
    "                            job_description = \"Description not available\"\n",
    "                    else:\n",
    "                        job_description = \"Description not available\"\n",
    "                    \n",
    "                    # Add job details to the jobs list\n",
    "                    job_data = {\n",
    "                        \"title\": job_title_text,\n",
    "                        \"company\": job_company_text,\n",
    "                        \"location\": job_location_text,\n",
    "                        \"link\": apply_link,\n",
    "                        \"description\": job_description,\n",
    "                        \"searched_for\": job_title,\n",
    "                        \"experience_level_filter\": experience_level,\n",
    "                        \"days_back\": days_back,\n",
    "                        \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    jobs.append(job_data)\n",
    "                    logging.info(f'Scraped \"{job_title_text}\" at {job_company_text} in {job_location_text}...')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing job listing {idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logging.info(f\"Successfully scraped {len(jobs)} jobs\")\n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while scraping jobs: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"WebDriver closed successfully\")\n",
    "    \n",
    "    # Add methods to the class\n",
    "    LinkedInJobScraper.scrape_jobs = scrape_jobs\n",
    "    LinkedInJobScraper.close = close\n",
    "\n",
    "# Execute the function to add methods to the class\n",
    "add_methods_to_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ff3776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for job scraping and data management\n",
    "\n",
    "def save_jobs_to_csv(jobs_data: list, filename: str = \"linkedin_jobs.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Save job data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        jobs_data: A list of dictionaries containing job data\n",
    "        filename: Name of the CSV file to save to\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not jobs_data:\n",
    "        logging.warning(\"No job data to save\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create a pandas DataFrame from the job data\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file without including the index column\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Log a message indicating how many jobs were successfully scraped and saved\n",
    "        logging.info(f\"Successfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        print(f\"âœ… Saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving jobs to CSV: {str(e)}\")\n",
    "        print(f\"âŒ Error saving jobs to CSV: {str(e)}\")\n",
    "\n",
    "def scrape_jobs_for_resume(resume_data: dict, pages_per_role: int = 1, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Scrape LinkedIn jobs based on resume analysis results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_data : dict\n",
    "        Resume analysis data containing recommended_roles and experience_level\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Combined list of all scraped jobs\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    # Extract recommended roles and experience level\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    experience_level = resume_data.get(\"experience_level\", \"\")\n",
    "    \n",
    "    if not recommended_roles:\n",
    "        logging.warning(\"No recommended roles found in resume data\")\n",
    "        print(\"âš ï¸ No recommended roles found in resume data\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Found {len(recommended_roles)} recommended roles: {recommended_roles}\")\n",
    "    print(f\"ðŸ“Š Experience Level: {experience_level}\")\n",
    "    print(f\"â° Looking for jobs posted in the last {days_back} days\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nðŸ” Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"âœ… Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Total jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"âŒ Error during job scraping: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def analyze_and_scrape_jobs(resume_path: str, file_type: str = \"pdf\", \n",
    "                          pages_per_role: int = 1, days_back: int = 7) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Analyze resume and scrape relevant jobs\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_path : str\n",
    "        Path to the resume file\n",
    "    file_type : str\n",
    "        Type of resume file (pdf or docx)\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Combined resume analysis and job scraping results\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Extract and analyze resume\n",
    "    print(\"ðŸ“„ Step 1: Analyzing resume...\")\n",
    "    text = extract_text_from_file(resume_path, file_type)\n",
    "    \n",
    "    try:\n",
    "        raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "        resume_data = json.loads(raw_output)\n",
    "        \n",
    "        print(\"âœ… Resume analysis completed\")\n",
    "        print(f\"ðŸ‘¤ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "        print(f\"ðŸ“ Location: {resume_data.get('location', 'N/A')}\")\n",
    "        print(f\"ðŸ“Š Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "        print(f\"ðŸŽ¯ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error analyzing resume: {str(e)}\")\n",
    "        return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    # Step 2: Scrape jobs based on analysis\n",
    "    print(f\"\\nðŸ” Step 2: Scraping jobs for recommended roles...\")\n",
    "    scraped_jobs = scrape_jobs_for_resume(resume_data, pages_per_role, days_back)\n",
    "    \n",
    "    # Step 3: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": scraped_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(scraped_jobs),\n",
    "            \"recommended_roles_searched\": resume_data.get(\"recommended_roles\", []),\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    if scraped_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"jobs_{resume_data.get('name', 'candidate').replace(' ', '_')}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(scraped_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f998073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing Fixed Pipeline...\n",
      "ðŸš€ Starting Resume Analysis and Job Scraping Pipeline\n",
      "================================================================================\n",
      "ðŸ“„ Step 1: Analyzing resume...\n",
      "âœ… Resume analysis completed\n",
      "ðŸ‘¤ Candidate: KODIPAKA SAIDIVYA\n",
      "ðŸ“ Location: India\n",
      "ðŸ“Š Experience Level: Entry Level\n",
      "ðŸŽ¯ Recommended Roles: ['Junior Electrical Engineer', 'Data Science Intern', 'Machine Learning Intern']\n",
      "\n",
      "ðŸ” Step 2: Scraping real jobs for 3 roles...\n",
      "âš ï¸ This will open a browser window and scrape real LinkedIn jobs\n",
      "\n",
      "ðŸ” Scraping jobs for role 1/3: 'Junior Electrical Engineer'\n",
      "âœ… Found 15 jobs for 'Junior Electrical Engineer'\n",
      "â³ Waiting before next search...\n",
      "\n",
      "ðŸ” Scraping jobs for role 2/3: 'Data Science Intern'\n",
      "âœ… Found 24 jobs for 'Data Science Intern'\n",
      "â³ Waiting before next search...\n",
      "\n",
      "ðŸ” Scraping jobs for role 3/3: 'Machine Learning Intern'\n",
      "âœ… Found 30 jobs for 'Machine Learning Intern'\n",
      "\n",
      "ðŸŽ‰ Total real jobs scraped: 69\n",
      "ðŸ”’ Browser closed\n",
      "âœ… Saved 69 jobs to jobs_KODIPAKA_SAIDIVYA_20251008_151622.csv\n",
      "ðŸ’¾ Jobs saved to: jobs_KODIPAKA_SAIDIVYA_20251008_151622.csv\n",
      "\n",
      "ðŸŽ‰ Pipeline completed successfully!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š PIPELINE SUMMARY:\n",
      "==================================================\n",
      "Total Jobs Found: 69\n",
      "Roles Searched: ['Junior Electrical Engineer', 'Data Science Intern', 'Machine Learning Intern']\n",
      "Experience Level: Entry Level\n",
      "Status: Real LinkedIn jobs scraped\n",
      "CSV File: jobs_KODIPAKA_SAIDIVYA_20251008_151622.csv\n"
     ]
    }
   ],
   "source": [
    "# Fixed version with better error handling\n",
    "def analyze_and_scrape_jobs_fixed(resume_path: str, file_type: str = \"pdf\", \n",
    "                                pages_per_role: int = 1, days_back: int = 7) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Analyze resume and scrape relevant jobs (with better error handling)\n",
    "    \"\"\"\n",
    "    print(\"ðŸš€ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Extract and analyze resume\n",
    "    print(\"ðŸ“„ Step 1: Analyzing resume...\")\n",
    "    text = extract_text_from_file(resume_path, file_type)\n",
    "    \n",
    "    try:\n",
    "        raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "        \n",
    "        # Clean the output to handle markdown formatting\n",
    "        cleaned_output = raw_output.strip()\n",
    "        if cleaned_output.startswith(\"```json\"):\n",
    "            cleaned_output = cleaned_output[7:]\n",
    "        if cleaned_output.endswith(\"```\"):\n",
    "            cleaned_output = cleaned_output[:-3]\n",
    "        cleaned_output = cleaned_output.strip()\n",
    "        \n",
    "        # Try to parse JSON\n",
    "        try:\n",
    "            resume_data = json.loads(cleaned_output)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"âŒ JSON parsing failed: {e}\")\n",
    "            \n",
    "            # Try to extract JSON from the output\n",
    "            json_start = cleaned_output.find('{')\n",
    "            if json_start != -1:\n",
    "                json_part = cleaned_output[json_start:]\n",
    "                try:\n",
    "                    resume_data = json.loads(json_part)\n",
    "                    print(\"âœ… JSON parsing successful after extraction!\")\n",
    "                except json.JSONDecodeError as e2:\n",
    "                    print(f\"âŒ Still failed after extraction: {e2}\")\n",
    "                    return {\"error\": f\"JSON parsing failed: {e2}\"}\n",
    "            else:\n",
    "                return {\"error\": f\"No JSON found in output: {cleaned_output[:200]}...\"}\n",
    "        \n",
    "        print(\"âœ… Resume analysis completed\")\n",
    "        print(f\"ðŸ‘¤ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "        print(f\"ðŸ“ Location: {resume_data.get('location', 'N/A')}\")\n",
    "        print(f\"ðŸ“Š Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "        print(f\"ðŸŽ¯ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error analyzing resume: {str(e)}\")\n",
    "        return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    # Step 2: Check if we have recommended roles\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    if not recommended_roles:\n",
    "        print(\"âš ï¸ No recommended roles found. Cannot scrape jobs.\")\n",
    "        return {\n",
    "            \"resume_analysis\": resume_data,\n",
    "            \"scraped_jobs\": [],\n",
    "            \"summary\": {\n",
    "                \"total_jobs_found\": 0,\n",
    "                \"recommended_roles_searched\": [],\n",
    "                \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "                \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"status\": \"No recommended roles found\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Step 3: Scrape jobs based on analysis\n",
    "    print(f\"\\nðŸ” Step 2: Scraping real jobs for {len(recommended_roles)} roles...\")\n",
    "    print(\"âš ï¸ This will open a browser window and scrape real LinkedIn jobs\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nðŸ” Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=resume_data.get(\"experience_level\", \"\"),\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"âœ… Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                print(f\"â³ Waiting before next search...\")\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Total real jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"âŒ Error during job scraping: {str(e)}\")\n",
    "        all_jobs = []\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "        print(\"ðŸ”’ Browser closed\")\n",
    "    \n",
    "    # Step 4: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": all_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(all_jobs),\n",
    "            \"recommended_roles_searched\": recommended_roles,\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"status\": \"Real LinkedIn jobs scraped\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    if all_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        candidate_name = resume_data.get('name', 'candidate').replace(' ', '_')\n",
    "        csv_filename = f\"jobs_{candidate_name}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(all_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "        print(f\"ðŸ’¾ Jobs saved to: {csv_filename}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No jobs found to save\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"ðŸ§ª Testing Fixed Pipeline...\")\n",
    "result = analyze_and_scrape_jobs_fixed(\n",
    "    resume_path=\"app/resumes/SaidivyaResume.pdf\",\n",
    "    file_type=\"pdf\",\n",
    "    pages_per_role=1,\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Print summary safely\n",
    "print(\"\\nðŸ“Š PIPELINE SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "if \"summary\" in result:\n",
    "    print(f\"Total Jobs Found: {result['summary']['total_jobs_found']}\")\n",
    "    print(f\"Roles Searched: {result['summary']['recommended_roles_searched']}\")\n",
    "    print(f\"Experience Level: {result['summary']['experience_level']}\")\n",
    "    print(f\"Status: {result['summary']['status']}\")\n",
    "    if 'csv_file' in result:\n",
    "        print(f\"CSV File: {result['csv_file']}\")\n",
    "else:\n",
    "    print(f\"âŒ Error: {result.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple function to scrape jobs for specific roles\n",
    "def scrape_jobs_directly(job_roles: list, location: str = \"India\", pages: int = 1, \n",
    "                        experience_level: str = None, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Directly scrape jobs for given roles without resume analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    job_roles : list\n",
    "        List of job titles to search for\n",
    "    location : str\n",
    "        Location to search in (default: \"India\")\n",
    "    pages : int\n",
    "        Number of pages to scrape per role\n",
    "    experience_level : str\n",
    "        Experience level filter\n",
    "    days_back : int\n",
    "        Days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of scraped job dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"ðŸ” Direct Job Scraping for {len(job_roles)} roles\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"ðŸ“ Location: {location}\")\n",
    "    print(f\"ðŸ“Š Experience Level: {experience_level or 'Any'}\")\n",
    "    print(f\"â° Days Back: {days_back}\")\n",
    "    print(f\"ðŸ“„ Pages per role: {pages}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)\n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(job_roles, 1):\n",
    "            print(f\"\\nðŸ” [{i}/{len(job_roles)}] Scraping: '{role}'\")\n",
    "            \n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=location,\n",
    "                pages=pages,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"âœ… Found {len(jobs)} jobs for '{role}'\")\n",
    "                \n",
    "                # Show first few job titles as preview\n",
    "                for j, job in enumerate(jobs[:3], 1):\n",
    "                    print(f\"   {j}. {job['title']} at {job['company']}\")\n",
    "                if len(jobs) > 3:\n",
    "                    print(f\"   ... and {len(jobs) - 3} more\")\n",
    "            else:\n",
    "                print(f\"âš ï¸ No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between searches\n",
    "            if i < len(job_roles):\n",
    "                delay = random.choice(list(range(5, 10)))\n",
    "                print(f\"â³ Waiting {delay} seconds before next search...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        print(f\"\\nðŸŽ‰ Scraping completed!\")\n",
    "        print(f\"ðŸ“Š Total jobs found: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during scraping: {str(e)}\")\n",
    "        logging.error(f\"Direct scraping error: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        scraper.close()\n",
    "        print(\"ðŸ”’ Browser closed\")\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "\"\"\"\n",
    "# Test with specific roles\n",
    "test_roles = [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\n",
    "jobs = scrape_jobs_directly(\n",
    "    job_roles=test_roles,\n",
    "    location=\"India\", \n",
    "    pages=1,\n",
    "    experience_level=\"Junior\",\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Save results\n",
    "if jobs:\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"direct_jobs_{timestamp}.csv\"\n",
    "    save_jobs_to_csv(jobs, filename)\n",
    "    print(f\"ðŸ’¾ Saved {len(jobs)} jobs to {filename}\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
