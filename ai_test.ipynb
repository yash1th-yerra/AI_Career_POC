{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7058eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "import pdfplumber\n",
    "import docx2txt\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43d5e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ADVANCED JOB MATCHING SYSTEM WITH EMBEDDINGS\n",
    "# ============================================================================\n",
    "# This section contains the embedding-based job matching system\n",
    "# Move this to the bottom as requested\n",
    "\n",
    "# Additional imports for embedding system\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139e825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env for API key\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c3295eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not api_key:\n",
    "    raise ValueError(\"‚ùå GOOGLE_API_KEY not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6196d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759950126.485457    4873 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    }
   ],
   "source": [
    "# Initialize Gemini 2.5 Flash LLM with safe optimizations (maintains consistency)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2,  # Restore original temperature for consistency\n",
    "    api_key=api_key,\n",
    "    # max_output_tokens=4096,  # Set explicit limit for faster response\n",
    "    max_retries=2,  # Keep reasonable retries for reliability\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a109c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_file(file_path: str, file_type: str) -> str:\n",
    "    \"\"\"Extract text from PDF or DOCX resume.\"\"\"\n",
    "    if file_type.lower() == \"pdf\":\n",
    "        text_parts = []\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            max_pages = min(2, len(pdf.pages))\n",
    "            for i in range(max_pages):\n",
    "                text_parts.append(pdf.pages[i].extract_text() or \"\")\n",
    "        text = \"\\n\".join(text_parts)\n",
    "    elif file_type.lower() == \"docx\":\n",
    "        text = docx2txt.process(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type\")\n",
    "\n",
    "    # Optional cleaning\n",
    "    text = text.replace('\\n', ' ').replace('\\t', ' ').strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9814675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4873/85761643.py:106: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  resume_parser_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "# Combined Unified Resume Extraction with JSON Output (Fixed Parser Issue)\n",
    "import json\n",
    "\n",
    "# Create a LangChain prompt with direct JSON output\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert Resume Intelligence Agent that extracts structured data and evaluates resumes for ATS compatibility.\n",
    "\n",
    "Analyze the following resume text and return ONLY a valid JSON object with these exact keys:\n",
    "\n",
    "{{\n",
    "  \"name\": \"\",\n",
    "  \"location\": \"\",\n",
    "  \"summary\": \"\",\n",
    "  \"skills\": [],\n",
    "  \"extra_skills\": [],\n",
    "  \"work_experience\": [],\n",
    "  \"projects\": [],\n",
    "  \"certifications\": [],\n",
    "  \"education\": [],\n",
    "  \"experience_level\": \"\",\n",
    "  \"recommended_roles\": [],\n",
    "  \"ats_feedback\": {{\n",
    "    \"score\": 0,\n",
    "    \"summary\": \"\",\n",
    "    \"strengths\": [],\n",
    "    \"improvements\": []\n",
    "  }}\n",
    "}}\n",
    "\n",
    "CRITICAL EXTRACTION RULES FOR ALL SECTIONS:\n",
    "\n",
    "1. **NAME**: Extract the full name exactly as written, using the most prominent name (usually at the top).\n",
    "\n",
    "2. **LOCATION**: Extract specific city and country from contact info, address, or personal details. Format as \"City, Country\" (e.g., \"Chennai, India\", \"Bangalore, India\"). If no city is specified, use \" Country\".\n",
    "\n",
    "3. **SUMMARY**: Look for sections titled \"Summary\", \"Objective\", \"Profile\", \"About Me\", \"Career Summary\". Extract complete professional summary.\n",
    "\n",
    "4. **SKILLS**: Extract skills ONLY from dedicated \"Skills\", \"Technical Skills\", \"Core Skills\", \"Programming Languages\", or similar sections:\n",
    "   - ONLY include skills explicitly listed in a dedicated skills section\n",
    "   - Programming languages, frameworks, tools, technologies mentioned in skills section\n",
    "   - Return as array of individual skills from the skills section only\n",
    "\n",
    "5. **EXTRA_SKILLS**: Extract additional skills mentioned in other contexts:\n",
    "   - Skills mentioned in work experience descriptions\n",
    "   - Technologies used in projects\n",
    "   - Skills mentioned in certifications or education\n",
    "   - Any other skills not in the main skills section\n",
    "   - Return as array of individual skills from non-skills sections\n",
    "\n",
    "6. **WORK_EXPERIENCE**: Extract each position with:\n",
    "   - Job title, company, duration, location\n",
    "   - Key responsibilities and achievements\n",
    "   - Format as structured objects with consistent fields\n",
    "\n",
    "7. **PROJECTS**: Extract personal/academic projects with:\n",
    "   - Project name, duration, technologies used\n",
    "   - Brief description and key features\n",
    "   - Any notable achievements or results\n",
    "\n",
    "8. **CERTIFICATIONS**: Extract all certifications with:\n",
    "   - Certification name, issuing organization, year\n",
    "   - Include online courses, professional certifications\n",
    "\n",
    "9. **EDUCATION**: Extract educational background with:\n",
    "   - Degree, institution, graduation year\n",
    "   - Relevant coursework or achievements\n",
    "\n",
    "10. **EXPERIENCE_LEVEL**: Analyze the candidate's work experience and determine their experience level:\n",
    "    - \"Entry Level\" (0-1 years): Fresh graduates, internships, or minimal professional experience\n",
    "    - \"Junior\" (1-3 years): Some professional experience, early career roles\n",
    "    - \"Mid-Level\" (3-7 years): Solid professional experience, can work independently\n",
    "    - \"Senior\" (7-12 years): Advanced experience, can lead projects and mentor others\n",
    "    - \"Lead/Principal\" (12+ years): Expert level, can architect solutions and lead teams\n",
    "    - Consider total years of experience, complexity of roles, leadership responsibilities\n",
    "    - Return a single string value\n",
    "\n",
    "11. **RECOMMENDED_ROLES**: Based on the candidate's skills, experience, education, and projects, recommend 2-3 specific job roles they would be suitable for:\n",
    "    - Consider their technical skills, domain expertise, and career progression\n",
    "    - Include roles that match their current skill level and potential growth areas\n",
    "    - Format as array of role titles (e.g., [\"Software Engineer\", \"Data Analyst\", \"Frontend Developer\"])\n",
    "    - Be specific and industry-relevant\n",
    "\n",
    "12. **ATS_FEEDBACK**: Provide objective evaluation:\n",
    "    - score: 0-100 based on ATS compatibility\n",
    "    - summary: Brief assessment\n",
    "    - strengths: Positive aspects\n",
    "    - improvements: Areas for enhancement \n",
    "\n",
    "Guidelines:\n",
    "- Detect section names dynamically (e.g., \"Profile\", \"About Me\", \"Objective\" ‚Üí summary).\n",
    "- CRITICAL: Skills extraction must be source-aware:\n",
    "  * \"skills\" array: ONLY from dedicated skills sections (Skills, Technical Skills, Core Skills, Programming Languages, etc.)\n",
    "  * \"extra_skills\" array: Skills mentioned in work experience, projects, certifications, education, or other contexts\n",
    "- Extract job/project details separately.\n",
    "- For EXPERIENCE_LEVEL: Analyze total years of professional experience, role complexity, and leadership indicators\n",
    "- For RECOMMENDED_ROLES: Analyze the candidate's profile holistically and suggest roles that align with their skills and experience level\n",
    "- Be consistent and produce clean JSON only.\n",
    "- Prioritize accuracy over completeness.\n",
    "- IMPORTANT: Return ONLY the JSON object, no additional text or explanations.\n",
    "\n",
    "Resume Text:\n",
    "{resume_text}\n",
    "\"\"\")\n",
    "\n",
    "# Build the chain with StrOutputParser for better JSON handling\n",
    "resume_parser_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    output_parser=StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57a559f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Parsing Error: Expecting value: line 1 column 1 (char 0)\n",
      "Raw output that failed to parse:\n",
      "```json\n",
      "{\n",
      "  \"name\": \"Yeswanth Yerra\",\n",
      "  \"location\": \"Chennai, India\",\n",
      "  \"summary\": \"\",\n",
      "  \"skills\": [\n",
      "    \"Java\",\n",
      "    \"JavaScript\",\n",
      "    \"Python\",\n",
      "    \"Data Structures & Algorithms\",\n",
      "    \"Computer Networks\",\n",
      "    \"Operating Systems\",\n",
      "    \"DBMS\",\n",
      "    \"Spring Boot\",\n",
      "    \"Hibernate\",\n",
      "    \"React.js\",\n",
      "    \"Flask\",\n",
      "    \"Git\",\n",
      "    \"Docker\",\n",
      "    \"Terraform\",\n",
      "    \"Ansible\",\n",
      "    \"Jenkins\",\n",
      "    \"MySQL\",\n",
      "    \"MongoDB\",\n",
      "    \"PostgreSQL\",\n",
      "    \"AWS\",\n",
      "    \"Kubernetes\"\n",
      "  ],\n",
      "  \"extra_skills\": [\n",
      "    \"RESTful APIs\",\n",
      "    \"Design Patterns\",\n",
      "    \"Microservice Architecture\",\n",
      "    \"Scalable Web Development\",\n",
      "    \"Authentication\",\n",
      "    \"Authorization\",\n",
      "    \"Clean Coding Practices\",\n",
      "    \"JUnit5\",\n",
      "    \"Mockito\",\n",
      "    \"Containerization\",\n",
      "    \"Collaborative Filtering\",\n",
      "    \"Content-Based Filtering\",\n",
      "    \"Matrix Factorization\",\n",
      "    \"TF-IDF\",\n",
      "    \"ALS Matrix Factorization\",\n",
      "    \"BiLSTM\",\n",
      "    \"Attention Layer\",\n",
      "    \"Hyperparameter Tuning\",\n",
      "    \"Grid Search\",\n",
      "    \"Random Search\",\n",
      "    \"Eureka\",\n",
      "    \"Feign Clients\",\n",
      "    \"JWT-based Authentication\",\n",
      "    \"Role-Based Security\",\n",
      "    \"RabbitMQ\",\n",
      "    \"Auto-scaling\",\n",
      "    \"Caching Strategies\",\n",
      "    \"Cosine Similarity\",\n",
      "    \"Render\",\n",
      "    \"Data Science\",\n",
      "    \"Software Engineering\",\n",
      "    \"GenAI\",\n",
      "    \"Machine Learning\",\n",
      "    \"Accounting\",\n",
      "    \"Data Analytics\"\n",
      "  ],\n",
      "  \"work_experience\": [\n",
      "    {\n",
      "      \"job_title\": \"Full Stack Development Intern\",\n",
      "      \"company\": \"Pantech Prolabs Pvt Ltd\",\n",
      "      \"duration\": \"June 2024 - October 2024\",\n",
      "      \"location\": \"Chennai, TN (Remote)\",\n",
      "      \"responsibilities\": [\n",
      "        \"Engineered a web application using Spring Boot for the backend, employing RESTful APIs and efficient database querying with MongoDB.\",\n",
      "        \"Gained expertise in design patterns and microservice architecture for building efficient, modular systems.\",\n",
      "        \"Applied scalable web development practices to ensure optimal performance under increasing user loads and Implemented authentication and authorization.\",\n",
      "        \"Maintained high code quality by adhering to clean coding practices and leveraging testing frameworks like JUnit5 and Mockito, achieving 95% test coverage.\",\n",
      "        \"Enhanced application reliability with Docker containerization, cutting deployment time by 50% and ensuring consistent performance across all environments.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"projects\": [\n",
      "    {\n",
      "      \"project_name\": \"Movie Recommendation System\",\n",
      "      \"duration\": \"October 2021 - March 2022\",\n",
      "      \"technologies\": \"collaborative filtering, content-based filtering, matrix factorization, TF-IDF, ALS matrix factorization\",\n",
      "      \"description\": \"Led a team of 4 members for movie recommendation system by integrating collaborative filtering, content-based filtering, and matrix factorization.\",\n",
      "      \"achievements\": [\n",
      "        \"Resulted in a 33% improvement in accuracy using the MovieLens dataset.\",\n",
      "        \"Extracted metadata features and employed TF-IDF for precise vector representation and enhancing feature quality by 30%.\",\n",
      "        \"Reduced RMSE from 2.2489 to 1.1000 by finetuning ALS matrix factorization for collaborative filtering.\",\n",
      "        \"Addressed sparsity and cold-start challenges effectively using content-based filtering, reducing cold-start failures by 50% and improving recommendation coverage.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Network Intrusion Detection System (NIDS)\",\n",
      "      \"duration\": \"October 2022 - March 2023\",\n",
      "      \"technologies\": \"BiLSTM, attention layer, NSL-KDD dataset, hyperparameter tuning, grid search, random search\",\n",
      "      \"description\": \"Engineered a Network Intrusion Detection System Utilizing BiLSTM to capture both forward and backward dependencies, Integrated an attention layer to prioritize critical features.\",\n",
      "      \"achievements\": [\n",
      "        \"Resulted in a 25% increase in detection accuracy.\",\n",
      "        \"Trained the model on the refined NSL-KDD dataset, addressing class imbalance and boosting overall performance by 30%.\",\n",
      "        \"Implemented hyperparameter tuning using grid search and random search, optimizing the BiLSTM network‚Äôs learning rate, batch size, and number of hidden layers, leading to a 20% increase in accuracy.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Job Application Microservices API\",\n",
      "      \"duration\": \"October 2023 - December 2023\",\n",
      "      \"technologies\": \"Spring Boot, Eureka, Feign clients, JWT, RabbitMQ, Kubernetes\",\n",
      "      \"description\": \"Designed and deployed a microservices architecture using Spring Boot, Eureka, and Feign clients. Implemented JWT-based authentication and role-based security. Integrated RabbitMQ for asynchronous communication. Deployed the application on Kubernetes.\",\n",
      "      \"achievements\": [\n",
      "        \"Enabled seamless communication between 3+ services, with 99.9% availability.\",\n",
      "        \"Implemented JWT-based authentication and role-based security across 3 microservices, securing 100+ endpoints with 2 or more roles per service.\",\n",
      "        \"Integrated RabbitMQ for asynchronous communication, handling up to 50,000 messages per day between microservices, improving system performance and reliability.\",\n",
      "        \"Deployed the application on Kubernetes, leveraging auto-scaling and caching strategies to efficiently handle traffic spikes and ensure high availability for up to 100,000 concurrent users.\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"project_name\": \"Simple Song Search\",\n",
      "      \"duration\": \"November 2024 - December 2024\",\n",
      "      \"technologies\": \"Flask, TF-IDF, cosine similarity, Render\",\n",
      "      \"description\": \"Engineered a robust song search application using Flask and advanced algorithms like TF-IDF and cosine similarity across a dataset of over 50,000 tracks. Designed a responsive web interface and deployed the application on Render.\",\n",
      "      \"achievements\": [\n",
      "        \"Improving search speed and accuracy by 40%.\",\n",
      "        \"Achieving a 99.9% deployment success rate.\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"certifications\": [\n",
      "    {\n",
      "      \"name\": \"DataScience for Engineers\",\n",
      "      \"issuing_organization\": \"NPTEL-IIT MADRAS\",\n",
      "      \"year\": \"2023\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Software Engineering Job Simulation\",\n",
      "      \"issuing_organization\": \"Hewlett Packard Enterprise (Forage)\",\n",
      "      \"year\": \"2024\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"GenAI path in Google Cloud Platform\",\n",
      "      \"issuing_organization\": \"Google\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Java 17 MasterClass\",\n",
      "      \"issuing_organization\": \"Udemy\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Machine Learning Using Python\",\n",
      "      \"issuing_organization\": \"SimpliLearn\",\n",
      "      \"year\": null\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Accounting and Data Analytics with Python\",\n",
      "      \"issuing_organization\": \"Coursera-Illinois University\",\n",
      "      \"year\": null\n",
      "    }\n",
      "  ],\n",
      "  \"education\": [\n",
      "    {\n",
      "      \"degree\": \"Bachelor of Technology, Computer Science and Engineering\",\n",
      "      \"institution\": \"Pragati Engineering College Surampalem, AP\",\n",
      "      \"graduation_year\": \"2023\",\n",
      "      \"achievements\": \"GPA- 8.14/10.0\"\n",
      "    },\n",
      "    {\n",
      "      \"degree\": \"Intermediate\",\n",
      "      \"institution\": \"Sasi junior College Eluru, AP\",\n",
      "      \"graduation_year\": \"2019\",\n",
      "      \"achievements\": \"GPA-9.8/10.0\"\n",
      "    }\n",
      "  ],\n",
      "  \"experience_level\": \"Entry Level\",\n",
      "  \"recommended_roles\": [\n",
      "    \"Junior Full Stack Developer\",\n",
      "    \"Junior Backend Developer\",\n",
      "    \"Junior Software Engineer\"\n",
      "  ],\n",
      "  \"ats_feedback\": {\n",
      "    \"score\": 85,\n",
      "    \"summary\": \"The candidate demonstrates strong technical skills in full-stack development, cloud, and machine learning, backed by practical project experience and an internship. The resume is well-organized and highlights quantifiable achievements.\",\n",
      "    \"strengths\": [\n",
      "      \"Clear and organized structure with dedicated sections for skills, experience, and projects.\",\n",
      "      \"Quantifiable achievements in work experience and projects (e.g., \\\"95% test coverage\\\", \\\"33% improvement in accuracy\\\", \\\"50% deployment time cut\\\").\",\n",
      "      \"Strong technical skill set covering multiple domains including Java, Python, web development, cloud, and machine learning.\",\n",
      "      \"Experience with modern tools and frameworks such as Spring Boot, React.js, Docker, Kubernetes, and AWS.\",\n",
      "      \"Good project diversity showcasing problem-solving and implementation skills across different areas.\"\n",
      "    ],\n",
      "    \"improvements\": [\n",
      "      \"Add a professional summary or objective statement to provide a concise overview and career goals.\",\n",
      "      \"Ensure consistent formatting for dates and locations (e.g., \\\"Chennai, TN\\\" vs \\\"Surampalem, AP\\\").\",\n",
      "      \"Clarify the 'Simple Song Search' project duration (November 2024 - December 2024) if it is a future or ongoing project, or a typo for a past project.\"\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Extract text\n",
    "resume_path = \"app/resumes/Yeswanth_Yerra_CV.pdf\"\n",
    "text = extract_text_from_file(resume_path, \"pdf\")\n",
    "\n",
    "# Step 2: Parse with Gemini\n",
    "try:\n",
    "    raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "    # print(\"Raw LLM Output:\")\n",
    "    # print(raw_output)\n",
    "    # print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Parse JSON from the output\n",
    "    structured_output = json.loads(raw_output)\n",
    "    print(\"Parsed JSON Output:\")\n",
    "    print(json.dumps(structured_output, indent=2))\n",
    "    \n",
    "    # Display experience level prominently\n",
    "    # if \"experience_level\" in structured_output and structured_output[\"experience_level\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"üìä EXPERIENCE LEVEL:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     print(f\"Level: {structured_output['experience_level']}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "    # # Display recommended roles prominently\n",
    "    # if \"recommended_roles\" in structured_output and structured_output[\"recommended_roles\"]:\n",
    "    #     print(\"\\n\" + \"=\"*60)\n",
    "    #     print(\"üéØ RECOMMENDED ROLES FOR THIS CANDIDATE:\")\n",
    "    #     print(\"=\"*60)\n",
    "    #     for i, role in enumerate(structured_output[\"recommended_roles\"], 1):\n",
    "    #         print(f\"{i}. {role}\")\n",
    "    #     print(\"=\"*60)\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"JSON Parsing Error: {e}\")\n",
    "    print(\"Raw output that failed to parse:\")\n",
    "    print(raw_output)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Raw output:\")\n",
    "    print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c197ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkedIn Job Scraper Integration\n",
    "# Import necessary packages for web scraping and logging\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "# Configure logging settings\n",
    "logging.basicConfig(filename=\"linkedin_scraping.log\", level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40393c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkedInJobScraper:\n",
    "    def __init__(self, headless=False):\n",
    "        \"\"\"\n",
    "        Initialize the LinkedIn Job Scraper\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        headless : bool\n",
    "            Whether to run Chrome in headless mode (default: False)\n",
    "        \"\"\"\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.setup_driver()\n",
    "    \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Setup Chrome WebDriver with appropriate options\"\"\"\n",
    "        try:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            \n",
    "            # Basic options\n",
    "            options.add_argument(\"--start-maximized\")\n",
    "            options.add_argument(\"--no-sandbox\")\n",
    "            options.add_argument(\"--headless\")\n",
    "            options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "            options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "            options.add_experimental_option('useAutomationExtension', False)\n",
    "            \n",
    "            # User agent to avoid detection\n",
    "            options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "            \n",
    "            if self.headless:\n",
    "                options.add_argument(\"--headless\")\n",
    "            \n",
    "            self.driver = webdriver.Chrome(options=options)\n",
    "            self.driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            \n",
    "            logging.info(\"Chrome WebDriver initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to initialize WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def build_search_url(self, job_title: str, location: str = \"India\", experience_level: str = None, \n",
    "                        time_posted: str = None, remote: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Build LinkedIn job search URL with filters\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        job_title : str\n",
    "            Job title to search for\n",
    "        location : str\n",
    "            Location to search in (default: \"India\")\n",
    "        experience_level : str\n",
    "            Experience level filter (Entry level, Associate, Mid-Senior level, Director, Executive)\n",
    "        time_posted : str\n",
    "            Time posted filter (r86400, r604800, r2592000, r31536000)\n",
    "        remote : bool\n",
    "            Whether to include remote jobs only\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete LinkedIn job search URL\n",
    "        \"\"\"\n",
    "        base_url = \"https://www.linkedin.com/jobs/search/\"\n",
    "        \n",
    "        # URL encode parameters\n",
    "        job_title_encoded = urllib.parse.quote(job_title)\n",
    "        location_encoded = urllib.parse.quote(location)\n",
    "        \n",
    "        # Build query parameters\n",
    "        params = {\n",
    "            \"keywords\": job_title_encoded,\n",
    "            \"location\": location_encoded,\n",
    "            \"f_TPR\": time_posted if time_posted else None,  # Time posted filter\n",
    "            \"f_E\": self._get_experience_filter(experience_level) if experience_level else None,  # Experience filter\n",
    "            \n",
    "        }\n",
    "        \n",
    "        # Filter out None values and build query string\n",
    "        query_params = {k: v for k, v in params.items() if v is not None}\n",
    "        query_string = \"&\".join([f\"{k}={v}\" for k, v in query_params.items()])\n",
    "        \n",
    "        full_url = f\"{base_url}?{query_string}\"\n",
    "        logging.info(f\"Built search URL: {full_url}\")\n",
    "        \n",
    "        return full_url\n",
    "    \n",
    "    def _get_experience_filter(self, experience_level: str) -> str:\n",
    "        \"\"\"\n",
    "        Map experience level to LinkedIn filter values\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        experience_level : str\n",
    "            Experience level from resume analysis\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn experience filter value\n",
    "        \"\"\"\n",
    "        experience_mapping = {\n",
    "            \"Entry Level\": \"1\",           # Entry level\n",
    "            \"Junior\": \"2\",                # Associate\n",
    "            \"Mid-Level\": \"3\",             # Mid-Senior level\n",
    "            \"Senior\": \"4\",                # Director\n",
    "            \"Lead/Principal\": \"5\"         # Executive\n",
    "        }\n",
    "        \n",
    "        return experience_mapping.get(experience_level, \"1\")  # Default to Entry level\n",
    "    \n",
    "    def _get_time_filter(self, days: int) -> str:\n",
    "        \"\"\"\n",
    "        Get time posted filter based on days\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        days : int\n",
    "            Number of days to look back\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            LinkedIn time filter value\n",
    "        \"\"\"\n",
    "        time_mapping = {\n",
    "            1: \"r86400\",      # Past 24 hours\n",
    "            7: \"r604800\",     # Past week\n",
    "            30: \"r2592000\",   # Past month\n",
    "            365: \"r31536000\"  # Past year\n",
    "        }\n",
    "        \n",
    "        return time_mapping.get(days, \"r604800\")  # Default to past week\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8927a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the remaining methods to LinkedInJobScraper class\n",
    "def add_methods_to_scraper():\n",
    "    \"\"\"Add the scrape_jobs and close methods to LinkedInJobScraper class\"\"\"\n",
    "    \n",
    "    def scrape_jobs(self, job_title: str, location: str = \"India\", pages: int = 1, \n",
    "                   experience_level: str = None, days_back: int = 7) -> list:\n",
    "        \"\"\"Scrape job listings from LinkedIn with filters\"\"\"\n",
    "        logging.info(f'Starting LinkedIn job scrape for \"{job_title}\" in \"{location}\"...')\n",
    "        \n",
    "        # Build search URL with filters\n",
    "        time_filter = self._get_time_filter(days_back)\n",
    "        search_url = self.build_search_url(\n",
    "            job_title=job_title,\n",
    "            location=location,\n",
    "            experience_level=experience_level,\n",
    "            time_posted=time_filter\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Navigate to the LinkedIn job search page\n",
    "            self.driver.get(search_url)\n",
    "            time.sleep(3)  # Wait for page to load\n",
    "            \n",
    "            # Scroll through the specified number of pages\n",
    "            for i in range(pages):\n",
    "                logging.info(f\"Scrolling to bottom of page {i+1}...\")\n",
    "                \n",
    "                # Scroll to the bottom of the page using JavaScript\n",
    "                self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                \n",
    "                try:\n",
    "                    # Wait for the \"Show more\" button to be present on the page\n",
    "                    element = WebDriverWait(self.driver, 5).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.XPATH, \"/html/body/div[1]/div/main/section[2]/button\")\n",
    "                        )\n",
    "                    )\n",
    "                    # Click on the \"Show more\" button\n",
    "                    element.click()\n",
    "                    logging.info(\"Clicked 'Show more' button\")\n",
    "                    \n",
    "                except Exception:\n",
    "                    logging.info(\"Show more button not found or not clickable\")\n",
    "                \n",
    "                # Wait for a random amount of time before scrolling to the next page\n",
    "                time.sleep(random.choice(list(range(3, 7))))\n",
    "            \n",
    "            # Scrape the job postings\n",
    "            jobs = []\n",
    "            soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "            \n",
    "            # Updated selectors for current LinkedIn structure\n",
    "            job_listings = soup.find_all(\n",
    "                \"div\",\n",
    "                class_=\"base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card\",\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Found {len(job_listings)} job listings to process\")\n",
    "            \n",
    "            for idx, job in enumerate(job_listings):\n",
    "                try:\n",
    "                    # Extract job details with error handling\n",
    "                    job_title_elem = job.find(\"h3\", class_=\"base-search-card__title\")\n",
    "                    job_title_text = job_title_elem.text.strip() if job_title_elem else \"N/A\"\n",
    "                    \n",
    "                    job_company_elem = job.find(\"h4\", class_=\"base-search-card__subtitle\")\n",
    "                    job_company_text = job_company_elem.text.strip() if job_company_elem else \"N/A\"\n",
    "                    \n",
    "                    job_location_elem = job.find(\"span\", class_=\"job-search-card__location\")\n",
    "                    job_location_text = job_location_elem.text.strip() if job_location_elem else \"N/A\"\n",
    "                    \n",
    "                    apply_link_elem = job.find(\"a\", class_=\"base-card__full-link\")\n",
    "                    apply_link = apply_link_elem[\"href\"] if apply_link_elem else \"N/A\"\n",
    "                    \n",
    "                    # Navigate to the job posting page and scrape the description\n",
    "                    if apply_link != \"N/A\":\n",
    "                        self.driver.get(apply_link)\n",
    "                        time.sleep(random.choice(list(range(5, 11))))\n",
    "                        \n",
    "                        try:\n",
    "                            description_soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "                            job_description_elem = description_soup.find(\n",
    "                                \"div\", class_=\"description__text description__text--rich\"\n",
    "                            )\n",
    "                            job_description = job_description_elem.text.strip() if job_description_elem else \"Description not available\"\n",
    "                        except Exception as e:\n",
    "                            logging.warning(f\"Could not retrieve job description: {str(e)}\")\n",
    "                            job_description = \"Description not available\"\n",
    "                    else:\n",
    "                        job_description = \"Description not available\"\n",
    "                    \n",
    "                    # Add job details to the jobs list\n",
    "                    job_data = {\n",
    "                        \"title\": job_title_text,\n",
    "                        \"company\": job_company_text,\n",
    "                        \"location\": job_location_text,\n",
    "                        \"link\": apply_link,\n",
    "                        \"description\": job_description,\n",
    "                        \"searched_for\": job_title,\n",
    "                        \"experience_level_filter\": experience_level,\n",
    "                        \"days_back\": days_back,\n",
    "                        \"scraped_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                    }\n",
    "                    \n",
    "                    jobs.append(job_data)\n",
    "                    logging.info(f'Scraped \"{job_title_text}\" at {job_company_text} in {job_location_text}...')\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing job listing {idx}: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            logging.info(f\"Successfully scraped {len(jobs)} jobs\")\n",
    "            return jobs\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred while scraping jobs: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"WebDriver closed successfully\")\n",
    "    \n",
    "    # Add methods to the class\n",
    "    LinkedInJobScraper.scrape_jobs = scrape_jobs\n",
    "    LinkedInJobScraper.close = close\n",
    "\n",
    "# Execute the function to add methods to the class\n",
    "add_methods_to_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ff3776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for job scraping and data management\n",
    "\n",
    "def save_jobs_to_csv(jobs_data: list, filename: str = \"linkedin_jobs.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Save job data to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        jobs_data: A list of dictionaries containing job data\n",
    "        filename: Name of the CSV file to save to\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    if not jobs_data:\n",
    "        logging.warning(\"No job data to save\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Create a pandas DataFrame from the job data\n",
    "        df = pd.DataFrame(jobs_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file without including the index column\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "        # Log a message indicating how many jobs were successfully scraped and saved\n",
    "        logging.info(f\"Successfully saved {len(jobs_data)} jobs to {filename}\")\n",
    "        print(f\"‚úÖ Saved {len(jobs_data)} jobs to {filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving jobs to CSV: {str(e)}\")\n",
    "        print(f\"‚ùå Error saving jobs to CSV: {str(e)}\")\n",
    "\n",
    "def scrape_jobs_for_resume(resume_data: dict, pages_per_role: int = 1, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Scrape LinkedIn jobs based on resume analysis results\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_data : dict\n",
    "        Resume analysis data containing recommended_roles and experience_level\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        Combined list of all scraped jobs\n",
    "    \"\"\"\n",
    "    all_jobs = []\n",
    "    \n",
    "    # Extract recommended roles and experience level\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    experience_level = resume_data.get(\"experience_level\", \"\")\n",
    "    \n",
    "    if not recommended_roles:\n",
    "        logging.warning(\"No recommended roles found in resume data\")\n",
    "        print(\"‚ö†Ô∏è No recommended roles found in resume data\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"üéØ Found {len(recommended_roles)} recommended roles: {recommended_roles}\")\n",
    "    print(f\"üìä Experience Level: {experience_level}\")\n",
    "    print(f\"‚è∞ Looking for jobs posted in the last {days_back} days\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nüîç Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nüéâ Total jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"‚ùå Error during job scraping: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "def analyze_and_scrape_jobs(resume_path: str, file_type: str = \"pdf\", \n",
    "                          pages_per_role: int = 1, days_back: int = 7) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Analyze resume and scrape relevant jobs\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    resume_path : str\n",
    "        Path to the resume file\n",
    "    file_type : str\n",
    "        Type of resume file (pdf or docx)\n",
    "    pages_per_role : int\n",
    "        Number of pages to scrape for each recommended role\n",
    "    days_back : int\n",
    "        Number of days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Combined resume analysis and job scraping results\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Extract and analyze resume\n",
    "    print(\"üìÑ Step 1: Analyzing resume...\")\n",
    "    text = extract_text_from_file(resume_path, file_type)\n",
    "    \n",
    "    try:\n",
    "        raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "        resume_data = json.loads(raw_output)\n",
    "        \n",
    "        print(\"‚úÖ Resume analysis completed\")\n",
    "        print(f\"üë§ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "        print(f\"üìç Location: {resume_data.get('location', 'N/A')}\")\n",
    "        print(f\"üìä Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "        print(f\"üéØ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error analyzing resume: {str(e)}\")\n",
    "        return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    # Step 2: Scrape jobs based on analysis\n",
    "    print(f\"\\nüîç Step 2: Scraping jobs for recommended roles...\")\n",
    "    scraped_jobs = scrape_jobs_for_resume(resume_data, pages_per_role, days_back)\n",
    "    \n",
    "    # Step 3: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": scraped_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(scraped_jobs),\n",
    "            \"recommended_roles_searched\": resume_data.get(\"recommended_roles\", []),\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 4: Save results\n",
    "    if scraped_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        csv_filename = f\"jobs_{resume_data.get('name', 'candidate').replace(' ', '_')}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(scraped_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9f998073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Fixed Pipeline...\n",
      "üöÄ Starting Resume Analysis and Job Scraping Pipeline\n",
      "================================================================================\n",
      "üìÑ Step 1: Analyzing resume...\n",
      "‚úÖ Resume data ready\n",
      "üë§ Candidate: Yeswanth Yerra\n",
      "üìç Location: India\n",
      "üìä Experience Level: Entry Level\n",
      "üéØ Recommended Roles: ['Software Engineer', 'Backend Developer', 'Data Scientist', 'DevOps Engineer']\n",
      "\n",
      "üîç Step 2: Scraping real jobs for 4 roles...\n",
      "‚ö†Ô∏è This will open a browser window and scrape real LinkedIn jobs\n",
      "\n",
      "üîç Scraping jobs for role 1/4: 'Software Engineer'\n",
      "‚úÖ Found 57 jobs for 'Software Engineer'\n",
      "‚è≥ Waiting before next search...\n",
      "\n",
      "üîç Scraping jobs for role 2/4: 'Backend Developer'\n",
      "‚úÖ Found 29 jobs for 'Backend Developer'\n",
      "‚è≥ Waiting before next search...\n",
      "\n",
      "üîç Scraping jobs for role 3/4: 'Data Scientist'\n",
      "‚úÖ Found 59 jobs for 'Data Scientist'\n",
      "‚è≥ Waiting before next search...\n",
      "\n",
      "üîç Scraping jobs for role 4/4: 'DevOps Engineer'\n",
      "‚úÖ Found 9 jobs for 'DevOps Engineer'\n",
      "\n",
      "üéâ Total real jobs scraped: 154\n",
      "üîí Browser closed\n",
      "‚úÖ Saved 154 jobs to jobs_Yeswanth_Yerra_20251008_212813.csv\n",
      "üíæ Jobs saved to: jobs_Yeswanth_Yerra_20251008_212813.csv\n",
      "\n",
      "üéâ Pipeline completed successfully!\n",
      "================================================================================\n",
      "\n",
      "üìä PIPELINE SUMMARY:\n",
      "==================================================\n",
      "Total Jobs Found: 154\n",
      "Roles Searched: ['Software Engineer', 'Backend Developer', 'Data Scientist', 'DevOps Engineer']\n",
      "Experience Level: Entry Level\n",
      "Status: Real LinkedIn jobs scraped\n",
      "CSV File: jobs_Yeswanth_Yerra_20251008_212813.csv\n"
     ]
    }
   ],
   "source": [
    "# Fixed version with better error handling\n",
    "def analyze_and_scrape_jobs_fixed(resume_path: str, file_type: str = \"pdf\", \n",
    "                                pages_per_role: int = 1, days_back: int = 7, resume_data: dict | None = None) -> dict:\n",
    "    \"\"\"\n",
    "    Complete pipeline: Reuse parsed resume if available; otherwise analyze, then scrape jobs.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Starting Resume Analysis and Job Scraping Pipeline\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Step 1: Get resume data (prefer existing parsed output)\n",
    "    if resume_data is None:\n",
    "        try:\n",
    "            if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "                resume_data = structured_output\n",
    "                print(\"‚úÖ Using existing parsed resume from structured_output\")\n",
    "            elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "                resume_data = result['resume_analysis']\n",
    "                print(\"‚úÖ Using existing parsed resume from result['resume_analysis']\")\n",
    "        except Exception:\n",
    "            resume_data = None\n",
    "    \n",
    "    if resume_data is None:\n",
    "        print(\"üìÑ Step 1: Analyzing resume...\")\n",
    "        text = extract_text_from_file(resume_path, file_type)\n",
    "        try:\n",
    "            raw_output = resume_parser_chain.run({\"resume_text\": text})\n",
    "            # Clean the output to handle markdown formatting\n",
    "            cleaned_output = raw_output.strip()\n",
    "            if cleaned_output.startswith(\"```json\"):\n",
    "                cleaned_output = cleaned_output[7:]\n",
    "            if cleaned_output.endswith(\"```\"):\n",
    "                cleaned_output = cleaned_output[:-3]\n",
    "            cleaned_output = cleaned_output.strip()\n",
    "            # Try to parse JSON\n",
    "            try:\n",
    "                resume_data = json.loads(cleaned_output)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"‚ùå JSON parsing failed: {e}\")\n",
    "                json_start = cleaned_output.find('{')\n",
    "                if json_start != -1:\n",
    "                    json_part = cleaned_output[json_start:]\n",
    "                    try:\n",
    "                        resume_data = json.loads(json_part)\n",
    "                        print(\"‚úÖ JSON parsing successful after extraction!\")\n",
    "                    except json.JSONDecodeError as e2:\n",
    "                        print(f\"‚ùå Still failed after extraction: {e2}\")\n",
    "                        return {\"error\": f\"JSON parsing failed: {e2}\"}\n",
    "                else:\n",
    "                    return {\"error\": f\"No JSON found in output: {cleaned_output[:200]}...\"}\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing resume: {str(e)}\")\n",
    "            return {\"error\": f\"Resume analysis failed: {str(e)}\"}\n",
    "    \n",
    "    print(\"‚úÖ Resume data ready\")\n",
    "    print(f\"üë§ Candidate: {resume_data.get('name', 'N/A')}\")\n",
    "    print(f\"üìç Location: {resume_data.get('location', 'N/A')}\")\n",
    "    print(f\"üìä Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "    print(f\"üéØ Recommended Roles: {resume_data.get('recommended_roles', [])}\")\n",
    "    \n",
    "    # Step 2: Check if we have recommended roles\n",
    "    recommended_roles = resume_data.get(\"recommended_roles\", [])\n",
    "    if not recommended_roles:\n",
    "        print(\"‚ö†Ô∏è No recommended roles found. Cannot scrape jobs.\")\n",
    "        return {\n",
    "            \"resume_analysis\": resume_data,\n",
    "            \"scraped_jobs\": [],\n",
    "            \"summary\": {\n",
    "                \"total_jobs_found\": 0,\n",
    "                \"recommended_roles_searched\": [],\n",
    "                \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "                \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"status\": \"No recommended roles found\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Step 3: Scrape jobs based on analysis\n",
    "    print(f\"\\nüîç Step 2: Scraping real jobs for {len(recommended_roles)} roles...\")\n",
    "    print(\"‚ö†Ô∏è This will open a browser window and scrape real LinkedIn jobs\")\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)  # Set to True for headless mode\n",
    "    \n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(recommended_roles, 1):\n",
    "            print(f\"\\nüîç Scraping jobs for role {i}/{len(recommended_roles)}: '{role}'\")\n",
    "            \n",
    "            # Scrape jobs for this role\n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=\"India\",\n",
    "                pages=pages_per_role,\n",
    "                experience_level=resume_data.get(\"experience_level\", \"\"),\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between role searches to be respectful\n",
    "            if i < len(recommended_roles):\n",
    "                print(f\"‚è≥ Waiting before next search...\")\n",
    "                time.sleep(random.choice(list(range(5, 10))))\n",
    "        \n",
    "        print(f\"\\nüéâ Total real jobs scraped: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during job scraping: {str(e)}\")\n",
    "        print(f\"‚ùå Error during job scraping: {str(e)}\")\n",
    "        all_jobs = []\n",
    "    \n",
    "    finally:\n",
    "        # Always close the scraper\n",
    "        scraper.close()\n",
    "        print(\"üîí Browser closed\")\n",
    "    \n",
    "    # Step 4: Combine results\n",
    "    result = {\n",
    "        \"resume_analysis\": resume_data,\n",
    "        \"scraped_jobs\": all_jobs,\n",
    "        \"summary\": {\n",
    "            \"total_jobs_found\": len(all_jobs),\n",
    "            \"recommended_roles_searched\": recommended_roles,\n",
    "            \"experience_level\": resume_data.get(\"experience_level\", \"\"),\n",
    "            \"scraping_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"status\": \"Real LinkedIn jobs scraped\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Step 5: Save results\n",
    "    if all_jobs:\n",
    "        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        candidate_name = resume_data.get('name', 'candidate').replace(' ', '_')\n",
    "        csv_filename = f\"jobs_{candidate_name}_{timestamp}.csv\"\n",
    "        save_jobs_to_csv(all_jobs, csv_filename)\n",
    "        result[\"csv_file\"] = csv_filename\n",
    "        print(f\"üíæ Jobs saved to: {csv_filename}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No jobs found to save\")\n",
    "    \n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the fixed version\n",
    "print(\"üß™ Testing Fixed Pipeline...\")\n",
    "result = analyze_and_scrape_jobs_fixed(\n",
    "    resume_path=\"app/resumes/Yeswanth_Yerra_CV.pdf\",\n",
    "    file_type=\"pdf\",\n",
    "    pages_per_role=1,\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Print summary safely\n",
    "print(\"\\nüìä PIPELINE SUMMARY:\")\n",
    "print(\"=\"*50)\n",
    "if \"summary\" in result:\n",
    "    print(f\"Total Jobs Found: {result['summary']['total_jobs_found']}\")\n",
    "    print(f\"Roles Searched: {result['summary']['recommended_roles_searched']}\")\n",
    "    print(f\"Experience Level: {result['summary']['experience_level']}\")\n",
    "    print(f\"Status: {result['summary']['status']}\")\n",
    "    if 'csv_file' in result:\n",
    "        print(f\"CSV File: {result['csv_file']}\")\n",
    "else:\n",
    "    print(f\"‚ùå Error: {result.get('error', 'Unknown error')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a1a7d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Test with specific roles\\ntest_roles = [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\\njobs = scrape_jobs_directly(\\n    job_roles=test_roles,\\n    location=\"India\", \\n    pages=1,\\n    experience_level=\"Junior\",\\n    days_back=7\\n)\\n\\n# Save results\\nif jobs:\\n    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\\n    filename = f\"direct_jobs_{timestamp}.csv\"\\n    save_jobs_to_csv(jobs, filename)\\n    print(f\"üíæ Saved {len(jobs)} jobs to {filename}\")\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple function to scrape jobs for specific roles\n",
    "def scrape_jobs_directly(job_roles: list, location: str = \"India\", pages: int = 1, \n",
    "                        experience_level: str = None, days_back: int = 7) -> list:\n",
    "    \"\"\"\n",
    "    Directly scrape jobs for given roles without resume analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    job_roles : list\n",
    "        List of job titles to search for\n",
    "    location : str\n",
    "        Location to search in (default: \"India\")\n",
    "    pages : int\n",
    "        Number of pages to scrape per role\n",
    "    experience_level : str\n",
    "        Experience level filter\n",
    "    days_back : int\n",
    "        Days to look back for job postings\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of scraped job dictionaries\n",
    "    \"\"\"\n",
    "    print(f\"üîç Direct Job Scraping for {len(job_roles)} roles\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìç Location: {location}\")\n",
    "    print(f\"üìä Experience Level: {experience_level or 'Any'}\")\n",
    "    print(f\"‚è∞ Days Back: {days_back}\")\n",
    "    print(f\"üìÑ Pages per role: {pages}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize scraper\n",
    "    scraper = LinkedInJobScraper(headless=False)\n",
    "    all_jobs = []\n",
    "    \n",
    "    try:\n",
    "        for i, role in enumerate(job_roles, 1):\n",
    "            print(f\"\\nüîç [{i}/{len(job_roles)}] Scraping: '{role}'\")\n",
    "            \n",
    "            jobs = scraper.scrape_jobs(\n",
    "                job_title=role,\n",
    "                location=location,\n",
    "                pages=pages,\n",
    "                experience_level=experience_level,\n",
    "                days_back=days_back\n",
    "            )\n",
    "            \n",
    "            if jobs:\n",
    "                all_jobs.extend(jobs)\n",
    "                print(f\"‚úÖ Found {len(jobs)} jobs for '{role}'\")\n",
    "                \n",
    "                # Show first few job titles as preview\n",
    "                for j, job in enumerate(jobs[:3], 1):\n",
    "                    print(f\"   {j}. {job['title']} at {job['company']}\")\n",
    "                if len(jobs) > 3:\n",
    "                    print(f\"   ... and {len(jobs) - 3} more\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è No jobs found for '{role}'\")\n",
    "            \n",
    "            # Add delay between searches\n",
    "            if i < len(job_roles):\n",
    "                delay = random.choice(list(range(5, 10)))\n",
    "                print(f\"‚è≥ Waiting {delay} seconds before next search...\")\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        print(f\"\\nüéâ Scraping completed!\")\n",
    "        print(f\"üìä Total jobs found: {len(all_jobs)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during scraping: {str(e)}\")\n",
    "        logging.error(f\"Direct scraping error: {str(e)}\")\n",
    "    \n",
    "    finally:\n",
    "        scraper.close()\n",
    "        print(\"üîí Browser closed\")\n",
    "    \n",
    "    return all_jobs\n",
    "\n",
    "# Example usage (uncomment to test):\n",
    "\"\"\"\n",
    "# Test with specific roles\n",
    "test_roles = [\"Data Analyst\", \"Software Engineer\", \"Python Developer\"]\n",
    "jobs = scrape_jobs_directly(\n",
    "    job_roles=test_roles,\n",
    "    location=\"India\", \n",
    "    pages=1,\n",
    "    experience_level=\"Junior\",\n",
    "    days_back=7\n",
    ")\n",
    "\n",
    "# Save results\n",
    "if jobs:\n",
    "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"direct_jobs_{timestamp}.csv\"\n",
    "    save_jobs_to_csv(jobs, filename)\n",
    "    print(f\"üíæ Saved {len(jobs)} jobs to {filename}\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "860d0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "# Advanced Embedding Service for Job Matching\n",
    "class EmbeddingService:\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"Initialize the embedding service with a sentence-transformer model\"\"\"\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    \n",
    "    def create_resume_embeddings(self, resume_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create both global and section-based embeddings for resume (vectors + token sets)\n",
    "        \"\"\"\n",
    "        # Build texts\n",
    "        global_text = self._build_global_resume_text(resume_data)\n",
    "        skills_list = (resume_data.get('skills') or []) + (resume_data.get('extra_skills') or [])\n",
    "        experience_text = self._build_experience_text(resume_data)\n",
    "        certifications_list = resume_data.get('certifications') or []\n",
    "        \n",
    "        # Encode to vectors\n",
    "        vectors = {\n",
    "            'global': self._encode_text(global_text),\n",
    "            'skills': self._encode_text(' '.join(skills_list)),\n",
    "            'experience': self._encode_text(experience_text),\n",
    "            'certifications': self._encode_text(' '.join([c if isinstance(c, str) else c.get('certification', '') for c in certifications_list]))\n",
    "        }\n",
    "        \n",
    "        # Token sets for overlap\n",
    "        tokens = lambda s: {t.lower() for t in (s or '').split() if t}\n",
    "        overlap = {\n",
    "            'skills_set': {s.lower() for s in skills_list if isinstance(s, str)},\n",
    "            'experience_set': tokens(experience_text),\n",
    "            'certifications_set': { (c if isinstance(c, str) else c.get('certification', '')).lower() for c in certifications_list }\n",
    "        }\n",
    "        return { 'vectors': vectors, 'overlap': overlap }\n",
    "    \n",
    "    def create_job_embeddings(self, job_data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Create embeddings for job posting (vectors + token sets)\n",
    "        \"\"\"\n",
    "        title = job_data.get('title', '')\n",
    "        description = job_data.get('description', '')\n",
    "        company = job_data.get('company', '')\n",
    "        global_job_text = f\"{title} {description} {company}\".strip()\n",
    "        requirements_text = self._extract_requirements(description).strip()\n",
    "        if not requirements_text:\n",
    "            requirements_text = description.strip() or global_job_text\n",
    "        combined_job = f\"{title} {requirements_text}\".strip()\n",
    "        \n",
    "        vectors = {\n",
    "            'global': self._encode_text(global_job_text),\n",
    "            'requirements': self._encode_text(requirements_text),\n",
    "            'context': self._encode_text(combined_job)\n",
    "        }\n",
    "        \n",
    "        # naive skill tokens from title+description\n",
    "        tokens = lambda s: {t.lower() for t in (s or '').split() if t}\n",
    "        overlap = {\n",
    "            'skills_set': tokens(title + ' ' + requirements_text),\n",
    "            'experience_set': tokens(description)\n",
    "        }\n",
    "        return { 'vectors': vectors, 'overlap': overlap }\n",
    "    \n",
    "    def _zero_vec(self):\n",
    "        try:\n",
    "            probe = self.embeddings.embed_query(\" \")\n",
    "            return np.zeros(len(probe), dtype=float)\n",
    "        except Exception:\n",
    "            return np.zeros(384, dtype=float)  # common default for MiniLM\n",
    "    \n",
    "    def _encode_text(self, text: str):\n",
    "        text_clean = (text or \"\").strip()\n",
    "        if not text_clean or text_clean.lower() in {\"description not available\", \"n/a\"}:\n",
    "            return self._zero_vec()\n",
    "        vec = self.embeddings.embed_query(text_clean)\n",
    "        return np.array(vec, dtype=float)\n",
    "    \n",
    "    def _build_global_resume_text(self, resume_data: dict) -> str:\n",
    "        parts = []\n",
    "        if resume_data.get('summary'):\n",
    "            parts.append(resume_data['summary'])\n",
    "        if resume_data.get('skills'):\n",
    "            parts.extend(resume_data['skills'])\n",
    "        if resume_data.get('extra_skills'):\n",
    "            parts.extend(resume_data['extra_skills'])\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                parts.append(f\"{exp.get('job_title', '')} at {exp.get('company', '')}\")\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    parts.append(exp['description'])\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                parts.append(f\"Project: {project.get('project_name', '')}\")\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    parts.extend(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    parts.append(project['description'])\n",
    "        if resume_data.get('education'):\n",
    "            for edu in resume_data['education']:\n",
    "                parts.append(f\"{edu.get('degree', '')} from {edu.get('institution', '')}\")\n",
    "        return ' '.join(parts)\n",
    "    \n",
    "    def _build_skills_text(self, resume_data: dict) -> str:\n",
    "        skills_parts = []\n",
    "        if resume_data.get('skills'):\n",
    "            skills_parts.extend(resume_data['skills'])\n",
    "        if resume_data.get('extra_skills'):\n",
    "            skills_parts.extend(resume_data['extra_skills'])\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    skills_parts.extend(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    skills_parts.append(exp['description'])\n",
    "        return ' '.join(skills_parts)\n",
    "    \n",
    "    def _build_experience_text(self, resume_data: dict) -> str:\n",
    "        experience_parts = []\n",
    "        if resume_data.get('work_experience'):\n",
    "            for exp in resume_data['work_experience']:\n",
    "                exp_text = f\"{exp.get('job_title', '')} {exp.get('company', '')}\"\n",
    "                if isinstance(exp.get('description'), list):\n",
    "                    exp_text += ' ' + ' '.join(exp['description'])\n",
    "                elif exp.get('description'):\n",
    "                    exp_text += ' ' + exp['description']\n",
    "                experience_parts.append(exp_text)\n",
    "        if resume_data.get('projects'):\n",
    "            for project in resume_data['projects']:\n",
    "                project_text = f\"Project: {project.get('project_name', '')}\"\n",
    "                if isinstance(project.get('description'), list):\n",
    "                    project_text += ' ' + ' '.join(project['description'])\n",
    "                elif project.get('description'):\n",
    "                    project_text += ' ' + project['description']\n",
    "                experience_parts.append(project_text)\n",
    "        return ' '.join(experience_parts)\n",
    "    \n",
    "    def _extract_requirements(self, job_description: str) -> str:\n",
    "        if not job_description:\n",
    "            return \"\"\n",
    "        requirements_keywords = [\n",
    "            'requirements', 'qualifications', 'skills', 'experience',\n",
    "            'must have', 'should have', 'preferred', 'bachelor', 'master',\n",
    "            'years of experience', 'proficient', 'knowledge of'\n",
    "        ]\n",
    "        sentences = job_description.split('.')\n",
    "        requirement_sentences = []\n",
    "        for sentence in sentences:\n",
    "            sentence_lower = sentence.lower()\n",
    "            if any(keyword in sentence_lower for keyword in requirements_keywords):\n",
    "                requirement_sentences.append(sentence.strip())\n",
    "        # Fallback: first few sentences if none\n",
    "        if not requirement_sentences:\n",
    "            requirement_sentences = sentences[:3]\n",
    "        return ' '.join([s.strip() for s in requirement_sentences if s.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63cd7cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Job Matching System with Weighted Similarity\n",
    "class JobMatchingService:\n",
    "    def __init__(self, embedding_service: EmbeddingService):\n",
    "        self.embedding_service = embedding_service\n",
    "        self.weights = {\n",
    "            'global': 0.7,\n",
    "            'skills': 0.2,\n",
    "            'experience': 0.1\n",
    "        }\n",
    "    \n",
    "    def _jaccard(self, a_set: set, b_set: set) -> float:\n",
    "        if not a_set or not b_set:\n",
    "            return 0.0\n",
    "        inter = len(a_set & b_set)\n",
    "        union = len(a_set | b_set)\n",
    "        return inter / union if union else 0.0\n",
    "    \n",
    "    def calculate_similarity(self, resume_embeddings: dict, job_embeddings: dict) -> dict:\n",
    "        similarities = {}\n",
    "        try:\n",
    "            # Unpack vectors and overlap sets\n",
    "            r_vecs = resume_embeddings['vectors']\n",
    "            r_ov = resume_embeddings['overlap']\n",
    "            j_vecs = job_embeddings['vectors']\n",
    "            j_ov = job_embeddings['overlap']\n",
    "            \n",
    "            # Semantic similarities\n",
    "            global_sem = self._cosine(r_vecs['global'], j_vecs['global'])\n",
    "            skills_sem = self._cosine(r_vecs['skills'], j_vecs.get('requirements', j_vecs['global']))\n",
    "            exp_sem = self._cosine(r_vecs['experience'], j_vecs.get('context', j_vecs['global']))\n",
    "            cert_sem = self._cosine(r_vecs.get('certifications'), j_vecs.get('requirements'))\n",
    "            \n",
    "            # Overlap (lexical) similarities\n",
    "            skills_overlap = self._jaccard(r_ov['skills_set'], j_ov['skills_set'])\n",
    "            exp_overlap = self._jaccard(r_ov['experience_set'], j_ov['experience_set'])\n",
    "            cert_overlap = self._jaccard(r_ov['certifications_set'], j_ov.get('certifications_set', set()))\n",
    "            \n",
    "            # Blend semantics and overlap per channel\n",
    "            skills_score = 0.7 * skills_sem + 0.3 * skills_overlap\n",
    "            exp_score = 0.6 * exp_sem + 0.4 * exp_overlap\n",
    "            cert_score = 0.6 * cert_sem + 0.4 * cert_overlap\n",
    "            \n",
    "            similarities['global'] = global_sem\n",
    "            similarities['skills'] = skills_score\n",
    "            similarities['experience'] = exp_score\n",
    "            similarities['certifications'] = cert_score\n",
    "            \n",
    "            final_score = (\n",
    "                self.weights['global'] * similarities['global'] +\n",
    "                self.weights['skills'] * similarities['skills'] +\n",
    "                self.weights['experience'] * similarities['experience']\n",
    "            )\n",
    "            # small bonus for certifications\n",
    "            final_score = 0.95 * final_score + 0.05 * similarities['certifications']\n",
    "            \n",
    "            similarities['final_score'] = float(final_score)\n",
    "            return similarities\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating similarity: {e}\")\n",
    "            return {'global': 0.0, 'skills': 0.0, 'experience': 0.0, 'final_score': 0.0, 'error': str(e)}\n",
    "    \n",
    "    def _ensure_vec(self, x) -> np.ndarray:\n",
    "        # Accept either vector or raw text; convert text to embedding\n",
    "        if isinstance(x, np.ndarray):\n",
    "            return x\n",
    "        if isinstance(x, (list, tuple)):\n",
    "            return np.array(x, dtype=float)\n",
    "        if isinstance(x, str):\n",
    "            return self.embedding_service._encode_text(x)\n",
    "        return self.embedding_service._zero_vec()\n",
    "    \n",
    "    def _cosine(self, a, b) -> float:\n",
    "        a_vec = self._ensure_vec(a)\n",
    "        b_vec = self._ensure_vec(b)\n",
    "        a_norm = np.linalg.norm(a_vec)\n",
    "        b_norm = np.linalg.norm(b_vec)\n",
    "        if a_norm == 0.0 or b_norm == 0.0:\n",
    "            return 0.0\n",
    "        return float(np.dot(a_vec, b_vec) / (a_norm * b_norm))\n",
    "    \n",
    "    def match_jobs(self, resume_data: dict, jobs_df: pd.DataFrame, threshold: float = 0.2, top_n: int = 0) -> pd.DataFrame:\n",
    "        print(f\"üéØ Starting job matching for {len(jobs_df)} jobs...\")\n",
    "        print(f\"üìä Using threshold: {threshold}\")\n",
    "        \n",
    "        resume_embeddings = self.embedding_service.create_resume_embeddings(resume_data)\n",
    "        job_scores = []\n",
    "        for idx, job_row in jobs_df.iterrows():\n",
    "            job_data = {\n",
    "                'title': job_row.get('title', ''),\n",
    "                'description': job_row.get('description', ''),\n",
    "                'company': job_row.get('company', ''),\n",
    "                'location': job_row.get('location', ''),\n",
    "                'link': job_row.get('link', '')\n",
    "            }\n",
    "            job_embeddings = self.embedding_service.create_job_embeddings(job_data)\n",
    "            similarities = self.calculate_similarity(resume_embeddings, job_embeddings)\n",
    "            job_scores.append({\n",
    "                'job_index': idx,\n",
    "                'title': job_data['title'],\n",
    "                'company': job_data['company'],\n",
    "                'location': job_data['location'],\n",
    "                'link': job_data['link'],\n",
    "                'global_similarity': similarities['global'],\n",
    "                'skills_similarity': similarities['skills'],\n",
    "                'experience_similarity': similarities['experience'],\n",
    "                'final_score': similarities['final_score']\n",
    "            })\n",
    "        results_df = pd.DataFrame(job_scores)\n",
    "        filtered_df = results_df[results_df['final_score'] >= threshold].copy()\n",
    "        filtered_df = filtered_df.sort_values('final_score', ascending=False)\n",
    "        print(f\"‚úÖ Found {len(filtered_df)} jobs above threshold {threshold}\")\n",
    "        return filtered_df\n",
    "    \n",
    "    def update_weights(self, global_weight: float = None, skills_weight: float = None, experience_weight: float = None):\n",
    "        if global_weight is not None:\n",
    "            self.weights['global'] = global_weight\n",
    "        if skills_weight is not None:\n",
    "            self.weights['skills'] = skills_weight\n",
    "        if experience_weight is not None:\n",
    "            self.weights['experience'] = experience_weight\n",
    "        total_weight = sum(self.weights.values())\n",
    "        if total_weight > 0:\n",
    "            for key in self.weights:\n",
    "                self.weights[key] = self.weights[key] / total_weight\n",
    "        print(f\"üîÑ Updated weights: {self.weights}\")\n",
    "\n",
    "# Initialize services\n",
    "embedding_service = EmbeddingService()\n",
    "job_matching_service = JobMatchingService(embedding_service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23772f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Running Job Matching Demo...\n",
      "üöÄ AI Career System - Advanced Job Matching Demo\n",
      "======================================================================\n",
      "‚úÖ Loaded 154 jobs from jobs_Yeswanth_Yerra_20251008_212813.csv\n",
      "\n",
      "üìä Sample Job Data:\n",
      "Columns: ['title', 'company', 'location', 'link', 'description', 'searched_for', 'experience_level_filter', 'days_back', 'scraped_at']\n",
      "First job title: Software Development Engineer Intern\n",
      "First company: HackerRank\n",
      "‚úÖ Using resume data from result['resume_analysis']\n",
      "\n",
      "üë§ Resume Analysis:\n",
      "Name: Yeswanth Yerra\n",
      "Experience Level: Entry Level\n",
      "Key Skills: ['Java', 'JavaScript', 'Python', 'Data Structures & Algorithms', 'Computer Networks']\n",
      "\n",
      "üéØ Running Job Matching...\n",
      "\n",
      "üìä Testing with threshold: 0.2\n",
      "üéØ Starting job matching for 154 jobs...\n",
      "üìä Using threshold: 0.2\n",
      "‚úÖ Found 134 jobs above threshold 0.2\n",
      "‚úÖ Found 134 matching jobs:\n",
      "  124. Artificial Intelligence Intern at Zetheta Algorithms Private Limited (Score: 0.392)\n",
      "  67. Back End Developer at Zetheta Algorithms Private Limited (Score: 0.392)\n",
      "  70. Junior AI & Back-End Engineer at JobAlchemy (Score: 0.381)\n",
      "  101. Junior AI & Back-End Engineer at JobAlchemy (Score: 0.381)\n",
      "  100. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  19. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  62. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  78. We Are Hiring Java  Developer  Intern at EB Softco at EB Softco (Score: 0.372)\n",
      "  66. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.363)\n",
      "  38. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.363)\n",
      "  88. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.361)\n",
      "  55. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.361)\n",
      "  47. SDE Intern at Zorvyn (Score: 0.349)\n",
      "  46. Full Stack Developer Intern at OneDot Communications (Score: 0.342)\n",
      "  69. Full Stack Developer Intern at OneDot Communications (Score: 0.342)\n",
      "  96. Artificial Intelligence Researcher at Valiance Solutions (Score: 0.337)\n",
      "  131. Post Doc Fellow at GE Aerospace (Score: 0.333)\n",
      "  9. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.333)\n",
      "  63. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.333)\n",
      "  71. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  20. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  118. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  77. Full Stack Engineer at Zetheta Algorithms Private Limited (Score: 0.331)\n",
      "  95. Data Science Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.330)\n",
      "  103. Data Science Intern at UM IT PRIVATE LIMITED (Score: 0.330)\n",
      "  112. Engineer at KP Group (Score: 0.325)\n",
      "  39. Associate Developer Intern at Jivox (Score: 0.323)\n",
      "  149. DevOps Engineer Intern at FinacPlus (Score: 0.323)\n",
      "  30. Software Development Intern at Talentorix (Score: 0.323)\n",
      "  81. Full Stack React Native Developer Intern at EnxtAI (Score: 0.321)\n",
      "  6. Web Development Internship in Jaipur at Trumpet Media (Score: 0.321)\n",
      "  27. AI Engineer, Intern at Newfold Digital (Score: 0.320)\n",
      "  93. AI Engineer, Intern at Newfold Digital (Score: 0.320)\n",
      "  86. Internship Program at SUM INAgri (Score: 0.320)\n",
      "  125. Django + AI Developer Intern (0-1 year of experience) at Codevantage Inc (Score: 0.310)\n",
      "  102. Data Science & Analytics Intern at Future Interns (Score: 0.310)\n",
      "  114. Software Engineer Intern at INDmoney (Score: 0.309)\n",
      "  11. Software Engineer Intern at INDmoney (Score: 0.309)\n",
      "  153. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.309)\n",
      "  154. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.309)\n",
      "  117. Intern Generative/ Agentic AI at Tecnod8.AI (Score: 0.309)\n",
      "  34. Web Development Internship in Delhi at Traincape Technology (Score: 0.308)\n",
      "  97. Artificial Intelligence Engineer (Inetrn) at Corporate Website Solutions (Score: 0.307)\n",
      "  84. React Native Developer Intern at Sihari Labs Pvt Ltd (Score: 0.304)\n",
      "  145. React Native Developer Intern at Sihari Labs Pvt Ltd (Score: 0.304)\n",
      "  150. Interesting Job Opportunity: Cloud Support Engineer - SysOps at Comprinno (Score: 0.298)\n",
      "  141. Interesting Job Opportunity: Cloud Support Engineer - SysOps at Comprinno (Score: 0.298)\n",
      "  138. Software Engineer Intern at Readyly (Score: 0.298)\n",
      "  42. Software Engineer Intern at Readyly (Score: 0.298)\n",
      "  147. DEVOPS INTERN at Elevate Labs (Score: 0.295)\n",
      "  40. Full Stack Development Internship in Noida (Hybrid) at Namekart (Score: 0.294)\n",
      "  129. Full Stack Development Internship in Noida (Hybrid) at Namekart (Score: 0.294)\n",
      "  41. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.293)\n",
      "  50. Frontend Developer at Zetheta Algorithms Private Limited (Score: 0.293)\n",
      "  144. Frontend Developer at Zetheta Algorithms Private Limited (Score: 0.293)\n",
      "  14. Frontend Intern at GreedyGame (Score: 0.292)\n",
      "  89. Data Science Intern at Corporate Website Solutions (Score: 0.290)\n",
      "  18. SQL DEVELOPER INTERN at Elevate Labs (Score: 0.286)\n",
      "  26. front-end developer intern at Geniuses Factory (Score: 0.285)\n",
      "  132. front-end developer intern at Geniuses Factory (Score: 0.285)\n",
      "  85. Internship - React Native Developer at Nurdd (Score: 0.285)\n",
      "  60. Full Stack Developer Intern at PinSec.Ai (Score: 0.283)\n",
      "  22. Full Stack Developer Intern at PinSec.Ai (Score: 0.283)\n",
      "  151. DevOps and Cloud Deployment Internship at KaaryaConnect (Score: 0.282)\n",
      "  10. INTERN - Full Stack Python(Angular) at Kots (Score: 0.280)\n",
      "  73. JavaScript Developer Intern at UM IT PRIVATE LIMITED (Score: 0.279)\n",
      "  152. Cloud Engineer Intern at NanoByte IT Services (Score: 0.278)\n",
      "  91. Data Science Intern at Zetheta Algorithms Private Limited (Score: 0.277)\n",
      "  148. AWS DevOps Engineer Internship in Noida at URE Legal Advocates (Score: 0.277)\n",
      "  92. PYTHON DEVELOPER INTERN at Elevate Labs (Score: 0.277)\n",
      "  59. PYTHON DEVELOPER INTERN at Elevate Labs (Score: 0.277)\n",
      "  12. PYTHON DEVELOPER INTERN at Elevate Labs (Score: 0.277)\n",
      "  119. Tester Fresher at Crisil (Score: 0.276)\n",
      "  122. Junior React / Mobile App Developer at Prudent Gaming (Score: 0.275)\n",
      "  80. Junior React / Mobile App Developer at Prudent Gaming (Score: 0.275)\n",
      "  32. Software Engineer Intern at OpenSphere (Score: 0.274)\n",
      "  49. Front End Development Intern at Essor Talents Consultancy (Score: 0.271)\n",
      "  98. Artificial Intelligence Intern at OpenSphere (Score: 0.270)\n",
      "  44. Curriculum Developer Intern (AI & Coding) at Codingal (Score: 0.269)\n",
      "  120. Curriculum Developer Intern (AI & Coding) at Codingal (Score: 0.269)\n",
      "  111. AI Applications & Prompt Engineering Intern at Lingopanda (previously PingoLearn) (Score: 0.267)\n",
      "  64. Full Stack Developer Internship at KaaryaConnect (Score: 0.265)\n",
      "  36. Full Stack Developer Internship at KaaryaConnect (Score: 0.265)\n",
      "  142. React Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.263)\n",
      "  79. React Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.263)\n",
      "  5. MTS 2 (C++) at Adobe (Score: 0.260)\n",
      "  90. AI/ML Intern at CODEXINTERN (Score: 0.259)\n",
      "  109. AI Engineer at Smartan FitTech (Score: 0.256)\n",
      "  4. Intern - Software Engineering (.Net) at Icertis (Score: 0.254)\n",
      "  3. Full Stack Developer (Internship) at CloudRedux (Score: 0.249)\n",
      "  58. Full Stack Developer (Internship) at CloudRedux (Score: 0.249)\n",
      "  137. Full Stack Developer (Internship) at CloudRedux (Score: 0.249)\n",
      "  51. Front-End Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.249)\n",
      "  61. Full Stack Web Developer Intern at Corporate Website Solutions (Score: 0.249)\n",
      "  37. Full Stack Web Developer Intern at Corporate Website Solutions (Score: 0.249)\n",
      "  113. Generative AI Intern (Immediate Joiner) at techolution (Score: 0.247)\n",
      "  43. WEB DEVELOPMENT INTERN at Elevate Labs (Score: 0.245)\n",
      "  74. WEB DEVELOPMENT INTERN at Elevate Labs (Score: 0.245)\n",
      "  76. Full Stack Web Developer Intern at UM IT PRIVATE LIMITED (Score: 0.244)\n",
      "  72. Full Stack Web Developer Intern at UM IT Solutions (Score: 0.244)\n",
      "  31. SDE Intern (Frontend) at UniCult (Score: 0.240)\n",
      "  21. Software Engineer Intern at LegalBridge (Score: 0.239)\n",
      "  106. Python Developer Intern at CODEXINTERN (Score: 0.238)\n",
      "  104. Python Developer Intern at CODEXINTERN (Score: 0.238)\n",
      "  140. DevOps Engineer Intern - Indore/Hyderabad - Job ID-109579 at Techdome (Score: 0.237)\n",
      "  146. DevOps Engineer Intern - Indore/Hyderabad - Job ID-109579 at Techdome (Score: 0.237)\n",
      "  94. Data Science / AI-ML / Gen AI Mentor Intern at HCL GUVI (Score: 0.236)\n",
      "  24. Web Developer Intern at EazyByts.com (Score: 0.236)\n",
      "  110. Computer Vision Intern Remote at Bhatiyani Astute Intelligence (Score: 0.233)\n",
      "  1. Software Development Engineer Intern at HackerRank (Score: 0.226)\n",
      "  25. Android Engineering Intern at 10x (Score: 0.224)\n",
      "  75. Full Stack Web Developer Intern at UM IT Solutions (Score: 0.223)\n",
      "  68. Full Stack Web Developer Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.223)\n",
      "  116. Development Engineer, India at AlgoSec (Score: 0.221)\n",
      "  28. Python Developer Internship in Gurgaon at Monkhub Innovations (Score: 0.221)\n",
      "  105. Python Developer Internship in Gurgaon at Monkhub Innovations (Score: 0.221)\n",
      "  65. Full Stack Developer Intern at Calanjiyam Consultancies and Technologies (Score: 0.219)\n",
      "  53. Full Stack Developer Intern at Calanjiyam Consultancies and Technologies (Score: 0.219)\n",
      "  17. Frontend Developer Intern at CODEXINTERN (Score: 0.219)\n",
      "  23. Frontend Developer Intern at CODEXINTERN (Score: 0.219)\n",
      "  54. Front End Web Development Intern at Calanjiyam Consultancies and Technologies (Score: 0.217)\n",
      "  108. Causal Inference - Intern at Sony Research India (Score: 0.214)\n",
      "  133. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  56. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  45. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  48. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  130. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  134. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  128. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  135. Game Development Internship in Noida, Delhi, Ghaziabad, Greater Noida, Gautam Buddha Nagar (Hybrid) at Web3task (Score: 0.214)\n",
      "  99. AI Infra Intern at Bharat.Law (Score: 0.210)\n",
      "  29. Software Trainee Engineer at Essor Talents Consultancy (Score: 0.205)\n",
      "  7. Web Development Internship in Pune, Mumbai at Novus Logic (Score: 0.202)\n",
      "  15. Web Development Internship in Pune, Mumbai at Novus Logic (Score: 0.202)\n",
      "\n",
      "üìä Testing with threshold: 0.3\n",
      "üéØ Starting job matching for 154 jobs...\n",
      "üìä Using threshold: 0.3\n",
      "‚úÖ Found 45 jobs above threshold 0.3\n",
      "‚úÖ Found 45 matching jobs:\n",
      "  124. Artificial Intelligence Intern at Zetheta Algorithms Private Limited (Score: 0.392)\n",
      "  67. Back End Developer at Zetheta Algorithms Private Limited (Score: 0.392)\n",
      "  101. Junior AI & Back-End Engineer at JobAlchemy (Score: 0.381)\n",
      "  70. Junior AI & Back-End Engineer at JobAlchemy (Score: 0.381)\n",
      "  19. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  100. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  62. Full Stack AI/Gen-AI Developer (Internship) at CloudRedux (Score: 0.375)\n",
      "  78. We Are Hiring Java  Developer  Intern at EB Softco at EB Softco (Score: 0.372)\n",
      "  66. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.363)\n",
      "  38. We Are Hiring Java Developer Intern at EazyByts.com (Score: 0.363)\n",
      "  55. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.361)\n",
      "  88. Artificial Intelligence (AI) Internship in Pune at NoBrokerage.com (Score: 0.361)\n",
      "  47. SDE Intern at Zorvyn (Score: 0.349)\n",
      "  46. Full Stack Developer Intern at OneDot Communications (Score: 0.342)\n",
      "  69. Full Stack Developer Intern at OneDot Communications (Score: 0.342)\n",
      "  96. Artificial Intelligence Researcher at Valiance Solutions (Score: 0.337)\n",
      "  131. Post Doc Fellow at GE Aerospace (Score: 0.333)\n",
      "  63. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.333)\n",
      "  9. JAVA DEVELOPER INTERN at Elevate Labs (Score: 0.333)\n",
      "  20. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  71. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  118. Jr. React Developer (NV46FCT RM 3320) at Source-Right (Score: 0.331)\n",
      "  77. Full Stack Engineer at Zetheta Algorithms Private Limited (Score: 0.331)\n",
      "  95. Data Science Intern at WEBBOOST SOLUTION IT SERVICES (Score: 0.330)\n",
      "  103. Data Science Intern at UM IT PRIVATE LIMITED (Score: 0.330)\n",
      "  112. Engineer at KP Group (Score: 0.325)\n",
      "  39. Associate Developer Intern at Jivox (Score: 0.323)\n",
      "  149. DevOps Engineer Intern at FinacPlus (Score: 0.323)\n",
      "  30. Software Development Intern at Talentorix (Score: 0.323)\n",
      "  81. Full Stack React Native Developer Intern at EnxtAI (Score: 0.321)\n",
      "  6. Web Development Internship in Jaipur at Trumpet Media (Score: 0.321)\n",
      "  27. AI Engineer, Intern at Newfold Digital (Score: 0.320)\n",
      "  93. AI Engineer, Intern at Newfold Digital (Score: 0.320)\n",
      "  86. Internship Program at SUM INAgri (Score: 0.320)\n",
      "  125. Django + AI Developer Intern (0-1 year of experience) at Codevantage Inc (Score: 0.310)\n",
      "  102. Data Science & Analytics Intern at Future Interns (Score: 0.310)\n",
      "  11. Software Engineer Intern at INDmoney (Score: 0.309)\n",
      "  114. Software Engineer Intern at INDmoney (Score: 0.309)\n",
      "  154. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.309)\n",
      "  153. Global Contract Logistics IT DevOps Systems Engineer at Kuehne+Nagel (Score: 0.309)\n",
      "  117. Intern Generative/ Agentic AI at Tecnod8.AI (Score: 0.309)\n",
      "  34. Web Development Internship in Delhi at Traincape Technology (Score: 0.308)\n",
      "  97. Artificial Intelligence Engineer (Inetrn) at Corporate Website Solutions (Score: 0.307)\n",
      "  84. React Native Developer Intern at Sihari Labs Pvt Ltd (Score: 0.304)\n",
      "  145. React Native Developer Intern at Sihari Labs Pvt Ltd (Score: 0.304)\n",
      "\n",
      "üìä Testing with threshold: 0.4\n",
      "üéØ Starting job matching for 154 jobs...\n",
      "üìä Using threshold: 0.4\n",
      "‚úÖ Found 0 jobs above threshold 0.4\n",
      "‚ö†Ô∏è No jobs found above threshold 0.4\n",
      "\n",
      "üìä Testing with threshold: 0.5\n",
      "üéØ Starting job matching for 154 jobs...\n",
      "üìä Using threshold: 0.5\n",
      "‚úÖ Found 0 jobs above threshold 0.5\n",
      "‚ö†Ô∏è No jobs found above threshold 0.5\n"
     ]
    }
   ],
   "source": [
    "# Demo: Advanced Job Matching with Real Data\n",
    "def demo_job_matching():\n",
    "    \"\"\"\n",
    "    Demo function to test the advanced job matching system\n",
    "    \"\"\"\n",
    "    print(\"üöÄ AI Career System - Advanced Job Matching Demo\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load the scraped jobs data\n",
    "    jobs_file = \"jobs_Yeswanth_Yerra_20251008_212813.csv\"\n",
    "    \n",
    "    try:\n",
    "        jobs_df = pd.read_csv(jobs_file)\n",
    "        print(f\"‚úÖ Loaded {len(jobs_df)} jobs from {jobs_file}\")\n",
    "        \n",
    "        # Display sample job data\n",
    "        print(f\"\\nüìä Sample Job Data:\")\n",
    "        print(f\"Columns: {list(jobs_df.columns)}\")\n",
    "        print(f\"First job title: {jobs_df.iloc[0]['title']}\")\n",
    "        print(f\"First company: {jobs_df.iloc[0]['company']}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Jobs file not found: {jobs_file}\")\n",
    "        print(\"Please run the job scraping first or provide a valid CSV file\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading jobs: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Use real parsed resume data instead of a hardcoded sample\n",
    "    resume_data = None\n",
    "    try:\n",
    "        if 'structured_output' in globals() and isinstance(structured_output, dict):\n",
    "            resume_data = structured_output\n",
    "            print(\"‚úÖ Using resume data from structured_output\")\n",
    "        elif 'result' in globals() and isinstance(result, dict) and 'resume_analysis' in result:\n",
    "            resume_data = result['resume_analysis']\n",
    "            print(\"‚úÖ Using resume data from result['resume_analysis']\")\n",
    "    except Exception:\n",
    "        resume_data = None\n",
    "    \n",
    "    if not resume_data:\n",
    "        print(\"‚ö†Ô∏è No structured resume data available (structured_output or result['resume_analysis']).\")\n",
    "        print(\"   Please run the resume parsing cell first, then re-run this demo.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"\\nüë§ Resume Analysis:\")\n",
    "    print(f\"Name: {resume_data.get('name', 'N/A')}\")\n",
    "    print(f\"Experience Level: {resume_data.get('experience_level', 'N/A')}\")\n",
    "    skills_list = resume_data.get('skills', []) or []\n",
    "    print(f\"Key Skills: {skills_list[:5]}\")\n",
    "    \n",
    "    # Test the matching system\n",
    "    print(f\"\\nüéØ Running Job Matching...\")\n",
    "    \n",
    "    # Test with different thresholds\n",
    "    thresholds = [0.2, 0.3, 0.4, 0.5]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\nüìä Testing with threshold: {threshold}\")\n",
    "        matched_jobs = job_matching_service.match_jobs(\n",
    "            resume_data=resume_data,\n",
    "            jobs_df=jobs_df,\n",
    "            threshold=threshold,\n",
    "            top_n=5\n",
    "        )\n",
    "        \n",
    "        if len(matched_jobs) > 0:\n",
    "            print(f\"‚úÖ Found {len(matched_jobs)} matching jobs:\")\n",
    "            for idx, job in matched_jobs.iterrows():\n",
    "                print(f\"  {idx+1}. {job['title']} at {job['company']} (Score: {job['final_score']:.3f})\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No jobs found above threshold {threshold}\")\n",
    "    \n",
    "    return matched_jobs if 'matched_jobs' in locals() else None\n",
    "\n",
    "# Run the demo\n",
    "print(\"üß™ Running Job Matching Demo...\")\n",
    "demo_results = demo_job_matching()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
